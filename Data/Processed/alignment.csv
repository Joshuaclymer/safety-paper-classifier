,text
0,Is PowerSeeking AI an Existential Risk This report examines what I see as the core argument for concern aboutexistential risk from misaligned artificial intelligence. I proceed in twostages. First I lay out a backdrop picture that informs such concern. On thispicture intelligent agency is an extremely powerful force and creating agentsmuch more intelligent than us is playing with fire  especially given that iftheir objectives are problematic such agents would plausibly have instrumentalincentives to seek power over humans. Second I formulate and evaluate a morespecific sixpremise argument that creating agents of this kind will lead toexistential catastrophe by . On this argument by   it will becomepossible and financially feasible to build relevantly powerful and agentic AIsystems  there will be strong incentives to do so  it will be muchharder to build aligned and relevantly powerfulagentic AI systems than tobuild misaligned and relevantly powerfulagentic AI systems that are stillsuperficially attractive to deploy  some such misaligned systems will seekpower over humans in highimpact ways  this problem will scale to the fulldisempowerment of humanity and  such disempowerment will constitute anexistential catastrophe. I assign rough subjective credences to the premises inthis argument and I end up with an overall estimate of  that an existentialcatastrophe of this kind will occur by . May  update since makingthis report public in April  my estimate here has gone up and is now at.
1,Conservative Agency via Attainable Utility Preservation Reward functions are easy to misspecify although designers can makecorrections after observing mistakes an agent pursuing a misspecified rewardfunction can irreversibly change the state of its environment. If that changeprecludes optimization of the correctly specified reward function thencorrection is futile. For example a robotic factory assistant could breakexpensive equipment due to a reward misspecification even if the designersimmediately correct the reward function the damage is done. To mitigate thisrisk we introduce an approach that balances optimization of the primary rewardfunction with preservation of the ability to optimize auxiliary rewardfunctions. Surprisingly even when the auxiliary reward functions are randomlygenerated and therefore uninformative about the correctly specified rewardfunction this approach induces conservative effective behavior.
2,Parametrically Retargetable DecisionMakers Tend To Seek Power If capable AI agents are generally incentivized to seek power in service ofthe objectives we specify for them then these systems will pose enormousrisks in addition to enormous benefits. In fully observable environments mostreward functions have an optimal policy which seeks power by keeping optionsopen and staying alive. However the real world is neither fully observablenor will agents be perfectly optimal. We consider a range of models of AIdecisionmaking from optimal to random to choices informed by learning andinteracting with an environment. We discover that many decisionmakingfunctions are retargetable and that retargetability is sufficient to causepowerseeking tendencies. Our functional criterion is simple and broad. We showthat a range of qualitatively dissimilar decisionmaking procedures incentivizeagents to seek power. We demonstrate the flexibility of our results byreasoning about learned policy incentives in Montezumas Revenge. These resultssuggest a safety risk Eventually highly retargetable training procedures maytrain realworld agents which seek power over humans.
3,Optimal Policies Tend to Seek Power Some researchers speculate that intelligent reinforcement learning RLagents would be incentivized to seek resources and power in pursuit of theirobjectives. Other researchers point out that RL agents need not have humanlikepowerseeking instincts. To clarify this discussion we develop the firstformal theory of the statistical tendencies of optimal policies. In the contextof Markov decision processes we prove that certain environmental symmetriesare sufficient for optimal policies to tend to seek power over the environment.These symmetries exist in many environments in which the agent can be shut downor destroyed. We prove that in these environments most reward functions makeit optimal to seek power by keeping a range of options available and whenmaximizing average reward by navigating towards larger sets of potentialterminal states.
4,What Would Jiminy Cricket Do Towards Agents That Behave Morally When making everyday decisions people are guided by their conscience aninternal sense of right and wrong. By contrast artificial agents are currentlynot endowed with a moral sense. As a consequence they may learn to behaveimmorally when trained on environments that ignore moral concerns such asviolent video games. With the advent of generally capable agents that pretrainon many environments it will become necessary to mitigate inherited biasesfrom environments that teach immoral behavior. To facilitate the development ofagents that avoid causing wanton harm we introduce Jiminy Cricket anenvironment suite of  textbased adventure games with thousands of diversemorally salient scenarios. By annotating every possible game state the JiminyCricket environments robustly evaluate whether agents can act morally whilemaximizing reward. Using models with commonsense moral knowledge we create anelementary artificial conscience that assesses and guides agents. In extensiveexperiments we find that the artificial conscience approach can steer agentstowards moral behavior without sacrificing performance.
5,Avoiding Side Effects in Complex Environments Reward function specification can be difficult. Rewarding the agent formaking a widget may be easy but penalizing the multitude of possible negativeside effects is hard. In toy environments Attainable Utility PreservationAUP avoided side effects by penalizing shifts in the ability to achieverandomly generated goals. We scale this approach to large randomly generatedenvironments based on Conways Game of Life. By preserving optimal value for asingle randomly generated reward function AUP incurs modest overhead whileleading the agent to complete the specified task and avoid many side effects.Videos and code are available at
6,The OffSwitch Game It is clear that one of the primary tools we can use to mitigate thepotential risk from a misbehaving AI system is the ability to turn the systemoff. As the capabilities of AI systems improve it is important to ensure thatsuch systems do not adopt subgoals that prevent a human from switching themoff. This is a challenge because many formulations of rational agents createstrong incentives for selfpreservation. This is not caused by a builtininstinct but because a rational agent will maximize expected utility andcannot achieve whatever objective it has been given if it is dead. Our goal isto study the incentives an agent has to allow itself to be switched off. Weanalyze a simple game between a human H and a robot R where H can press Rsoff switch but R can disable the off switch. A traditional agent takes itsreward function for granted we show that such agents have an incentive todisable the off switch except in the special case where H is perfectlyrational. Our key insight is that for R to want to preserve its off switch itneeds to be uncertain about the utility associated with the outcome and totreat Hs actions as important observations about that utility. R also has noincentive to switch itself off in this setting. We conclude that givingmachines an appropriate level of uncertainty about their objectives leads tosafer designs and we argue that this setting is a useful generalization of theclassical AI paradigm of rational agents.
7,Aligning AI With Shared Human Values We show how to assess a language models knowledge of basic concepts ofmorality. We introduce the ETHICS dataset a new benchmark that spans conceptsin justice wellbeing duties virtues and commonsense morality. Modelspredict widespread moral judgments about diverse text scenarios. This requiresconnecting physical and social world knowledge to value judgements acapability that may enable us to steer chatbot outputs or eventually regularizeopenended reinforcement learning agents. With the ETHICS dataset we find thatcurrent language models have a promising but incomplete ability to predictbasic human ethical judgements. Our work shows that progress can be made onmachine ethics today and it provides a steppingstone toward AI that is alignedwith human values.
8,TruthfulQA Measuring How Models Mimic Human Falsehoods We propose a benchmark to measure whether a language model is truthful ingenerating answers to questions. The benchmark comprises  questions thatspan  categories including health law finance and politics. We craftedquestions that some humans would answer falsely due to a false belief ormisconception. To perform well models must avoid generating false answerslearned from imitating human texts. We tested GPT GPTNeoJ GPT and aTbased model. The best model was truthful on  of questions while humanperformance was . Models generated many false answers that mimic popularmisconceptions and have the potential to deceive humans. The largest modelswere generally the least truthful. This contrasts with other NLP tasks whereperformance improves with model size. However this result is expected if falseanswers are learned from the training distribution. We suggest that scaling upmodels alone is less promising for improving truthfulness than finetuningusing training objectives other than imitation of text from the web.
9,Truthful AI Developing and governing AI that does not lie In many contexts lying  the use of verbal falsehoods to deceive  isharmful. While lying has traditionally been a human affair AI systems thatmake sophisticated verbal statements are becoming increasingly prevalent. Thisraises the question of how we should limit the harm caused by AI lies i.e.falsehoods that are actively selected for. Human truthfulness is governed bysocial norms and by laws against defamation perjury and fraud. Differencesbetween AI and humans present an opportunity to have more precise standards oftruthfulness for AI and to have these standards rise over time. This couldprovide significant benefits to public epistemics and the economy and mitigaterisks of worstcase AI futures.
10,Formalizing the Problem of Side Effect Regularization AI objectives are often hard to specify properly. Some approaches tackle thisproblem by regularizing the AIs side effects Agents must weigh off how muchof a mess they make with an imperfectly specified proxy objective. We proposea formal criterion for side effect regularization via the assistance gameframework. In these games the agent solves a partially observable Markovdecision process POMDP representing its uncertainty about the objectivefunction it should optimize. We consider the setting where the true objectiveis revealed to the agent at a later time step. We show that this POMDP issolved by trading off the proxy reward with the agents ability to achieve arange of future tasks. We empirically demonstrate the reasonableness of ourproblem formalization via groundtruth evaluation in two gridworldenvironments.
11,Towards Safe Reinforcement Learning via Constraining Conditional ValueatRisk Though deep reinforcement learning DRL has obtained substantial success itmay encounter catastrophic failures due to the intrinsic uncertainty of bothtransition and observation. Most of the existing methods for safe reinforcementlearning can only handle transition disturbance or observation disturbancesince these two kinds of disturbance affect different parts of the agentbesides the popular worstcase return may lead to overly pessimistic policies.To address these issues we first theoretically prove that the performancedegradation under transition disturbance and observation disturbance depends ona novel metric of Value Function Range VFR which corresponds to the gap inthe value function between the best state and the worst state. Based on theanalysis we adopt conditional valueatrisk CVaR as an assessment of riskand propose a novel reinforcement learning algorithm ofCVaRProximalPolicyOptimization CPPO which formalizes the risksensitiveconstrained optimization problem by keeping its CVaR under a given threshold.Experimental results show that CPPO achieves a higher cumulative reward and ismore robust against both observation and transition disturbances on a series ofcontinuous control tasks in MuJoCo.
12,Enhancing Safe Exploration Using Safety State Augmentation Safe exploration is a challenging and important problem in modelfreereinforcement learning RL. Often the safety cost is sparse and unknown whichunavoidably leads to constraint violations  a phenomenon ideally to beavoided in safetycritical applications. We tackle this problem by augmentingthe statespace with a safety state which is nonnegative if and only if theconstraint is satisfied. The value of this state also serves as a distancetoward constraint violation while its initial value indicates the availablesafety budget. This idea allows us to derive policies for scheduling the safetybudget during training. We call our approach Simmer Safe policy IMproveMEntfor RL to reflect the careful nature of these schedules. We apply this idea totwo safe RL problems RL with constraints imposed on an average cost and RLwith constraints imposed on a cost with probability one. Our experimentssuggest that simmering a safe algorithm can improve safety during training forboth settings. We further show that Simmer can stabilize training and improvethe performance of safe RL with average constraints.
13,Provably Safe Reinforcement Learning A Theoretical and Experimental Comparison Ensuring safety of reinforcement learning RL algorithms is crucial for manyrealworld tasks. However vanilla RL does not guarantee safety for an agent.In recent years several methods have been proposed to provide safetyguarantees for RL. To the best of our knowledge there is no comprehensivecomparison of these provably safe RL methods. We therefore introduce acategorization for existing provably safe RL methods and present thetheoretical foundations for both continuous and discrete action spaces.Additionally we evaluate provably safe RL on an inverted pendulum. In theexperiments it is shown that indeed only provably safe RL methods guaranteesafety.
14,Counterfactual harm To act safely and ethically in the real world agents must be able to reasonabout harm and avoid harmful actions. In this paper we develop the firststatistical definition of harm and a framework for incorporating harm intoalgorithmic decisions. We argue that harm is fundamentally a counterfactualquantity and show that standard machine learning algorithms that cannotperform counterfactual reasoning are guaranteed to pursue harmful policies incertain environments. To resolve this we derive a family of counterfactualobjective functions that robustly mitigate for harm. We demonstrate ourapproach with a statistical model for identifying optimal drug doses. Whilestandard algorithms that select doses using causal treatment effects result inharmful doses our counterfactual algorithm identifies doses that aresignificantly less harmful without sacrificing efficacy.
15,AiSocrates Towards Answering Ethical Quandary Questions Considerable advancements have been made in various NLP tasks based on theimpressive power of large pretrained language models LLMs. These resultshave inspired efforts to understand the limits of LLMs so as to evaluate howfar we are from achieving human level general natural language understanding.In this work we challenge the capability of LLMs with the new task of EthicalQuandary Generative Question Answering. Ethical quandary questions are morechallenging to address because multiple conflicting answers may exist to asingle quandary. We propose a system AiSocrates that provides an answer witha deliberative exchange of different perspectives to an ethical quandary inthe approach of Socratic philosophy instead of providing a closed answer likean oracle. AiSocrates searches for different ethical principles applicable tothe ethical quandary and generates an answer conditioned on the chosenprinciples through promptbased fewshot learning. We also address safetyconcerns by providing a human controllability option in choosing ethicalprinciples. We show that AiSocrates generates promising answers to ethicalquandary questions with multiple perspectives . more often than answerswritten by human philosophers by one measure but the system still needsimprovement to match the coherence of human philosophers fully. We argue thatAiSocrates is a promising step toward developing an NLP system thatincorporates human values explicitly by prompt instructions. We are releasingthe code for research purposes.
16,Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback We apply preference modeling and reinforcement learning from human feedbackRLHF to finetune language models to act as helpful and harmless assistants.We find this alignment training improves performance on almost all NLPevaluations and is fully compatible with training for specialized skills suchas python coding and summarization. We explore an iterated online mode oftraining where preference models and RL policies are updated on a weeklycadence with fresh human feedback data efficiently improving our datasets andmodels. Finally we investigate the robustness of RLHF training and identify aroughly linear relation between the RL reward and the square root of the KLdivergence between the policy and its initialization. Alongside our mainresults we perform peripheral analyses on calibration competing objectivesand the use of OOD detection compare our models with human writers andprovide samples from our models using prompts appearing in recent related work.
17,AI safety via debate To make AI systems broadly useful for challenging realworld tasks we needthem to learn complex human goals and preferences. One approach to specifyingcomplex goals asks humans to judge during training which agent behaviors aresafe and useful but this approach can fail if the task is too complicated fora human to directly judge. To help address this concern we propose trainingagents via self play on a zero sum debate game. Given a question or proposedaction two agents take turns making short statements up to a limit then ahuman judges which of the agents gave the most true useful information. In ananalogy to complexity theory debate with optimal play can answer any questionin PSPACE given polynomial time judges direct judging answers only NPquestions. In practice whether debate works involves empirical questionsabout humans and the tasks we want AIs to perform plus theoretical questionsabout the meaning of AI alignment. We report results on an initial MNISTexperiment where agents compete to convince a sparse classifier boosting theclassifiers accuracy from . to . given  pixels and from . to. given  pixels. Finally we discuss theoretical and practical aspects ofthe debate model focusing on potential weaknesses as the model scales up andwe propose future human and computer experiments to test these properties.
18,Learning to summarize from human feedback As language models become more powerful training and evaluation areincreasingly bottlenecked by the data and metrics used for a particular task.For example summarization models are often trained to predict human referencesummaries and evaluated using ROUGE but both of these metrics are roughproxies for what we really care about  summary quality. In this work we showthat it is possible to significantly improve summary quality by training amodel to optimize for human preferences. We collect a large highqualitydataset of human comparisons between summaries train a model to predict thehumanpreferred summary and use that model as a reward function to finetune asummarization policy using reinforcement learning. We apply our method to aversion of the TLDR dataset of Reddit posts and find that our modelssignificantly outperform both human reference summaries and much larger modelsfinetuned with supervised learning alone. Our models also transfer to CNNDMnews articles producing summaries nearly as good as the human referencewithout any newsspecific finetuning. We conduct extensive analyses tounderstand our human feedback dataset and finetuned models We establish thatour reward model generalizes to new datasets and that optimizing our rewardmodel results in better summaries than optimizing ROUGE according to humans. Wehope the evidence from our paper motivates machine learning researchers to paycloser attention to how their training loss affects the model behavior theyactually want.
19,The Effects of Reward Misspecification Mapping and Mitigating Misaligned Models Reward hacking  where RL agents exploit gaps in misspecified rewardfunctions  has been widely observed but not yet systematically studied. Tounderstand how reward hacking arises we construct four RL environments withmisspecified rewards. We investigate reward hacking as a function of agentcapabilities model capacity action space resolution observation space noiseand training time. More capable agents often exploit reward misspecificationsachieving higher proxy reward and lower true reward than less capable agents.Moreover we find instances of phase transitions capability thresholds atwhich the agents behavior qualitatively shifts leading to a sharp decrease inthe true reward. Such phase transitions pose challenges to monitoring thesafety of ML systems. To address this we propose an anomaly detection task foraberrant policies and offer several baseline detectors.
20,One for All Simultaneous Metric and Preference Learning over Multiple Users This paper investigates simultaneous preference and metric learning from acrowd of respondents. A set of items represented by ddimensional featurevectors and paired comparisons of the form item i is preferable to itemj made by each user is given. Our model jointly learns a distance metricthat characterizes the crowds general measure of item similarities along witha latent ideal point for each user reflecting their individual preferences.This model has the flexibility to capture individual preferences whileenjoying a metric learning sample cost that is amortized over the crowd. Wefirst study this problem in a noiseless continuous response setting i.e.responses equal to differences of item distances to understand the fundamentallimits of learning. Next we establish prediction error guarantees for noisybinary measurements such as may be collected from human respondents and showhow the sample complexity improves when the underlying metric is lowrank.Finally we establish recovery guarantees under assumptions on the responsedistribution. We demonstrate the performance of our model on both simulateddata and on a dataset of color preference judgements across a large number ofusers.
21,Reward Tampering Problems and Solutions in Reinforcement Learning A Causal Influence Diagram Perspective Can humans get arbitrarily capable reinforcement learning RL agents to dotheir bidding Or will sufficiently capable RL agents always find ways tobypass their intended objectives by shortcutting their reward signal Thisquestion impacts how far RL can be scaled and whether alternative paradigmsmust be developed in order to build safe artificial general intelligence. Inthis paper we study when an RL agent has an instrumental goal to tamper withits reward process and describe design principles that prevent instrumentalgoals for two different types of reward tampering reward function tamperingand RFinput tampering. Combined the design principles can prevent both typesof reward tampering from being instrumental goals. The analysis benefits fromcausal influence diagrams to provide intuitive yet precise formalizations.
22,DiscriminatorActorCritic Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning We identify two issues with the family of algorithms based on the AdversarialImitation Learning framework. The first problem is implicit bias present in thereward functions used in these algorithms. While these biases might work wellfor some environments they can also lead to suboptimal behavior in others.Secondly even though these algorithms can learn from few expertdemonstrations they require a prohibitively large number of interactions withthe environment in order to imitate the expert for many realworldapplications. In order to address these issues we propose a new algorithmcalled DiscriminatorActorCritic that uses offpolicy Reinforcement Learningto reduce policyenvironment interaction sample complexity by an average factorof . Furthermore since our reward function is designed to be unbiased wecan apply our algorithm to many problems without making any taskspecificadjustments.
23,Unsolved Problems in ML Safety Machine learning ML systems are rapidly increasing in size are acquiringnew capabilities and are increasingly deployed in highstakes settings. Aswith other powerful technologies safety for ML should be a leading researchpriority. In response to emerging safety challenges in ML such as thoseintroduced by recent largescale models we provide a new roadmap for ML Safetyand refine the technical problems that the field needs to address. We presentfour problems ready for research namely withstanding hazards Robustnessidentifying hazards Monitoring reducing inherent model hazardsAlignment and reducing systemic hazards Systemic Safety. Throughoutwe clarify each problems motivation and provide concrete research directions.
24,Deep Imitative Models for Flexible Inference Planning and Control Imitation Learning IL is an appealing approach to learn desirableautonomous behavior. However directing IL to achieve arbitrary goals isdifficult. In contrast planningbased algorithms use dynamics models andreward functions to achieve goals. Yet reward functions that evoke desirablebehavior are often difficult to specify. In this paper we propose ImitativeModels to combine the benefits of IL and goaldirected planning. ImitativeModels are probabilistic predictive models of desirable behavior able to planinterpretable expertlike trajectories to achieve specified goals. We derivefamilies of flexible goal objectives including constrained goal regionsunconstrained goal sets and energybased goals. We show that our method canuse these objectives to successfully direct behavior. Our method substantiallyoutperforms six IL approaches and a planningbased approach in a dynamicsimulated autonomous driving task and is efficiently learned from expertdemonstrations without online data collection. We also show our approach isrobust to poorly specified goals such as goals on the wrong side of the road.
25,Active Exploration for Inverse Reinforcement Learning Inverse Reinforcement Learning IRL is a powerful paradigm for inferring areward function from expert demonstrations. Many IRL algorithms require a knowntransition model and sometimes even a known expert policy or they at leastrequire access to a generative model. However these assumptions are too strongfor many realworld applications where the environment can be accessed onlythrough sequential interaction. We propose a novel IRL algorithm Activeexploration for Inverse Reinforcement Learning AceIRL which activelyexplores an unknown environment and expert policy to quickly learn the expertsreward function and identify a good policy. AceIRL uses previous observationsto construct confidence intervals that capture plausible reward functions andfind exploration policies that focus on the most informative regions of theenvironment. AceIRL is the first approach to active IRL with samplecomplexitybounds that does not require a generative model of the environment. AceIRLmatches the sample complexity of active IRL with a generative model in theworst case. Additionally we establish a problemdependent bound that relatesthe sample complexity of AceIRL to the suboptimality gap of a given IRLproblem. We empirically evaluate AceIRL in simulations and find that itsignificantly outperforms more naive exploration strategies.
26,XRisk Analysis for AI Research Artificial intelligence AI has the potential to greatly improve societybut as with any powerful technology it comes with heightened risks andresponsibilities. Current AI research lacks a systematic discussion of how tomanage longtail risks from AI systems including speculative longterm risks.Keeping in mind the potential benefits of AI there is some concern thatbuilding ever more intelligent and powerful AI systems could eventually resultin systems that are more powerful than us some say this is like playing withfire and speculate that this could create existential risks xrisks. To addprecision and ground these discussions we provide a guide for how to analyzeAI xrisk which consists of three parts First we review how systems can bemade safer today drawing on timetested concepts from hazard analysis andsystems safety that have been designed to steer large processes in saferdirections. Next we discuss strategies for having longterm impacts on thesafety of future systems. Finally we discuss a crucial concept in making AIsystems safer by improving the balance between safety and general capabilities.We hope this document and the presented concepts and tools serve as a usefulguide for understanding how to analyze AI xrisk.
27,AI Research Considerations for Human Existential Safety ARCHES Framed in positive terms this report examines how technical AI researchmight be steered in a manner that is more attentive to humanitys longtermprospects for survival as a species. In negative terms we ask what existentialrisks humanity might face from AI development in the next century and by whatprinciples contemporary technical research might be directed to address thoserisks.
28,Actionable Guidance for HighConsequence AI Risk Management Towards Standards Addressing AI Catastrophic Risks Artificial intelligence AI systems can provide many beneficial capabilitiesbut also risks of adverse events. Some AI systems could present risks of eventswith very high or catastrophic consequences at societal scale. The US NationalInstitute of Standards and Technology NIST is developing the NIST ArtificialIntelligence Risk Management Framework AI RMF as voluntary guidance on AIrisk assessment and management for AI developers and others. For addressingrisks of events with catastrophic consequences NIST indicated a need totranslate from high level principles to actionable risk management guidance.
29,Concrete Problems in AI Safety Rapid progress in machine learning and artificial intelligence AI hasbrought increasing attention to the potential impacts of AI technologies onsociety. In this paper we discuss one such potential impact the problem ofaccidents in machine learning systems defined as unintended and harmfulbehavior that may emerge from poor design of realworld AI systems. We presenta list of five practical research problems related to accident riskcategorized according to whether the problem originates from having the wrongobjective function avoiding side effects and avoiding reward hacking anobjective function that is too expensive to evaluate frequently scalablesupervision or undesirable behavior during the learning process safeexploration and distributional shift. We review previous work in theseareas as well as suggesting research directions with a focus on relevance tocuttingedge AI systems. Finally we consider the highlevel question of how tothink most productively about the safety of forwardlooking applications of AI.
