,text
0,Deep Anomaly Detection with Outlier Exposure It is important to detect anomalous inputs when deploying machine learningsystems. The use of larger and more complex inputs in deep learning magnifiesthe difficulty of distinguishing between anomalous and indistributionexamples. At the same time diverse image and text data are available inenormous quantities. We propose leveraging these data to improve deep anomalydetection by training anomaly detectors against an auxiliary dataset ofoutliers an approach we call Outlier Exposure OE. This enables anomalydetectors to generalize and detect unseen anomalies. In extensive experimentson natural language processing and small and largescale vision tasks we findthat Outlier Exposure significantly improves detection performance. We alsoobserve that cuttingedge generative models trained on CIFAR may assignhigher likelihoods to SVHN images than to CIFAR images we use OE tomitigate this issue. We also analyze the flexibility and robustness of OutlierExposure and identify characteristics of the auxiliary dataset that improveperformance.
1,PixMix Dreamlike Pictures Comprehensively Improve Safety Measures In realworld applications of machine learning reliable and safe systemsmust consider measures of performance beyond standard test set accuracy. Theseother goals include outofdistribution OOD robustness predictionconsistency resilience to adversaries calibrated uncertainty estimates andthe ability to detect anomalous inputs. However improving performance towardsthese goals is often a balancing act that todays methods cannot achievewithout sacrificing performance on other safety axes. For instance adversarialtraining improves adversarial robustness but sharply degrades other classifierperformance metrics. Similarly strong data augmentation and regularizationtechniques often improve OOD robustness but harm anomaly detection raising thequestion of whether a Pareto improvement on all existing safety measures ispossible. To meet this challenge we design a new data augmentation strategyutilizing the natural structural complexity of pictures such as fractals whichoutperforms numerous baselines is near Paretooptimal and roundly improvessafety measures.
2,ViM OutOfDistribution with Virtuallogit Matching Most of the existing OutOfDistribution OOD detection algorithms depend onsingle input source the feature the logit or the softmax probability.However the immense diversity of the OOD examples makes such methods fragile.There are OOD samples that are easy to identify in the feature space while hardto distinguish in the logit space and vice versa. Motivated by thisobservation we propose a novel OOD scoring method named Virtuallogit MatchingViM which combines the classagnostic score from feature space and theInDistribution ID classdependent logits. Specifically an additional logitrepresenting the virtual OOD class is generated from the residual of thefeature against the principal space and then matched with the original logitsby a constant scaling. The probability of this virtual logit after softmax isthe indicator of OODness. To facilitate the evaluation of largescale OODdetection in academia we create a new OOD dataset for ImageNetK which ishumanannotated and is .x the size of existing datasets. We conductedextensive experiments including CNNs and vision transformers to demonstratethe effectiveness of the proposed ViM score. In particular using the BiTSmodel our method gets an average AUROC . on four difficult OODbenchmarks which is  ahead of the best baseline. Code and dataset areavailable at
3,Scaling OutofDistribution Detection for RealWorld Settings Detecting outofdistribution examples is important for safetycriticalmachine learning applications such as detecting novel biological phenomena andselfdriving cars. However existing research mainly focuses on simplesmallscale settings. To set the stage for more realistic outofdistributiondetection we depart from smallscale settings and explore largescalemulticlass and multilabel settings with highresolution images and thousandsof classes. To make future work in realworld settings possible we create newbenchmarks for three largescale settings. To test ImageNet multiclass anomalydetectors we introduce the Species dataset containing over  images andover a thousand anomalous species. We leverage ImageNetK to evaluate PASCALVOC and COCO multilabel anomaly detectors. Third we introduce a new benchmarkfor anomaly segmentation by introducing a segmentation benchmark with roadanomalies. We conduct extensive experiments in these more realistic settingsfor outofdistribution detection and find that a surprisingly simple detectorbased on the maximum logit outperforms prior methods in all the largescalemulticlass multilabel and segmentation tasks establishing a simple newbaseline for future work.
4,A Baseline for Detecting Misclassified and OutofDistribution Examples in Neural Networks We consider the two related problems of detecting if an example ismisclassified or outofdistribution. We present a simple baseline thatutilizes probabilities from softmax distributions. Correctly classifiedexamples tend to have greater maximum softmax probabilities than erroneouslyclassified and outofdistribution examples allowing for their detection. Weassess performance by defining several tasks in computer vision naturallanguage processing and automatic speech recognition showing theeffectiveness of this baseline across all. We then show the baseline cansometimes be surpassed demonstrating the room for future research on theseunderexplored detection tasks.
5,On Calibration of Modern Neural Networks Confidence calibration  the problem of predicting probability estimatesrepresentative of the true correctness likelihood  is important forclassification models in many applications. We discover that modern neuralnetworks unlike those from a decade ago are poorly calibrated. Throughextensive experiments we observe that depth width weight decay and BatchNormalization are important factors influencing calibration. We evaluate theperformance of various postprocessing calibration methods on stateoftheartarchitectures with image and document classification datasets. Our analysis andexperiments not only offer insights into neural network learning but alsoprovide a simple and straightforward recipe for practical settings on mostdatasets temperature scaling  a singleparameter variant of Platt Scaling is surprisingly effective at calibrating predictions.
6,Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles Deep neural networks NNs are powerful black box predictors that haverecently achieved impressive performance on a wide spectrum of tasks.Quantifying predictive uncertainty in NNs is a challenging and yet unsolvedproblem. Bayesian NNs which learn a distribution over weights are currentlythe stateoftheart for estimating predictive uncertainty however theserequire significant modifications to the training procedure and arecomputationally expensive compared to standard nonBayesian NNs. We proposean alternative to Bayesian NNs that is simple to implement readilyparallelizable requires very little hyperparameter tuning and yields highquality predictive uncertainty estimates. Through a series of experiments onclassification and regression benchmarks we demonstrate that our methodproduces wellcalibrated uncertainty estimates which are as good or better thanapproximate Bayesian NNs. To assess robustness to dataset shift we evaluatethe predictive uncertainty on test examples from known and unknowndistributions and show that our method is able to express higher uncertaintyon outofdistribution examples. We demonstrate the scalability of our methodby evaluating predictive uncertainty estimates on ImageNet.
7,A Simple Unified Framework for Detecting OutofDistribution Samples and Adversarial Attacks Detecting test samples drawn sufficiently far away from the trainingdistribution statistically or adversarially is a fundamental requirement fordeploying a good classifier in many realworld machine learning applications.However deep neural networks with the softmax classifier are known to producehighly overconfident posterior distributions even for such abnormal samples. Inthis paper we propose a simple yet effective method for detecting any abnormalsamples which is applicable to any pretrained softmax neural classifier. Weobtain the class conditional Gaussian distributions with respect to low andupperlevel features of the deep models under Gaussian discriminant analysiswhich result in a confidence score based on the Mahalanobis distance. Whilemost prior methods have been evaluated for detecting either outofdistributionor adversarial samples but not both the proposed method achieves thestateoftheart performances for both cases in our experiments. Moreover wefound that our proposed method is more robust in harsh cases e.g. when thetraining dataset has noisy labels or small number of samples. Finally we showthat the proposed method enjoys broader usage by applying it toclassincremental learning whenever outofdistribution samples are detectedour classification rule can incorporate new classes well without furthertraining deep models.
8,VOS Learning What You Dont Know by Virtual Outlier Synthesis Outofdistribution OOD detection has received much attention lately due toits importance in the safe deployment of neural networks. One of the keychallenges is that models lack supervision signals from unknown data and as aresult can produce overconfident predictions on OOD data. Previous approachesrely on real outlier datasets for model regularization which can be costly andsometimes infeasible to obtain in practice. In this paper we present VOS anovel framework for OOD detection by adaptively synthesizing virtual outliersthat can meaningfully regularize the models decision boundary during training.Specifically VOS samples virtual outliers from the lowlikelihood region ofthe classconditional distribution estimated in the feature space. Alongsidewe introduce a novel unknownaware training objective which contrastivelyshapes the uncertainty space between the ID data and synthesized outlier data.VOS achieves competitive performance on both object detection and imageclassification models reducing the FPR by up to . compared to theprevious best method on object detectors. Code is available at
9,Posterior calibration and exploratory analysis for natural language processing models Many models in natural language processing define probabilistic distributionsover linguistic structures. We argue that  the quality of a model sposterior distribution can and should be directly evaluated as to whetherprobabilities correspond to empirical frequencies and  NLP uncertainty canbe projected not only to pipeline components but also to exploratory dataanalysis telling a user when to trust and not trust the NLP analysis. Wepresent a method to analyze calibration and apply it to compare themiscalibration of several commonly used models. We also contribute acoreference sampling algorithm that can create confidence intervals for apolitical event extraction task.
10,Accurate Uncertainties for Deep Learning Using Calibrated Regression Methods for reasoning under uncertainty are a key building block of accurateand reliable machine learning systems. Bayesian methods provide a generalframework to quantify uncertainty. However because of model misspecificationand the use of approximate inference Bayesian uncertainty estimates are ofteninaccurate  for example a  credible interval may not contain the trueoutcome  of the time. Here we propose a simple procedure for calibratingany regression algorithm when applied to Bayesian and probabilistic models itis guaranteed to produce calibrated uncertainty estimates given enough data.Our procedure is inspired by Platt scaling and extends previous work onclassification. We evaluate this approach on Bayesian linear regressionfeedforward and recurrent neural networks and find that it consistentlyoutputs wellcalibrated credible intervals while improving performance on timeseries forecasting and modelbased reinforcement learning tasks.
11,Can You Trust Your Models Uncertainty Evaluating Predictive Uncertainty Under Dataset Shift Modern machine learning methods including deep learning have achieved greatsuccess in predictive accuracy for supervised learning tasks but may stillfall short in giving useful estimates of their predictive em uncertainty.Quantifying uncertainty is especially critical in realworld settings whichoften involve input distributions that are shifted from the trainingdistribution due to a variety of factors including sample bias andnonstationarity. In such settings well calibrated uncertainty estimatesconvey information about when a models output should or should not betrusted. Many probabilistic deep learning methods including BayesianandnonBayesian methods have been proposed in the literature for quantifyingpredictive uncertainty but to our knowledge there has not previously been arigorous largescale empirical comparison of these methods under dataset shift.We present a largescale benchmark of existing stateoftheart methods onclassification problems and investigate the effect of dataset shift on accuracyand calibration. We find that traditional posthoc calibration does indeed fallshort as do several other previous methods. However some methods thatmarginalize over models give surprisingly strong results across a broadspectrum of tasks.
12,The Mythos of Model Interpretability Supervised machine learning models boast remarkable predictive capabilities.But can you trust your model Will it work in deployment What else can it tellyou about the world We want models to be not only good but interpretable. Andyet the task of interpretation appears underspecified. Papers provide diverseand sometimes nonoverlapping motivations for interpretability and offermyriad notions of what attributes render models interpretable. Despite thisambiguity many papers proclaim interpretability axiomatically absent furtherexplanation. In this paper we seek to refine the discourse oninterpretability. First we examine the motivations underlying interest ininterpretability finding them to be diverse and occasionally discordant. Thenwe address model properties and techniques thought to confer interpretabilityidentifying transparency to humans and posthoc explanations as competingnotions. Throughout we discuss the feasibility and desirability of differentnotions and question the oftmade assertions that linear models areinterpretable and that deep neural networks are not.
13,Sanity Checks for Saliency Maps Saliency methods have emerged as a popular tool to highlight features in aninput deemed relevant for the prediction of a learned model. Several saliencymethods have been proposed often guided by visual appeal on image data. Inthis work we propose an actionable methodology to evaluate what kinds ofexplanations a given method can and cannot provide. We find that reliancesolely on visual assessment can be misleading. Through extensive experimentswe show that some existing saliency methods are independent both of the modeland of the data generating process. Consequently methods that fail theproposed tests are inadequate for tasks that are sensitive to either data ormodel such as finding outliers in the data explaining the relationshipbetween inputs and outputs that the model learned and debugging the model. Weinterpret our findings through an analogy with edge detection in images atechnique that requires neither training data nor model. Theory in the case ofa linear model and a singlelayer convolutional neural network supports ourexperimental findings.
14,Locating and Editing Factual Associations in GPT We analyze the storage and recall of factual associations in autoregressivetransformer language models finding evidence that these associationscorrespond to localized directlyeditable computations. We first develop acausal intervention for identifying neuron activations that are decisive in amodels factual predictions. This reveals a distinct set of steps inmiddlelayer feedforward modules that mediate factual predictions whileprocessing subject tokens. To test our hypothesis that these computationscorrespond to factual association recall we modify feedforward weights toupdate specific factual associations using RankOne Model Editing ROME. Wefind that ROME is effective on a standard zeroshot relation extraction zsREmodelediting task comparable to existing methods. To perform a more sensitiveevaluation we also evaluate ROME on a new dataset of counterfactualassertions on which it simultaneously maintains both specificity andgeneralization whereas other methods sacrifice one or another. Our resultsconfirm an important role for midlayer feedforward modules in storing factualassociations and suggest that direct manipulation of computational mechanismsmay be a feasible approach for model editing. The code datasetvisualizations and an interactive demo notebook are available at
15,Interpretable Explanations of Black Boxes by Meaningful Perturbation As machine learning algorithms are increasingly applied to high impact yethigh risk tasks such as medical diagnosis or autonomous driving it iscritical that researchers can explain how such algorithms arrived at theirpredictions. In recent years a number of image saliency methods have beendeveloped to summarize where highly complex neural networks look in an imagefor evidence for their predictions. However these techniques are limited bytheir heuristic nature and architectural constraints. In this paper we maketwo main contributions First we propose a general framework for learningdifferent kinds of explanations for any black box algorithm. Second wespecialise the framework to find the part of an image most responsible for aclassifier decision. Unlike previous works our method is modelagnostic andtestable because it is grounded in explicit and interpretable imageperturbations.
16,Acquisition of Chess Knowledge in AlphaZero What is learned by sophisticated neural network agents such as AlphaZeroThis question is of both scientific and practical interest. If therepresentations of strong neural networks bear no resemblance to humanconcepts our ability to understand faithful explanations of their decisionswill be restricted ultimately limiting what we can achieve with neural networkinterpretability. In this work we provide evidence that human knowledge isacquired by the AlphaZero neural network as it trains on the game of chess. Byprobing for a broad range of human chess concepts we show when and where theseconcepts are represented in the AlphaZero network. We also provide abehavioural analysis focusing on opening play including qualitative analysisfrom chess Grandmaster Vladimir Kramnik. Finally we carry out a preliminaryinvestigation looking at the lowlevel details of AlphaZeros representationsand make the resulting behavioural and representational analyses availableonline.
17,Network Dissection Quantifying Interpretability of Deep Visual Representations We propose a general framework called Network Dissection for quantifying theinterpretability of latent representations of CNNs by evaluating the alignmentbetween individual hidden units and a set of semantic concepts. Given any CNNmodel the proposed method draws on a broad data set of visual concepts toscore the semantics of hidden units at each intermediate convolutional layer.The units with semantics are given labels across a range of objects partsscenes textures materials and colors. We use the proposed method to test thehypothesis that interpretability of units is equivalent to random linearcombinations of units then we apply our method to compare the latentrepresentations of various networks when trained to solve different supervisedand selfsupervised training tasks. We further analyze the effect of trainingiterations compare networks trained with different initializations examinethe impact of network depth and width and measure the effect of dropout andbatch normalization on the interpretability of deep visual representations. Wedemonstrate that the proposed method can shed light on characteristics of CNNmodels and training methods that go beyond measurements of their discriminativepower.
18,Network Dissection Quantifying Interpretability of Deep Visual Representations We propose a general framework called Network Dissection for quantifying theinterpretability of latent representations of CNNs by evaluating the alignmentbetween individual hidden units and a set of semantic concepts. Given any CNNmodel the proposed method draws on a broad data set of visual concepts toscore the semantics of hidden units at each intermediate convolutional layer.The units with semantics are given labels across a range of objects partsscenes textures materials and colors. We use the proposed method to test thehypothesis that interpretability of units is equivalent to random linearcombinations of units then we apply our method to compare the latentrepresentations of various networks when trained to solve different supervisedand selfsupervised training tasks. We further analyze the effect of trainingiterations compare networks trained with different initializations examinethe impact of network depth and width and measure the effect of dropout andbatch normalization on the interpretability of deep visual representations. Wedemonstrate that the proposed method can shed light on characteristics of CNNmodels and training methods that go beyond measurements of their discriminativepower.
19,Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead Black box machine learning models are currently being used for high stakesdecisionmaking throughout society causing problems throughout healthcarecriminal justice and in other domains. People have hoped that creating methodsfor explaining these black box models will alleviate some of these problemsbut trying to textitexplain black box models rather than creating modelsthat are textitinterpretable in the first place is likely to perpetuate badpractices and can potentially cause catastrophic harm to society. There is away forward  it is to design models that are inherently interpretable. Thismanuscript clarifies the chasm between explaining black boxes and usinginherently interpretable models outlines several key reasons why explainableblack boxes should be avoided in highstakes decisions identifies challengesto interpretable machine learning and provides several example applicationswhere interpretable models could potentially replace black box models incriminal justice healthcare and computer vision.
20,Convergent Learning Do different neural networks learn the same representations Recent success in training deep neural networks have prompted activeinvestigation into the features learned on their intermediate layers. Suchresearch is difficult because it requires making sense of nonlinearcomputations performed by millions of parameters but valuable because itincreases our ability to understand current models and create improved versionsof them. In this paper we investigate the extent to which neural networksexhibit what we call convergent learning which is when the representationslearned by multiple nets converge to a set of features which are eitherindividually similar between networks or where subsets of features span similarlowdimensional spaces. We propose a specific method of probingrepresentations training multiple networks and then comparing and contrastingtheir individual learned representations at the level of neurons or groups ofneurons. We begin research into this question using three techniques toapproximately align different neural networks on a feature level a bipartitematching approach that makes onetoone assignments between neurons a sparseprediction approach that finds onetomany mappings and a spectral clusteringapproach that finds manytomany mappings. This initial investigation reveals afew previously unknown properties of neural networks and we argue that futureresearch into the question of convergent learning will yield many more. Theinsights described here include  that some features are learned reliably inmultiple networks yet other features are not consistently learned  thatunits learn to span lowdimensional subspaces and while these subspaces arecommon to multiple networks the specific basis vectors learned are not that the representation codes show evidence of being a mix between a local codeand slightly but not fully distributed codes across multiple units.
21,Detecting AI Trojans Using Meta Neural Analysis In machine learning Trojan attacks an adversary trains a corrupted modelthat obtains good performance on normal data but behaves maliciously on datasamples with certain trigger patterns. Several approaches have been proposed todetect such attacks but they make undesirable assumptions about the attackstrategies or require direct access to the trained models which restrictstheir utility in practice.
22,Universal Litmus Patterns Revealing Backdoor Attacks in CNNs The unprecedented success of deep neural networks in many applications hasmade these networks a prime target for adversarial exploitation. In this paperwe introduce a benchmark technique for detecting backdoor attacks aka Trojanattacks on deep convolutional neural networks CNNs. We introduce the conceptof Universal Litmus Patterns ULPs which enable one to reveal backdoorattacks by feeding these universal patterns to the network and analyzing theoutput i.e. classifying the network as clean or corrupted. Thisdetection is fast because it requires only a few forward passes through a CNN.We demonstrate the effectiveness of ULPs for detecting backdoor attacks onthousands of networks with different architectures trained on four benchmarkdatasets namely the German Traffic Sign Recognition Benchmark GTSRB MNISTCIFAR and TinyImageNet. The codes and traintest models for this paper canbe found here
23,Poisoning and Backdooring Contrastive Learning Multimodal contrastive learning methods like CLIP train on noisy anduncurated training datasets. This is cheaper than labeling datasets manuallyand even improves outofdistribution robustness. We show that this practicemakes backdoor and poisoning attacks a significant threat. By poisoning just. of a dataset e.g. just  images of the  millionexample ConceptualCaptions dataset we can cause the model to misclassify test images byoverlaying a small patch. Targeted poisoning attacks whereby the modelmisclassifies a particular test input with an adversariallydesired label areeven easier requiring control of . of the dataset e.g. just three outof the  million images. Our attacks call into question whether training onnoisy and uncurated Internet scrapes is desirable.
24,STRIP A Defence Against Trojan Attacks on Deep Neural Networks A recent trojan attack on deep neural network DNN models is one insidiousvariant of data poisoning attacks. Trojan attacks exploit an effective backdoorcreated in a DNN model by leveraging the difficulty in interpretability of thelearned model to misclassify any inputs signed with the attackers chosentrojan trigger. Since the trojan trigger is a secret guarded and exploited bythe attacker detecting such trojan inputs is a challenge especially atruntime when models are in active operation. This work builds STRongIntentional Perturbation STRIP based runtime trojan attack detection systemand focuses on vision system. We intentionally perturb the incoming input forinstance by superimposing various image patterns and observe the randomness ofpredicted classes for perturbed inputs from a given deployed modelmaliciousor benign. A low entropy in predicted classes violates the inputdependenceproperty of a benign model and implies the presence of a malicious inputacharacteristic of a trojaned input. The high efficacy of our method isvalidated through case studies on three popular and contrasting datasetsMNIST CIFAR and GTSRB. We achieve an overall false acceptance rate FAR ofless than  given a preset false rejection rate FRR of  for differenttypes of triggers. Using CIFAR and GTSRB we have empirically achieved resultof  for both FRR and FAR. We have also evaluated STRIP robustness against anumber of trojan attack variants and adaptive attacks.
25,BadNets Identifying Vulnerabilities in the Machine Learning Model Supply Chain Deep learningbased techniques have achieved stateoftheart performance ona wide variety of recognition and classification tasks. However these networksare typically computationally expensive to train requiring weeks ofcomputation on many GPUs as a result many users outsource the trainingprocedure to the cloud or rely on pretrained models that are then finetunedfor a specific task. In this paper we show that outsourced training introducesnew security risks an adversary can create a maliciously trained network abackdoored neural network or a emphBadNet that has stateoftheartperformance on the users training and validation samples but behaves badly onspecific attackerchosen inputs. We first explore the properties of BadNets ina toy example by creating a backdoored handwritten digit classifier. Next wedemonstrate backdoors in a more realistic scenario by creating a U.S. streetsign classifier that identifies stop signs as speed limits when a specialsticker is added to the stop sign we then show in addition that the backdoorin our US street sign detector can persist even if the network is laterretrained for another task and cause a drop in accuracy of  on averagewhen the backdoor trigger is present. These results demonstrate that backdoorsin neural networks are both powerful andbecause the behavior of neuralnetworks is difficult to explicatestealthy. This work provides motivationfor further research into techniques for verifying and inspecting neuralnetworks just as we have developed tools for verifying and debugging software.
26,Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning Deep learning models have achieved high performance on many tasks and thushave been applied to many securitycritical scenarios. For example deeplearningbased face recognition systems have been used to authenticate users toaccess many securitysensitive applications like payment apps. Such usages ofdeep learning systems provide the adversaries with sufficient incentives toperform attacks against these systems for their adversarial purposes. In thiswork we consider a new type of attacks called backdoor attacks where theattackers goal is to create a backdoor into a learningbased authenticationsystem so that he can easily circumvent the system by leveraging the backdoor.Specifically the adversary aims at creating backdoor instances so that thevictim learning system will be misled to classify the backdoor instances as atarget label specified by the adversary. In particular we study backdoorpoisoning attacks which achieve backdoor attacks using poisoning strategies.Different from all existing work our studied poisoning strategies can applyunder a very weak threat model  the adversary has no knowledge of the modeland the training set used by the victim system  the attacker is allowed toinject only a small amount of poisoning samples  the backdoor key is hardto notice even by human beings to achieve stealthiness. We conduct evaluationto demonstrate that a backdoor adversary can inject only around  poisoningsamples while achieving an attack success rate of above . We are also thefirst work to show that a data poisoning attack can create physicallyimplementable backdoors without touching the training process. Our workdemonstrates that backdoor poisoning attacks pose real threats to a learningsystem and thus highlights the importance of further investigation andproposing defense strategies against them.
27,WeShort Outofdistribution Detection With Weak Shortcut structure Neural networks have achieved impressive performance for data in thedistribution which is the same as the training set but can produce anoverconfident incorrect result for the data these networks have never seen.Therefore it is essential to detect whether inputs come fromoutofdistributionOOD in order to guarantee the safety of neural networksdeployed in the real world. In this paper we propose a simple and effectiveposthoc technique WeShort to reduce the overconfidence of neural networks onOOD data. Our method is inspired by the observation of the internal residualstructure which shows the separation of the OOD and indistribution ID datain the shortcut layer. Our method is compatible with different OOD detectionscores and can generalize well to different architectures of networks. Wedemonstrate our method on various OOD datasets to show its competitiveperformances and provide reasonable hypotheses to explain why our method works.On the ImageNet benchmark Weshort achieves stateoftheart performance on thefalse positive rate FPR and the area under the receiver operatingcharacteristic AUROC on the family of posthoc methods.
28,The Effects of Reward Misspecification Mapping and Mitigating Misaligned Models Reward hacking  where RL agents exploit gaps in misspecified rewardfunctions  has been widely observed but not yet systematically studied. Tounderstand how reward hacking arises we construct four RL environments withmisspecified rewards. We investigate reward hacking as a function of agentcapabilities model capacity action space resolution observation space noiseand training time. More capable agents often exploit reward misspecificationsachieving higher proxy reward and lower true reward than less capable agents.Moreover we find instances of phase transitions capability thresholds atwhich the agents behavior qualitatively shifts leading to a sharp decrease inthe true reward. Such phase transitions pose challenges to monitoring thesafety of ML systems. To address this we propose an anomaly detection task foraberrant policies and offer several baseline detectors.
29,BertNet Harvesting Knowledge Graphs from Pretrained Language Models Symbolic knowledge graphs KGs have been constructed either by expensivehuman crowdsourcing or with domainspecific complex information extractionpipelines. The emerging large pretrained language models LMs such as Berthave shown to implicitly encode massive knowledge which can be queried withproperly designed prompts. However compared to the explicit KGs the implictknowledge in the blackbox LMs is often difficult to access or edit and lacksexplainability. In this work we aim at harvesting symbolic KGs from the LMs anew framework for automatic KG construction empowered by the neural LMsflexibility and scalability. Compared to prior works that often rely on largehuman annotated data or existing massive KGs our approach requires only theminimal definition of relations as inputs and hence is suitable for extractingknowledge of rich new relations not available before.The approach automaticallygenerates diverse prompts and performs efficient knowledge search within agiven LM for consistent and extensive outputs. The harvested knowledge with ourapproach is substantially more accurate than with previous methods as shown inboth automatic and human evaluation. As a result we derive from diverse LMs afamily of new KGs e.g. BertNet and RoBERTaNet that contain a richer set ofcommonsense relations including complex ones e.g. A is capable of but notgood at B than the humanannotated KGs e.g. ConceptNet. Besides theresulting KGs also serve as a vehicle to interpret the respective source LMsleading to new insights into the varying knowledge capability of different LMs.
30,Grokking Generalization Beyond Overfitting on Small Algorithmic Datasets In this paper we propose to study generalization of neural networks on smallalgorithmically generated datasets. In this setting questions about dataefficiency memorization generalization and speed of learning can be studiedin great detail. In some situations we show that neural networks learn througha process of grokking a pattern in the data improving generalizationperformance from random chance level to perfect generalization and that thisimprovement in generalization can happen well past the point of overfitting. Wealso study generalization as a function of dataset size and find that smallerdatasets require increasing amounts of optimization for generalization. Weargue that these datasets provide a fertile ground for studying a poorlyunderstood aspect of deep learning generalization of overparametrized neuralnetworks beyond memorization of the finite training dataset.
31,IBP Regularization for Verified Adversarial Robustness via BranchandBound Recent works have tried to increase the verifiability of adversariallytrained networks by running the attacks over domains larger than the originalperturbations and adding various regularization terms to the objective.However these algorithms either underperform or require complex and expensivestagewise training procedures hindering their practical applicability. Wepresent IBPR a novel verified training algorithm that is both simple andeffective. IBPR induces network verifiability by coupling adversarial attackson enlarged domains with a regularization term based on inexpensive intervalbound propagation that minimizes the gap between the nonconvex verificationproblem and its approximations. By leveraging recent branchandboundframeworks we show that IBPR obtains stateoftheart verifiedrobustnessaccuracy tradeoffs for small perturbations on CIFAR whiletraining significantly faster than relevant previous work. Additionally wepresent UPB a novel branching strategy that relying on a simple heuristicbased on betaCROWN reduces the cost of stateoftheart branchingalgorithms while yielding splits of comparable quality.
32,Oneshot Neural Backdoor Erasing via Adversarial Weight Masking Recent studies show that despite achieving high accuracy on a number ofrealworld applications deep neural networks DNNs can be backdoored byinjecting triggered data samples into the training dataset the adversary canmislead the trained model into classifying any test data to the target class aslong as the trigger pattern is presented. To nullify such backdoor threatsvarious methods have been proposed. Particularly a line of research aims topurify the potentially compromised model. However one major limitation of thisline of work is the requirement to access sufficient original training datathe purifying performance is a lot worse when the available training data islimited. In this work we propose Adversarial Weight Masking AWM a novelmethod capable of erasing the neural backdoors even in the oneshot setting.The key idea behind our method is to formulate this into a minmax optimizationproblem first adversarially recover the trigger patterns and then soft maskthe network weights that are sensitive to the recovered patterns. Comprehensiveevaluations of several benchmark datasets suggest that AWM can largely improvethe purifying effects over other stateoftheart methods on various availabletraining dataset sizes.
33,Defense Against Multitarget Trojan Attacks Adversarial attacks on deep learningbased models pose a significant threatto the current AI infrastructure. Among them Trojan attacks are the hardest todefend against. In this paper we first introduce a variation of the Badnetkind of attacks that introduces Trojan backdoors to multiple target classes andallows triggers to be placed anywhere in the image. The former makes it morepotent and the latter makes it extremely easy to carry out the attack in thephysical space. The stateoftheart Trojan detection methods fail with thisthreat model. To defend against this attack we first introduce a triggerreverseengineering mechanism that uses multiple images to recover a variety ofpotential triggers. We then propose a detection mechanism by measuring thetransferability of such recovered triggers. A Trojan trigger will have veryhigh transferability i.e. they make other images also go to the same class. Westudy many practical advantages of our attack method and then demonstrate thedetection performance using a variety of image datasets. The experimentalresults show the superior detection performance of our method over thestateofthearts.
34,BackdoorBench A Comprehensive Benchmark of Backdoor Learning Backdoor learning is an emerging and important topic of studying thevulnerability of deep neural networks DNNs. Many pioneering backdoor attackand defense methods are being proposed successively or concurrently in thestatus of a rapid arms race. However we find that the evaluations of newmethods are often unthorough to verify their claims and real performancemainly due to the rapid development diverse settings as well as thedifficulties of implementation and reproducibility. Without thoroughevaluations and comparisons it is difficult to track the current progress anddesign the future development roadmap of the literature. To alleviate thisdilemma we build a comprehensive benchmark of backdoor learning calledBackdoorBench. It consists of an extensible modular based codebase currentlyincluding implementations of  stateoftheart SOTA attack and  SOTAdefense algorithms as well as a standardized protocol of a complete backdoorlearning. We also provide comprehensive evaluations of every pair of  attacksagainst  defenses with  poisoning ratios based on  models and  datasetsthus  pairs of evaluations in total. We further present analysis fromdifferent perspectives about these  evaluations studying the effects ofattack against defense algorithms poisoning ratio model and dataset inbackdoor learning. All codes and evaluations of BackdoorBench are publiclyavailable at url
35,Auditing Visualizations Transparency Methods Struggle to Detect Anomalous Behavior Transparency methods such as model visualizations provide information thatoutputs alone might miss since they describe the internals of neural networks.But can we trust that model explanations reflect model behavior For instancecan they diagnose abnormal behavior such as backdoors or shape bias Toevaluate model explanations we define a model as anomalous if it differs froma reference set of normal models and we test whether transparency methodsassign different explanations to anomalous and normal models. We find thatwhile existing methods can detect stark anomalies such as shape bias oradversarial training they struggle to identify more subtle anomalies such asmodels trained on incomplete data. Moreover they generally fail to distinguishthe inputs that induce anomalous behavior e.g. images containing a backdoortrigger. These results reveal new blind spots in existing model explanationspointing to the need for further method development.
36,Natural Backdoor Datasets Extensive literature on backdoor poison attacks has studied attacks anddefenses for backdoors using digital trigger patterns. In contrast physicalbackdoors use physical objects as triggers have only recently beenidentified and are qualitatively different enough to resist all defensestargeting digital trigger backdoors. Research on physical backdoors is limitedby access to large datasets containing real images of physical objectscolocated with targets of classification. Building these datasets is time andlaborintensive. This works seeks to address the challenge of accessibility forresearch on physical backdoor attacks. We hypothesize that there may benaturally occurring physically colocated objects already present in populardatasets such as ImageNet. Once identified a careful relabeling of these datacan transform them into training samples for physical backdoor attacks. Wepropose a method to scalably identify these subsets of potential triggers inexisting datasets along with the specific classes they can poison. We callthese naturally occurring triggerclass subsets natural backdoor datasets. Ourtechniques successfully identify natural backdoors in widelyavailabledatasets and produce models behaviorally equivalent to those trained onmanually curated datasets. We release our code to allow the research communityto create their own datasets for research on physical backdoor attacks.
37,Emergent Abilities of Large Language Models Scaling up language models has been shown to predictably improve performanceand sample efficiency on a wide range of downstream tasks. This paper insteaddiscusses an unpredictable phenomenon that we refer to as emergent abilities oflarge language models. We consider an ability to be emergent if it is notpresent in smaller models but is present in larger models. Thus emergentabilities cannot be predicted simply by extrapolating the performance ofsmaller models. The existence of such emergence implies that additional scalingcould further expand the range of capabilities of language models.
38,Robust Calibration with Multidomain Temperature Scaling Uncertainty quantification is essential for the reliable deployment ofmachine learning models to highstakes application domains. Uncertaintyquantification is all the more challenging when training distribution and testdistribution are different even the distribution shifts are mild. Despite theubiquity of distribution shifts in realworld applications existinguncertainty quantification approaches mainly study the indistribution settingwhere the train and test distributions are the same. In this paper we developa systematic calibration model to handle distribution shifts by leveraging datafrom multiple domains. Our proposed method  multidomain temperature scaling uses the heterogeneity in the domains to improve calibration robustnessunder distribution shift. Through experiments on three benchmark data sets wefind our proposed method outperforms existing methods as measured on bothindistribution and outofdistribution test sets.
39,Can Backdoor Attacks Survive TimeVarying Models Backdoors are powerful attacks against deep neural networks DNNs. Bypoisoning training data attackers can inject hidden rules backdoors intoDNNs which only activate on inputs containing attackspecific triggers. Whileexisting work has studied backdoor attacks on a variety of DNN models theyonly consider static models which remain unchanged after initial deployment.
40,Understanding GamePlaying Agents with Natural Language Annotations We present a new dataset containing K humanannotated games of Go and showhow these natural language annotations can be used as a tool for modelinterpretability. Given a board state and its associated comment our approachuses linear probing to predict mentions of domainspecific terms e.g. koatari from the intermediate state representations of gameplaying agents likeAlphaGo Zero. We find these game concepts are nontrivially encoded in twodistinct policy networks one trained via imitation learning and anothertrained via reinforcement learning. Furthermore mentions of domainspecificterms are most easily predicted from the later layers of both modelssuggesting that these policy networks encode highlevel abstractions similar tothose used in the natural language annotations.
41,Natural Language Descriptions of Deep Visual Features Some neurons in deep networks specialize in recognizing highly specificperceptual structural or semantic features of inputs. In computer visiontechniques exist for identifying neurons that respond to individual conceptcategories like colors textures and object classes. But these techniques arelimited in scope labeling only a small subset of neurons and behaviors in anynetwork. Is a richer characterization of neuronlevel computation possible Weintroduce a procedure called MILAN for mutualinformationguided linguisticannotation of neurons that automatically labels neurons with openendedcompositional natural language descriptions. Given a neuron MILAN generates adescription by searching for a natural language string that maximizes pointwisemutual information with the image regions in which the neuron is active. MILANproduces finegrained descriptions that capture categorical relational andlogical structure in learned features. These descriptions obtain high agreementwith humangenerated feature descriptions across a diverse set of modelarchitectures and tasks and can aid in understanding and controlling learnedmodels. We highlight three applications of natural language neurondescriptions. First we use MILAN for analysis characterizing the distributionand importance of neurons selective for attribute category and relationalinformation in vision models. Second we use MILAN for auditing surfacingneurons sensitive to human faces in datasets designed to obscure them. Finallywe use MILAN for editing improving robustness in an image classifier bydeleting neurons sensitive to text features spuriously correlated with classlabels.
42,Imperceptible Backdoor Attack From Input Space to Feature Representation Backdoor attacks are rapidly emerging threats to deep neural networks DNNs.In the backdoor attack scenario attackers usually implant the backdoor intothe target model by manipulating the training dataset or training process.Then the compromised model behaves normally for benign input yet makesmistakes when the predefined trigger appears. In this paper we analyze thedrawbacks of existing attack approaches and propose a novel imperceptiblebackdoor attack. We treat the trigger pattern as a special kind of noisefollowing a multinomial distribution. A Unetbased network is employed togenerate concrete parameters of multinomial distribution for each benign input.This elaborated trigger ensures that our approach is invisible to both humansand statistical detection. Besides the design of the trigger we also considerthe robustness of our approach against model diagnosebased defences. We forcethe feature representation of malicious input stamped with the trigger to beentangled with the benign one. We demonstrate the effectiveness and robustnessagainst multiple stateoftheart defences through extensive datasets andnetworks. Our trigger only modifies less than  pixels of a benign imagewhile the modification magnitude is . Our source code is available at
43,Teaching Models to Express Their Uncertainty in Words We show that a GPT model can learn to express uncertainty about its ownanswers in natural language  without use of model logits. When given aquestion the model generates both an answer and a level of confidence e.g. confidence or high confidence. These levels map to probabilities thatare well calibrated. The model also remains moderately calibrated underdistribution shift and is sensitive to uncertainty in its own answers ratherthan imitating human examples. To our knowledge this is the first time a modelhas been shown to express calibrated uncertainty about its own answers innatural language. For testing calibration we introduce the CalibratedMathsuite of tasks. We compare the calibration of uncertainty expressed in wordsverbalized probability to uncertainty extracted from model logits. Bothkinds of uncertainty are capable of generalizing calibration under distributionshift. We also provide evidence that GPTs ability to generalize calibrationdepends on pretrained latent representations that correlate with epistemicuncertainty over its answers.
44,Missingness Bias in Model Debugging Missingness or the absence of features from an input is a conceptfundamental to many model debugging tools. However in computer vision pixelscannot simply be removed from an image. One thus tends to resort to heuristicssuch as blacking out pixels which may in turn introduce bias into thedebugging process. We study such biases and in particular show howtransformerbased architectures can enable a more natural implementation ofmissingness which sidesteps these issues and improves the reliability ofmodel debugging in practice. Our code is available at
45,SingleTurn Debate Does Not Help Humans Answer Hard ReadingComprehension Questions Current QA systems can generate reasonablesounding yet false answers withoutexplanation or evidence for the generated answer which is especiallyproblematic when humans cannot readily check the models answers. This presentsa challenge for building trust in machine learning systems. We take inspirationfrom realworld situations where difficult questions are answered byconsidering opposing sides see Irving et al. . For multiplechoice QAexamples we build a dataset of single arguments for both a correct andincorrect answer option in a debatestyle setup as an initial step in trainingmodels to produce explanations for two candidate answers. We use long contexts humans familiar with the context write convincing explanations forpreselected correct and incorrect answers and we test if those explanationsallow humans who have not read the full context to more accurately determinethe correct answer. We do not find that explanations in our setup improvehuman accuracy but a baseline condition shows that providing humanselectedtext snippets does improve accuracy. We use these findings to suggest ways ofimproving the debate set up for future data collection efforts.
46,A geometric framework for outlier detection in highdimensional data Outlier or anomaly detection is an important task in data analysis. Wediscuss the problem from a geometrical perspective and provide a framework thatexploits the metric structure of a data set. Our approach rests on the manifoldassumption i.e. that the observed nominally highdimensional data lie on amuch lower dimensional manifold and that this intrinsic structure can beinferred with manifold learning methods. We show that exploiting this structuresignificantly improves the detection of outlying observations inhighdimensional data. We also suggest a novel mathematically precise andwidely applicable distinction between distributional and structural outliersbased on the geometry and topology of the data manifold that clarifiesconceptual ambiguities prevalent throughout the literature. Our experimentsfocus on functional data as one class of structured highdimensional data butthe framework we propose is completely general and we include image and graphdata applications. Our results show that the outlier structure ofhighdimensional and nontabular data can be detected and visualized usingmanifold learning methods and quantified using standard outlier scoring methodsapplied to the manifold embedding vectors.
47,Exemplary Natural Images Explain CNN Activations Better than StateoftheArt Feature Visualization Feature visualizations such as synthetic maximally activating images are awidely used explanation method to better understand the information processingof convolutional neural networks CNNs. At the same time there are concernsthat these visualizations might not accurately represent CNNs inner workings.Here we measure how much extremely activating images help humans to predictCNN activations. Using a wellcontrolled psychophysical paradigm we comparethe informativeness of synthetic images by Olah et al.  with a simplebaseline visualization namely exemplary natural images that also stronglyactivate a specific feature map. Given either synthetic or natural referenceimages human participants choose which of two query images leads to strongpositive activation. The experiments are designed to maximize participantsperformance and are the first to probe intermediate instead of final layerrepresentations. We find that synthetic images indeed provide helpfulinformation about feature map activations pm accuracy chance would be. However natural images  originally intended as a baseline outperform synthetic images by a wide margin pm. Additionallyparticipants are faster and more confident for natural images whereassubjective impressions about the interpretability of the feature visualizationsare mixed. The higher informativeness of natural images holds across mostlayers for both expert and lay participants as well as for hand andrandomlypicked feature visualizations. Even if only a single reference imageis given synthetic images provide less information than natural imagespm vs. pm. In summary synthetic images from a popularfeature visualization method are significantly less informative for assessingCNN activations than natural images. We argue that visualization methods shouldimprove over this baseline.
