,text
0,Obfuscated Gradients Give a False Sense of Security Circumventing Defenses to Adversarial Examples We identify obfuscated gradients a kind of gradient masking as a phenomenonthat leads to a false sense of security in defenses against adversarialexamples. While defenses that cause obfuscated gradients appear to defeatiterative optimizationbased attacks we find defenses relying on this effectcan be circumvented. We describe characteristic behaviors of defensesexhibiting the effect and for each of the three types of obfuscated gradientswe discover we develop attack techniques to overcome it. In a case studyexamining noncertified whiteboxsecure defenses at ICLR  we findobfuscated gradients are a common occurrence with  of  defenses relying onobfuscated gradients. Our new attacks successfully circumvent  completely and partially in the original threat model each paper considers.
1,Universal Adversarial Triggers for Attacking and Analyzing NLP Adversarial examples highlight model vulnerabilities and are useful forevaluation and interpretation. We define universal adversarial triggersinputagnostic sequences of tokens that trigger a model to produce a specificprediction when concatenated to any input from a dataset. We propose agradientguided search over tokens which finds short trigger sequences e.g.one word for classification and four words for language modeling thatsuccessfully trigger the target prediction. For example triggers cause SNLIentailment accuracy to drop from . to .  of why questions inSQuAD to be answered to kill american people and the GPT language model tospew racist output even when conditioned on nonracial contexts. Furthermorealthough the triggers are optimized using whitebox access to a specific modelthey transfer to other models for all tasks we consider. Finally sincetriggers are inputagnostic they provide an analysis of global model behavior.For instance they confirm that SNLI models exploit dataset biases and help todiagnose heuristics learned by reading comprehension models.
2,Adversarial Examples for Evaluating Reading Comprehension Systems Standard accuracy metrics indicate that reading comprehension systems aremaking rapid progress but the extent to which these systems truly understandlanguage remains unclear. To reward systems with real language understandingabilities we propose an adversarial evaluation scheme for the StanfordQuestion Answering Dataset SQuAD. Our method tests whether systems can answerquestions about paragraphs that contain adversarially inserted sentences whichare automatically generated to distract computer systems without changing thecorrect answer or misleading humans. In this adversarial setting the accuracyof sixteen published models drops from an average of  F score to when the adversary is allowed to add ungrammatical sequences of words averageaccuracy on four models decreases further to . We hope our insights willmotivate the development of new models that understand language more precisely.
3,Data Augmentation Can Improve Robustness Adversarial training suffers from robust overfitting a phenomenon where therobust test accuracy starts to decrease during training. In this paper wefocus on reducing robust overfitting by using common data augmentation schemes.We demonstrate that contrary to previous findings when combined with modelweight averaging data augmentation can significantly boost robust accuracy.Furthermore we compare various augmentations techniques and observe thatspatial composition techniques work the best for adversarial training. Finallywe evaluate our approach on CIFAR against ellinfty and ellnormbounded perturbations of size epsilon   and epsilon  respectively. We show large absolute improvements of . and . inrobust accuracy compared to previous stateoftheart methods. In particularagainst ellinfty normbounded perturbations of size epsilon  our model reaches . robust accuracy without using any external data. Wealso achieve a significant performance boost with this approach while usingother architectures and datasets such as CIFAR SVHN and TinyImageNet.
4,Adversarial Examples for Evaluating Reading Comprehension Systems Standard accuracy metrics indicate that reading comprehension systems aremaking rapid progress but the extent to which these systems truly understandlanguage remains unclear. To reward systems with real language understandingabilities we propose an adversarial evaluation scheme for the StanfordQuestion Answering Dataset SQuAD. Our method tests whether systems can answerquestions about paragraphs that contain adversarially inserted sentences whichare automatically generated to distract computer systems without changing thecorrect answer or misleading humans. In this adversarial setting the accuracyof sixteen published models drops from an average of  F score to when the adversary is allowed to add ungrammatical sequences of words averageaccuracy on four models decreases further to . We hope our insights willmotivate the development of new models that understand language more precisely.
5,Towards Deep Learning Models Resistant to Adversarial Attacks Recent work has demonstrated that deep neural networks are vulnerable toadversarial examplesinputs that are almost indistinguishable from naturaldata and yet classified incorrectly by the network. In fact some of the latestfindings suggest that the existence of adversarial attacks may be an inherentweakness of deep learning models. To address this problem we study theadversarial robustness of neural networks through the lens of robustoptimization. This approach provides us with a broad and unifying view on muchof the prior work on this topic. Its principled nature also enables us toidentify methods for both training and attacking neural networks that arereliable and in a certain sense universal. In particular they specify aconcrete security guarantee that would protect against any adversary. Thesemethods let us train networks with significantly improved resistance to a widerange of adversarial attacks. They also suggest the notion of security againsta firstorder adversary as a natural and broad security guarantee. We believethat robustness against such welldefined classes of adversaries is animportant stepping stone towards fully resistant deep learning models. Code andpretrained models are available at
6,BERTATTACK Adversarial Attack Against BERT Using BERT Adversarial attacks for discrete data such as texts have been provedsignificantly more challenging than continuous data such as images since itis difficult to generate adversarial samples with gradientbased methods.Current successful attack methods for texts usually adopt heuristic replacementstrategies on the character or word level which remains challenging to findthe optimal solution in the massive space of possible combinations ofreplacements while preserving semantic consistency and language fluency. Inthis paper we propose textbfBERTAttack a highquality and effectivemethod to generate adversarial samples using pretrained masked language modelsexemplified by BERT. We turn BERT against its finetuned models and other deepneural models in downstream tasks so that we can successfully mislead thetarget models to predict incorrectly. Our method outperforms stateoftheartattack strategies in both success rate and perturb percentage while thegenerated adversarial samples are fluent and semantically preserved. Also thecost of calculation is low thus possible for largescale generations. The codeis available at
7,Gradientbased Adversarial Attacks against Text Transformers We propose the first generalpurpose gradientbased attack againsttransformer models. Instead of searching for a single adversarial example wesearch for a distribution of adversarial examples parameterized by acontinuousvalued matrix hence enabling gradientbased optimization. Weempirically demonstrate that our whitebox attack attains stateoftheartattack performance on a variety of natural language tasks. Furthermore we showthat a powerful blackbox transfer attack enabled by sampling from theadversarial distribution matches or exceeds existing methods while onlyrequiring hardlabel outputs.
8,Smooth Adversarial Training It is commonly believed that networks cannot be both accurate and robustthat gaining robustness means losing accuracy. It is also generally believedthat unless making networks larger network architectural elements wouldotherwise matter little in improving adversarial robustness. Here we presentevidence to challenge these common beliefs by a careful study about adversarialtraining. Our key observation is that the widelyused ReLU activation functionsignificantly weakens adversarial training due to its nonsmooth nature. Hencewe propose smooth adversarial training SAT in which we replace ReLU with itssmooth approximations to strengthen adversarial training. The purpose of smoothactivation functions in SAT is to allow it to find harder adversarial examplesand compute better gradient updates during adversarial training.
9,Reliable evaluation of adversarial robustness with an ensemble of diverse parameterfree attacks The field of defense strategies against adversarial attacks has significantlygrown over the last years but progress is hampered as the evaluation ofadversarial defenses is often insufficient and thus gives a wrong impression ofrobustness. Many promising defenses could be broken later on making itdifficult to identify the stateoftheart. Frequent pitfalls in the evaluationare improper tuning of hyperparameters of the attacks gradient obfuscation ormasking. In this paper we first propose two extensions of the PGDattackovercoming failures due to suboptimal step size and problems of the objectivefunction. We then combine our novel attacks with two complementary existingones to form a parameterfree computationally affordable and userindependentensemble of attacks to test adversarial robustness. We apply our ensemble toover  models from papers published at recent top machine learning andcomputer vision venues. In all except one of the cases we achieve lower robusttest accuracy than reported in these papers often by more than identifying several broken defenses.
10,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations In this paper we establish rigorous benchmarks for image classifierrobustness. Our first benchmark ImageNetC standardizes and expands thecorruption robustness topic while showing which classifiers are preferable insafetycritical applications. Then we propose a new dataset called ImageNetPwhich enables researchers to benchmark a classifiers robustness to commonperturbations. Unlike recent robustness research this benchmark evaluatesperformance on common corruptions and perturbations not worstcase adversarialperturbations. We find that there are negligible changes in relative corruptionrobustness from AlexNet classifiers to ResNet classifiers. Afterward wediscover ways to enhance corruption and perturbation robustness. We even findthat a bypassed adversarial defense provides substantial common perturbationrobustness. Together our benchmarks may aid future work toward networks thatrobustly generalize.
11,The Many Faces of Robustness A Critical Analysis of OutofDistribution Generalization We introduce four new realworld distribution shift datasets consisting ofchanges in image style image blurriness geographic location cameraoperation and more. With our new datasets we take stock of previouslyproposed methods for improving outofdistribution robustness and put them tothe test. We find that using larger models and artificial data augmentationscan improve robustness on realworld distribution shifts contrary to claims inprior work. We find improvements in artificial robustness benchmarks cantransfer to realworld distribution shifts contrary to claims in prior work.Motivated by our observation that data augmentations can help with realworlddistribution shifts we also introduce a new data augmentation method whichadvances the stateoftheart and outperforms models pretrained with  timesmore labeled data. Overall we find that some methods consistently help withdistribution shifts in texture and local image statistics but these methods donot help with some other distribution shifts like geographic changes. Ourresults show that future research must study multiple distribution shiftssimultaneously as we demonstrate that no evaluated method consistentlyimproves robustness.
12,Reliable evaluation of adversarial robustness with an ensemble of diverse parameterfree attacks The field of defense strategies against adversarial attacks has significantlygrown over the last years but progress is hampered as the evaluation ofadversarial defenses is often insufficient and thus gives a wrong impression ofrobustness. Many promising defenses could be broken later on making itdifficult to identify the stateoftheart. Frequent pitfalls in the evaluationare improper tuning of hyperparameters of the attacks gradient obfuscation ormasking. In this paper we first propose two extensions of the PGDattackovercoming failures due to suboptimal step size and problems of the objectivefunction. We then combine our novel attacks with two complementary existingones to form a parameterfree computationally affordable and userindependentensemble of attacks to test adversarial robustness. We apply our ensemble toover  models from papers published at recent top machine learning andcomputer vision venues. In all except one of the cases we achieve lower robusttest accuracy than reported in these papers often by more than identifying several broken defenses.
13,Using PreTraining Can Improve Model Robustness and Uncertainty He et al.  have called into question the utility of pretraining byshowing that training from scratch can often yield similar performance topretraining. We show that although pretraining may not improve performance ontraditional classification metrics it improves model robustness anduncertainty estimates. Through extensive experiments on adversarial exampleslabel corruption class imbalance outofdistribution detection andconfidence calibration we demonstrate large gains from pretraining andcomplementary effects with taskspecific methods. We introduce adversarialpretraining and show approximately a  absolute improvement over theprevious stateoftheart in adversarial robustness. In some cases usingpretraining without taskspecific methods also surpasses the stateofthearthighlighting the need for pretraining when evaluating future methods onrobustness and uncertainty tasks.
14,Motivating the Rules of the Game for Adversarial Example Research Advances in machine learning have led to broad deployment of systems withimpressive performance on important problems. Nonetheless these systems can beinduced to make errors on data that are surprisingly similar to examples thelearned system handles correctly. The existence of these errors raises avariety of questions about outofsample generalization and whether bad actorsmight use such examples to abuse deployed systems. As a result of thesesecurity concerns there has been a flurry of recent papers proposingalgorithms to defend against such malicious perturbations of correctly handledexamples. It is unclear how such misclassifications represent a different kindof security problem than other errors or even other attackerproduced examplesthat have no specific relationship to an uncorrupted input. In this paper weargue that adversarial example defense papers have to date mostly consideredabstract toy games that do not relate to any specific security concern.Furthermore defense papers have not yet precisely described all the abilitiesand limitations of attackers that would be relevant in practical security.Towards this end we establish a taxonomy of motivations constraints andabilities for more plausible adversaries. Finally we provide a series ofrecommendations outlining a path forward for future work to more clearlyarticulate the threat model and perform more meaningful evaluation.
15,Towards Evaluating the Robustness of Neural Networks Neural networks provide stateoftheart results for most machine learningtasks. Unfortunately neural networks are vulnerable to adversarial examplesgiven an input x and any target classification t it is possible to find anew input x that is similar to x but classified as t. This makes itdifficult to apply neural networks in securitycritical areas. Defensivedistillation is a recently proposed approach that can take an arbitrary neuralnetwork and increase its robustness reducing the success rate of currentattacks ability to find adversarial examples from  to ..
16,Certified Defenses against Adversarial Examples While neural networks have achieved high accuracy on standard imageclassification benchmarks their accuracy drops to nearly zero in the presenceof small adversarial perturbations to test inputs. Defenses based onregularization and adversarial training have been proposed but often followedby new stronger attacks that defeat these defenses. Can we somehow end thisarms race In this work we study this problem for neural networks with onehidden layer. We first propose a method based on a semidefinite relaxation thatoutputs a certificate that for a given network and test input no attack canforce the error to exceed a certain value. Second as this certificate isdifferentiable we jointly optimize it with the network parameters providingan adaptive regularizer that encourages robustness against all attacks. OnMNIST our approach produces a network and a certificate that no attack thatperturbs each pixel by at most epsilon  . can cause more than  testerror.
17,PixMix Dreamlike Pictures Comprehensively Improve Safety Measures In realworld applications of machine learning reliable and safe systemsmust consider measures of performance beyond standard test set accuracy. Theseother goals include outofdistribution OOD robustness predictionconsistency resilience to adversaries calibrated uncertainty estimates andthe ability to detect anomalous inputs. However improving performance towardsthese goals is often a balancing act that todays methods cannot achievewithout sacrificing performance on other safety axes. For instance adversarialtraining improves adversarial robustness but sharply degrades other classifierperformance metrics. Similarly strong data augmentation and regularizationtechniques often improve OOD robustness but harm anomaly detection raising thequestion of whether a Pareto improvement on all existing safety measures ispossible. To meet this challenge we design a new data augmentation strategyutilizing the natural structural complexity of pictures such as fractals whichoutperforms numerous baselines is near Paretooptimal and roundly improvessafety measures.
18,Adversarial NLI A New Benchmark for Natural Language Understanding We introduce a new largescale NLI benchmark dataset collected via aniterative adversarial humanandmodelintheloop procedure. We show thattraining models on this new dataset leads to stateoftheart performance on avariety of popular NLI benchmarks while posing a more difficult challenge withits new test set. Our analysis sheds light on the shortcomings of currentstateoftheart models and shows that nonexpert annotators are successful atfinding their weaknesses. The data collection method can be applied in aneverending learning scenario becoming a moving target for NLU rather than astatic benchmark that will quickly saturate.
19,Augmenting Softmax Information for Selective Classification with OutofDistribution Data Detecting outofdistribution OOD data is a task that is receiving anincreasing amount of research attention in the domain of deep learning forcomputer vision. However the performance of detection methods is generallyevaluated on the task in isolation rather than also considering potentialdownstream tasks in tandem. In this work we examine selective classificationin the presence of OOD data SCOD. That is to say the motivation fordetecting OOD samples is to reject them so their impact on the quality ofpredictions is reduced. We show under this task specification that existingposthoc methods perform quite differently compared to when evaluated only onOOD detection. This is because it is no longer an issue to conflateindistribution ID data with OOD data if the ID data is going to bemisclassified. However the conflation within ID data of correct and incorrectpredictions becomes undesirable. We also propose a novel method for SCODSoftmax Information Retaining Combination SIRC that augments softmaxbasedconfidence scores with featureagnostic information such that their ability toidentify OOD samples is improved without sacrificing separation between correctand incorrect ID predictions. Experiments on a wide variety of ImageNetscaledatasets and convolutional neural network architectures show that SIRC is ableto consistently match or outperform the baseline for SCOD whilst existing OODdetection methods fail to do so.
20,Models Out of Line A Fourier Lens on Distribution Shift Robustness Improving the accuracy of deep neural networks DNNs on outofdistributionOOD data is critical to an acceptance of deep learning DL in real worldapplications. It has been observed that accuracies on indistribution IDversus OOD data follow a linear trend and models that outperform this baselineare exceptionally rare and referred to as effectively robust. Recentlysome promising approaches have been developed to improve OOD robustness modelpruning data augmentation and ensembling or zeroshot evaluating largepretrained models. However there still is no clear understanding of theconditions on OOD data and model properties that are required to observeeffective robustness. We approach this issue by conducting a comprehensiveempirical study of diverse approaches that are known to impact OOD robustnesson a broad range of natural and synthetic distribution shifts of CIFAR andImageNet. In particular we view the effective robustness puzzle through aFourier lens and ask how spectral properties of both models and OOD datainfluence the corresponding effective robustness. We find this Fourier lensoffers some insight into why certain robust models particularly those from theCLIP family achieve OOD robustness. However our analysis also makes clearthat no known metric is consistently the best explanation or even a strongexplanation of OOD robustness. Thus to aid future research into the OODpuzzle we address the gap in publiclyavailable models with effectiverobustness by introducing a set of pretrained modelsRobustNetswith varyinglevels of OOD robustness.
21,WILDS A Benchmark of intheWild Distribution Shifts Distribution shifts  where the training distribution differs from the testdistribution  can substantially degrade the accuracy of machine learning MLsystems deployed in the wild. Despite their ubiquity in the realworlddeployments these distribution shifts are underrepresented in the datasetswidely used in the ML community today. To address this gap we present WILDS acurated benchmark of  datasets reflecting a diverse range of distributionshifts that naturally arise in realworld applications such as shifts acrosshospitals for tumor identification across camera traps for wildlifemonitoring and across time and location in satellite imaging and povertymapping. On each dataset we show that standard training yields substantiallylower outofdistribution than indistribution performance. This gap remainseven with models trained by existing methods for tackling distribution shiftsunderscoring the need for new methods for training models that are more robustto the types of distribution shifts that arise in practice. To facilitatemethod development we provide an opensource package that automates datasetloading contains default model architectures and hyperparameters andstandardizes evaluations. Code and leaderboards are available at
22,ImageNettrained CNNs are biased towards texture increasing shape bias improves accuracy and robustness Convolutional Neural Networks CNNs are commonly thought to recogniseobjects by learning increasingly complex representations of object shapes. Somerecent studies suggest a more important role of image textures. We here putthese conflicting hypotheses to a quantitative test by evaluating CNNs andhuman observers on images with a textureshape cue conflict. We show thatImageNettrained CNNs are strongly biased towards recognising textures ratherthan shapes which is in stark contrast to human behavioural evidence andreveals fundamentally different classification strategies. We then demonstratethat the same standard architecture ResNet that learns a texturebasedrepresentation on ImageNet is able to learn a shapebased representationinstead when trained on StylizedImageNet a stylized version of ImageNet.This provides a much better fit for human behavioural performance in ourwellcontrolled psychophysical lab setting nine experiments totalling psychophysical trials across  observers and comes with a number ofunexpected emergent benefits such as improved object detection performance andpreviously unseen robustness towards a wide range of image distortionshighlighting advantages of a shapebased representation.
23,Natural Adversarial Examples We introduce two challenging datasets that reliably cause machine learningmodel performance to substantially degrade. The datasets are collected with asimple adversarial filtration technique to create datasets with limitedspurious cues. Our datasets realworld unmodified examples transfer tovarious unseen models reliably demonstrating that computer vision models haveshared weaknesses. The first dataset is called ImageNetA and is like theImageNet test set but it is far more challenging for existing models. We alsocurate an adversarial outofdistribution detection dataset called ImageNetOwhich is the first outofdistribution detection dataset created for ImageNetmodels. On ImageNetA a DenseNet obtains around  accuracy an accuracydrop of approximately  and its outofdistribution detection performance onImageNetO is near random chance levels. We find that existing dataaugmentation techniques hardly boost performance and using other publictraining datasets provides improvements that are limited. However we find thatimprovements to computer vision architectures provide a promising path towardsrobust models.
24,Back to the Source DiffusionDriven TestTime Adaptation Testtime adaptation harnesses test inputs to improve the accuracy of a modeltrained on source data when tested on shifted target data. Existing methodsupdate the source model by retraining on each target domain. Whileeffective retraining is sensitive to the amount and order of the data and thehyperparameters for optimization. We instead update the target data byprojecting all test inputs toward the source domain with a generative diffusionmodel. Our diffusiondriven adaptation method DDA shares its models forclassification and generation across all domains. Both models are trained onthe source domain then fixed during testing. We augment diffusion with imageguidance and selfensembling to automatically decide how much to adapt. Inputadaptation by DDA is more robust than prior model adaptation approaches acrossa variety of corruptions architectures and data regimes on the ImageNetCbenchmark. With its inputwise updates DDA succeeds where model adaptationdegrades on too little data in small batches dependent data in nonuniformorder or mixed data with multiple corruptions.
25,GSCLIP  A Framework for Explaining Distribution Shifts in Natural Language Helping end users comprehend the abstract distribution shifts can greatlyfacilitate AI deployment. Motivated by this we propose a novel task datasetexplanation. Given two image data sets dataset explanation aims toautomatically point out their datasetlevel distribution shifts with naturallanguage. Current techniques for monitoring distribution shifts provideinadequate information to understand datasets with the goal of improving dataquality. Therefore we introduce GSCLIP a trainingfree framework to solve thedataset explanation task. In GSCLIP we propose the selector as the firstquantitative evaluation method to identify explanations that are proper tosummarize dataset shifts. Furthermore we leverage this selector to demonstratethe superiority of a generator based on language model generation. Systematicevaluation on natural data shift verifies that GSCLIP a combined system of ahybrid generator group and an efficient selector is not only easytouse butalso powerful for dataset explanation at scale.
26,Robustness of Epinets against Distributional Shifts Recent work introduced the epinet as a new approach to uncertainty modelingin deep learning. An epinet is a small neural network added to traditionalneural networks which together can produce predictive distributions. Inparticular using an epinet can greatly improve the quality of jointpredictions across multiple inputs a measure of how well a neural networkknows what it does not know. In this paper we examine whether epinets canoffer similar advantages under distributional shifts. We find that acrossImageNetAOC epinets generally improve robustness metrics. Moreover theseimprovements are more significant than those afforded by even very largeensembles at orders of magnitude lower computational costs. However theseimprovements are relatively small compared to the outstanding issues indistributionallyrobust deep learning. Epinets may be a useful tool in thetoolbox but they are far from the complete solution.
27,AgreementontheLine Predicting the Performance of Neural Networks under Distribution Shift Recently Miller et al. showed that a models indistribution ID accuracyhas a strong linear correlation with its outofdistribution OOD accuracy onseveral OOD benchmarks  a phenomenon they dubbed accuracyontheline.While a useful tool for model selection i.e. the model most likely to performthe best OOD is the one with highest ID accuracy this fact does not helpestimate the actual OOD performance of models without access to a labeled OODvalidation set. In this paper we show a similar but surprising phenomenon alsoholds for the agreement between pairs of neural network classifiers wheneveraccuracyontheline holds we observe that the OOD agreement between thepredictions of any two pairs of neural networks with potentially differentarchitectures also observes a strong linear correlation with their IDagreement. Furthermore we observe that the slope and bias of OOD vs IDagreement closely matches that of OOD vs ID accuracy. This phenomenon which wecall agreementontheline has important practical applications without anylabeled data we can predict the OOD accuracy of classifiers since OODagreement can be estimated with just unlabeled data. Our prediction algorithmoutperforms previous methods both in shifts where agreementontheline holdsand surprisingly when accuracy is not on the line. This phenomenon alsoprovides new insights into deep neural networks unlike accuracyonthelineagreementontheline appears to only hold for neural network classifiers.
28,Adversarial Text Normalization Textbased adversarial attacks are becoming more commonplace and accessibleto general internet users. As these attacks proliferate the need to addressthe gap in model robustness becomes imminent. While retraining on adversarialdata may increase performance there remains an additional class ofcharacterlevel attacks on which these models falter. Additionally the processto retrain a model is time and resource intensive creating a need for alightweight reusable defense. In this work we propose the Adversarial TextNormalizer a novel method that restores baseline performance on attackedcontent with low computational overhead. We evaluate the efficacy of thenormalizer on two problem areas prone to adversarial attacks i.e. Hate Speechand Natural Language Inference. We find that text normalization provides ataskagnostic defense against characterlevel attacks that can be implementedsupplementary to adversarial retraining solutions which are more suited forsemantic alterations.
29,Robustifying Vision Transformer without Retraining from Scratch by TestTime ClassConditional Feature Alignment Vision Transformer ViT is becoming more popular in image processing.Specifically we investigate the effectiveness of testtime adaptation TTA onViT a technique that has emerged to correct its prediction during testtime byitself. First we benchmark various testtime adaptation approaches on ViTBand ViTL. It is shown that the TTA is effective on ViT and thepriorconvention sensibly selecting modulation parameters is not necessarywhen using proper loss function. Based on the observation we propose a newtesttime adaptation method called classconditional feature alignment CFAwhich minimizes both the classconditional distribution differences and thewhole distribution differences of the hidden representation between the sourceand target in an online manner. Experiments of image classification tasks oncommon corruption CIFARC CIFARC and ImageNetC and domainadaptation digits datasets and ImageNetSketch show that CFA stablyoutperforms the existing baselines on various datasets. We also verify that CFAis model agnostic by experimenting on ResNet MLPMixer and several ViTvariants ViTAugReg DeiT and BeiT. Using BeiT backbone CFA achieves .top error rate on ImageNetC outperforming the existing testtime adaptationbaseline .. This is a stateoftheart result among TTA methods that do notneed to alter training phase.
30,Increasing Confidence in Adversarial Robustness Evaluations Hundreds of defenses have been proposed to make deep neural networks robustagainst minimal adversarial input perturbations. However only a handful ofthese defenses held up their claims because correctly evaluating robustness isextremely challenging Weak attacks often fail to find adversarial exampleseven if they unknowingly exist thereby making a vulnerable network lookrobust. In this paper we propose a test to identify weak attacks and thusweak defense evaluations. Our test slightly modifies a neural network toguarantee the existence of an adversarial example for every sample.Consequentially any correct attack must succeed in breaking this modifiednetwork. For eleven out of thirteen previouslypublished defenses the originalevaluation of the defense fails our test while stronger attacks that breakthese defenses pass it. We hope that attack unit tests  such as ours  will bea major component in future robustness evaluations and increase confidence inan empirical field that is currently riddled with skepticism.
31,Demystifying the Adversarial Robustness of Random Transformation Defenses Neural networks lack of robustness against attacks raises concerns insecuritysensitive settings such as autonomous vehicles. While manycountermeasures may look promising only a few withstand rigorous evaluation.Defenses using random transformations RT have shown impressive resultsparticularly BaRT Raff et al.  on ImageNet. However this type ofdefense has not been rigorously evaluated leaving its robustness propertiespoorly understood. Their stochastic properties make evaluation more challengingand render many proposed attacks on deterministic models inapplicable. Firstwe show that the BPDA attack Athalye et al. a used in BaRTs evaluationis ineffective and likely overestimates its robustness. We then attempt toconstruct the strongest possible RT defense through the informed selection oftransformations and Bayesian optimization for tuning their parameters.Furthermore we create the strongest possible attack to evaluate our RTdefense. Our new attack vastly outperforms the baseline reducing the accuracyby  compared to the  reduction by the commonly used EoT attack.times improvement. Our result indicates that the RT defense on theImagenette dataset a tenclass subset of ImageNet is not robust againstadversarial examples. Extending the study further we use our new attack toadversarially train RT defense called AdvRT resulting in a large robustnessgain. Code is available at
32,GSmooth Certified Robustness against Semantic Transformations via Generalized Randomized Smoothing Certified defenses such as randomized smoothing have shown promise towardsbuilding reliable machine learning systems against ellpnorm boundedattacks. However existing methods are insufficient or unable to provablydefend against semantic transformations especially those without closedformexpressions such as defocus blur and pixelate which are more common inpractice and often unrestricted. To fill up this gap we propose generalizedrandomized smoothing GSmooth a unified theoretical framework for certifyingrobustness against general semantic transformations via a novel dimensionaugmentation strategy. Under the GSmooth framework we present a scalablealgorithm that uses a surrogate imagetoimage network to approximate thecomplex transformation. The surrogate model provides a powerful tool forstudying the properties of semantic transformations and certifying robustness.Experimental results on several datasets demonstrate the effectiveness of ourapproach for robustness certification against multiple kinds of semantictransformations and corruptions which is not achievable by the alternativebaselines.
33,Can CNNs Be More Robust Than Transformers The recent success of Vision Transformers is shaking the long dominance ofConvolutional Neural Networks CNNs in image recognition for a decade.Specifically in terms of robustness on outofdistribution samples recentresearch finds that Transformers are inherently more robust than CNNsregardless of different training setups. Moreover it is believed that suchsuperiority of Transformers should largely be credited to theirselfattentionlike architectures per se. In this paper we question thatbelief by closely examining the design of Transformers. Our findings lead tothree highly effective architecture designs for boosting robustness yet simpleenough to be implemented in several lines of code namely a patchifying inputimages b enlarging kernel size and c reducing activation layers andnormalization layers. Bringing these components together we are able to buildpure CNN architectures without any attentionlike operations that is as robustas or even more robust than Transformers. We hope this work can help thecommunity better understand the design of robust neural architectures. The codeis publicly available at
34,On the Robustness of Safe Reinforcement Learning under Observational Perturbations Safe reinforcement learning RL trains a policy to maximize the task rewardwhile satisfying safety constraints. While prior works focus on the performanceoptimality we find that the optimal solutions of many safe RL problems are notrobust and safe against carefully designed observational perturbations. Weformally analyze the unique properties of designing effective state adversarialattackers in the safe RL setting. We show that baseline adversarial attacktechniques for standard RL tasks are not always effective for safe RL andproposed two new approaches  one maximizes the cost and the other maximizesthe reward. One interesting and counterintuitive finding is that the maximumreward attack is strong as it can both induce unsafe behaviors and make theattack stealthy by maintaining the reward. We further propose a more effectiveadversarial training framework for safe RL and evaluate it via comprehensiveexperiments. This work sheds light on the inherited connection betweenobservational robustness and safety in RL and provides a pioneer work forfuture safe RL studies.
35,Diffusion Models for Adversarial Purification Adversarial purification refers to a class of defense methods that removeadversarial perturbations using a generative model. These methods do not makeassumptions on the form of attack and the classification model and thus candefend preexisting classifiers against unseen threats. However theirperformance currently falls behind adversarial training methods. In this workwe propose DiffPure that uses diffusion models for adversarial purificationGiven an adversarial example we first diffuse it with a small amount of noisefollowing a forward diffusion process and then recover the clean image througha reverse generative process. To evaluate our method against strong adaptiveattacks in an efficient and scalable way we propose to use the adjoint methodto compute full gradients of the reverse generative process. Extensiveexperiments on three image datasets including CIFAR ImageNet and CelebAHQwith three classifier architectures including ResNet WideResNet and ViTdemonstrate that our method achieves the stateoftheart resultsoutperforming current adversarial training and adversarial purificationmethods often by a large margin. Project page
36,Distinction Maximization Loss Efficiently Improving Classification Accuracy Uncertainty Estimation and OutofDistribution Detection Simply Replacing the Loss and Calibrating Building robust deterministic neural networks remains a challenge. On the onehand some approaches improve outofdistribution detection at the cost ofreducing classification accuracy in some situations. On the other hand somemethods simultaneously increase classification accuracy uncertaintyestimation and outofdistribution detection at the expense of reducing theinference efficiency and requiring training the same model many times to tunehyperparameters. In this paper we propose training deterministic neuralnetworks using our DisMax loss which works as a dropin replacement for theusual SoftMax loss i.e. the combination of the linear output layer theSoftMax activation and the crossentropy loss. Starting from the IsoMaxloss we create each logit based on the distances to all prototypes rather thanjust the one associated with the correct class. We also introduce a mechanismto combine images to construct what we call fractional probabilityregularization. Moreover we present a fast way to calibrate the network aftertraining. Finally we propose a composite score to perform outofdistributiondetection. Our experiments show that DisMax usually outperforms currentapproaches simultaneously in classification accuracy uncertainty estimationand outofdistribution detection while maintaining deterministic neuralnetwork inference efficiency and avoiding training the same model repetitivelyfor hyperparameter tuning. The code to reproduce the results is available at
37,SmoothReduce Leveraging Patches for Improved Certified Robustness Randomized smoothing RS has been shown to be a fast scalable technique forcertifying the robustness of deep neural network classifiers. However methodsbased on RS require augmenting data with large amounts of noise which leads tosignificant drops in accuracy. We propose a trainingfree modified smoothingapproach SmoothReduce that leverages patching and aggregation to provideimproved classifier certificates. Our algorithm classifies overlapping patchesextracted from an input image and aggregates the predicted logits to certify alarger radius around the input. We study two aggregation schemes  max andmean  and show that both approaches provide better certificates in terms ofcertified accuracy average certified radii and abstention rates as compared toconcurrent approaches. We also provide theoretical guarantees for suchcertificates and empirically show significant improvements over otherrandomized smoothing methods that require expensive retraining. Further weextend our approach to videos and provide meaningful certificates for videoclassifiers. A project page can be found at
38,Adversarial Training for HighStakes Reliability In the future powerful AI systems may be deployed in highstakes settingswhere a single failure could be catastrophic. One technique for improving AIsafety in highstakes settings is adversarial training which uses an adversaryto generate examples to train on in order to achieve better worstcaseperformance.
39,Formulating Robustness Against Unforeseen Attacks Existing defenses against adversarial examples such as adversarial trainingtypically assume that the adversary will conform to a specific or known threatmodel such as ellp perturbations within a fixed budget. In this paper wefocus on the scenario where there is a mismatch in the threat model assumed bythe defense during training and the actual capabilities of the adversary attest time. We ask the question if the learner trains against a specificsource threat model when can we expect robustness to generalize to astronger unknown target threat model during testtime Our key contributionis to formally define the problem of learning and generalization with anunforeseen adversary which helps us reason about the increase in adversarialrisk from the conventional perspective of a known adversary. Applying ourframework we derive a generalization bound which relates the generalizationgap between source and target threat models to variation of the featureextractor which measures the expected maximum difference between extractedfeatures across a given threat model. Based on our generalization bound wepropose adversarial training with variation regularization ATVR whichreduces variation of the feature extractor across the source threat modelduring training. We empirically demonstrate that ATVR can lead to improvedgeneralization to unforeseen attacks during testtime compared to standardadversarial training on Gaussian and image datasets.
40,Can Rationalization Improve Robustness A growing line of work has investigated the development of neural NLP modelsthat can produce rationalessubsets of input that can explain their modelpredictions. In this paper we ask whether such rationale models can alsoprovide robustness to adversarial attacks in addition to their interpretablenature. Since these models need to first generate rationales rationalizerbefore making predictions predictor they have the potential to ignorenoise or adversarially added text by simply masking it out of the generatedrationale. To this end we systematically generate various types of AddTextattacks for both token and sentencelevel rationalization tasks and perform anextensive empirical evaluation of stateoftheart rationale models across fivedifferent tasks. Our experiments reveal that the rationale models show thepromise to improve robustness while they struggle in certain scenarioswhenthe rationalizer is sensitive to positional bias or lexical choices of attacktext. Further leveraging human rationale as supervision does not alwaystranslate to better performance. Our study is a first step towards exploringthe interplay between interpretability and robustness in therationalizethenpredict framework.
41,Probable Domain Generalization via Quantile Risk Minimization Domain generalization DG seeks predictors which perform well on unseen testdistributions by leveraging labeled training data from multiple relateddistributions or domains. To achieve this the standard formulation optimizesfor worstcase performance over the set of all possible domains. However withworstcase shifts very unlikely in practice this generally leads tooverlyconservative solutions. In fact a recent study found that no DGalgorithm outperformed empirical risk minimization in terms of averageperformance. In this work we argue that DG is neither a worstcase problem noran averagecase problem but rather a probabilistic one. To this end wepropose a probabilistic framework for DG which we call Probable DomainGeneralization wherein our key idea is that distribution shifts seen duringtraining should inform us of probable shifts at test time. To realize this weexplicitly relate training and test domains as draws from the same underlyingmetadistribution and propose a new optimization problem  Quantile RiskMinimization QRM  which requires that predictors generalize with highprobability. We then prove that QRM i produces predictors that generalize tonew domains with a desired probability given sufficiently many domains andsamples and ii recovers the causal predictor as the desired probability ofgeneralization approaches one. In our experiments we introduce a more holisticquantilefocused evaluation protocol for DG and show that our algorithmsoutperform stateoftheart baselines on real and synthetic data.
42,Fast AdvProp Adversarial Propagation AdvProp is an effective way to improve recognitionmodels leveraging adversarial examples. Nonetheless AdvProp suffers from theextremely slow training speed mainly because a extra forward and backwardpasses are required for generating adversarial examples b both originalsamples and their adversarial counterparts are used for training i.e.times data. In this paper we introduce Fast AdvProp which aggressivelyrevamps AdvProps costly training components rendering the method nearly ascheap as the vanilla training. Specifically our modifications in Fast AdvPropare guided by the hypothesis that disentangled learning with adversarialexamples is the key for performance improvements while other training recipese.g. paired clean and adversarial training samples multistep adversarialattackers could be largely simplified.
43,Intrinsic dimension estimation for discrete metrics Real worlddatasets characterized by discrete features are ubiquitous fromcategorical surveys to clinical questionnaires from unweighted networks to DNAsequences. Nevertheless the most common unsupervised dimensional reductionmethods are designed for continuous spaces and their use for discrete spacescan lead to errors and biases. In this letter we introduce an algorithm toinfer the intrinsic dimension ID of datasets embedded in discrete spaces. Wedemonstrate its accuracy on benchmark datasets and we apply it to analyze ametagenomic dataset for species fingerprinting finding a surprisingly smallID of order . This suggests that evolutive pressure acts on a lowdimensionalmanifold despite the highdimensionality of sequences space.
44,Streambased active learning with linear models The proliferation of automated data collection schemes and the advances insensorics are increasing the amount of data we are able to monitor inrealtime. However given the high annotation costs and the time required byquality inspections data is often available in an unlabeled form. This isfostering the use of active learning for the development of soft sensors andpredictive models. In production instead of performing random inspections toobtain product information labels are collected by evaluating the informationcontent of the unlabeled data. Several query strategy frameworks for regressionhave been proposed in the literature but most of the focus has been dedicatedto the static poolbased scenario. In this work we propose a new strategy forthe streambased scenario where instances are sequentially offered to thelearner which must instantaneously decide whether to perform the quality checkto obtain the label or discard the instance. The approach is inspired by theoptimal experimental design theory and the iterative aspect of thedecisionmaking process is tackled by setting a threshold on theinformativeness of the unlabeled data points. The proposed approach isevaluated using numerical simulations and the Tennessee Eastman Processsimulator. The results confirm that selecting the examples suggested by theproposed algorithm allows for a faster reduction in the prediction error.
45,A law of adversarial risk interpolation and label noise In supervised learning it has been shown that label noise in the data can beinterpolated without penalties on test accuracy under many circumstances. Weshow that interpolating label noise induces adversarial vulnerability andprove the first theorem showing the dependence of label noise and adversarialrisk in terms of the data distribution. Our results are almost sharp withoutaccounting for the inductive bias of the learning algorithm. We also show thatinductive bias makes the effect of label noise much stronger.
46,Adversarial Robustness is at Odds with Lazy Training Recent works show that random neural networks are vulnerable againstadversarial attacks Daniely and Schacham  and that such attacks can beeasily found using a single step of gradient descent Bubeck et al. . Inthis work we take it one step further and show that a single gradient step canfind adversarial examples for networks trained in the socalled lazy regime.This regime is interesting because even though the neural network weightsremain close to the initialization there exist networks with smallgeneralization error which can be found efficiently using firstorder methods.Our work challenges the model of the lazy regime the dominant regime in whichneural networks are provably efficiently learnable. We show that the networkstrained in this regime even though they enjoy good theoretical computationalguarantees remain vulnerable to adversarial examples. To the best of ourknowledge this is the first work to prove that such wellgeneralizable neuralnetworks are still vulnerable to adversarial attacks.
