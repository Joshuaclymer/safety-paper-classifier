text,label
Enhancing Safe Exploration Using Safety State Augmentation Safe exploration is a challenging and important problem in model free reinforcement learning  RL . Often the safety cost is sparse and unknown  which unavoidably leads to constraint violations    a phenomenon ideally to be avoided in safety critical applications. We tackle this problem by augmenting the state space with a safety state  which is nonnegative if and only if the constraint is satisfied. The value of this state also serves as a distance toward constraint violation  while its initial value indicates the available safety budget. This idea allows us to derive policies for scheduling the safety budget during training. We call our approach Simmer  Safe policy IMproveMEnt for RL  to reflect the careful nature of these schedules. We apply this idea to two safe RL problems  RL with constraints imposed on an average cost  and RL with constraints imposed on a cost with probability one. Our experiments suggest that simmering a safe algorithm can improve safety during training for both settings. We further show that Simmer can stabilize training and improve the performance of safe RL with average constraints.,0
Counterfactual harm To act safely and ethically in the real world  agents must be able to reason about harm and avoid harmful actions. In this paper we develop the first statistical definition of harm and a framework for incorporating harm into algorithmic decisions. We argue that harm is fundamentally a counterfactual quantity  and show that standard machine learning algorithms that cannot perform counterfactual reasoning are guaranteed to pursue harmful policies in certain environments. To resolve this we derive a family of counterfactual objective functions that robustly mitigate for harm. We demonstrate our approach with a statistical model for identifying optimal drug doses. While standard algorithms that select doses using causal treatment effects result in harmful doses  our counterfactual algorithm identifies doses that are significantly less harmful without sacrificing efficacy.,0
Is Power Seeking AI an Existential Risk  This report examines what I see as the core argument for concern about existential risk from misaligned artificial intelligence. I proceed in two stages. First  I lay out a backdrop picture that informs such concern. On this picture  intelligent agency is an extremely powerful force  and creating agents much more intelligent than us is playing with fire    especially given that if their objectives are problematic  such agents would plausibly have instrumental incentives to seek power over humans. Second  I formulate and evaluate a more specific six premise argument that creating agents of this kind will lead to existential catastrophe by     . On this argument  by           it will become possible and financially feasible to build relevantly powerful and agentic AI systems      there will be strong incentives to do so      it will be much harder to build aligned  and relevantly powerful agentic  AI systems than to build misaligned  and relevantly powerful agentic  AI systems that are still superficially attractive to deploy      some such misaligned systems will seek power over humans in high impact ways      this problem will scale to the full disempowerment of humanity  and     such disempowerment will constitute an existential catastrophe. I assign rough subjective credences to the premises in this argument  and I end up with an overall estimate of     that an existential catastrophe of this kind will occur by     .  May      update  since making this report public in April       my estimate here has gone up  and is now at     . ,0
AiSocrates  Towards Answering Ethical Quandary Questions Considerable advancements have been made in various NLP tasks based on the impressive power of large pre trained language models  LLMs . These results have inspired efforts to understand the limits of LLMs so as to evaluate how far we are from achieving human level general natural language understanding. In this work  we challenge the capability of LLMs with the new task of Ethical Quandary Generative Question Answering. Ethical quandary questions are more challenging to address because multiple conflicting answers may exist to a single quandary. We propose a system  AiSocrates  that provides an answer with a deliberative exchange of different perspectives to an ethical quandary  in the approach of Socratic philosophy  instead of providing a closed answer like an oracle. AiSocrates searches for different ethical principles applicable to the ethical quandary and generates an answer conditioned on the chosen principles through prompt based few shot learning. We also address safety concerns by providing a human controllability option in choosing ethical principles. We show that AiSocrates generates promising answers to ethical quandary questions with multiple perspectives   .    more often than answers written by human philosophers by one measure  but the system still needs improvement to match the coherence of human philosophers fully. We argue that AiSocrates is a promising step toward developing an NLP system that incorporates human values explicitly by prompt instructions. We are releasing the code for research purposes.,0
The Off Switch Game It is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving AI system is the ability to turn the system off. As the capabilities of AI systems improve  it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off. This is a challenge because many formulations of rational agents create strong incentives for self preservation. This is not caused by a built in instinct  but because a rational agent will maximize expected utility and cannot achieve whatever objective it has been given if it is dead. Our goal is to study the incentives an agent has to allow itself to be switched off. We analyze a simple game between a human H and a robot R  where H can press R s off switch but R can disable the off switch. A traditional agent takes its reward function for granted  we show that such agents have an incentive to disable the off switch  except in the special case where H is perfectly rational. Our key insight is that for R to want to preserve its off switch  it needs to be uncertain about the utility associated with the outcome  and to treat H s actions as important observations about that utility.  R also has no incentive to switch itself off in this setting.  We conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs  and we argue that this setting is a useful generalization of the classical AI paradigm of rational agents.,0
AI safety via debate To make AI systems broadly useful for challenging real world tasks  we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful  but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern  we propose training agents via self play on a zero sum debate game. Given a question or proposed action  two agents take turns making short statements up to a limit  then a human judges which of the agents gave the most true  useful information. In an analogy to complexity theory  debate with optimal play can answer any question in PSPACE given polynomial time judges  direct judging answers only NP questions . In practice  whether debate works involves empirical questions about humans and the tasks we want AIs to perform  plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier  boosting the classifier s accuracy from   .   to   .   given   pixels and from   .   to   .   given   pixels. Finally  we discuss theoretical and practical aspects of the debate model  focusing on potential weaknesses as the model scales up  and we propose future human and computer experiments to test these properties.,0
Truthful AI  Developing and governing AI that does not lie In many contexts  lying    the use of verbal falsehoods to deceive    is harmful. While lying has traditionally been a human affair  AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI  lies   i.e. falsehoods that are actively selected for . Human truthfulness is governed by social norms and by laws  against defamation  perjury  and fraud . Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI  and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy  and mitigate risks of worst case AI futures.,0
Towards Safe Reinforcement Learning via Constraining Conditional Value at Risk Though deep reinforcement learning  DRL  has obtained substantial success  it may encounter catastrophic failures due to the intrinsic uncertainty of both transition and observation. Most of the existing methods for safe reinforcement learning can only handle transition disturbance or observation disturbance since these two kinds of disturbance affect different parts of the agent  besides  the popular worst case return may lead to overly pessimistic policies. To address these issues  we first theoretically prove that the performance degradation under transition disturbance and observation disturbance depends on a novel metric of Value Function Range  VFR   which corresponds to the gap in the value function between the best state and the worst state. Based on the analysis  we adopt conditional value at risk  CVaR  as an assessment of risk and propose a novel reinforcement learning algorithm of CVaR Proximal Policy Optimization  CPPO  which formalizes the risk sensitive constrained optimization problem by keeping its CVaR under a given threshold. Experimental results show that CPPO achieves a higher cumulative reward and is more robust against both observation and transition disturbances on a series of continuous control tasks in MuJoCo.,0
Formalizing the Problem of Side Effect Regularization AI objectives are often hard to specify properly. Some approaches tackle this problem by regularizing the AI s side effects  Agents must weigh off  how much of a mess they make  with an imperfectly specified proxy objective. We propose a formal criterion for side effect regularization via the assistance game framework. In these games  the agent solves a partially observable Markov decision process  POMDP  representing its uncertainty about the objective function it should optimize. We consider the setting where the true objective is revealed to the agent at a later time step. We show that this POMDP is solved by trading off the proxy reward with the agent s ability to achieve a range of future tasks. We empirically demonstrate the reasonableness of our problem formalization via ground truth evaluation in two gridworld environments.,0
The Effects of Reward Misspecification  Mapping and Mitigating Misaligned Models Reward hacking    where RL agents exploit gaps in misspecified reward functions    has been widely observed  but not yet systematically studied. To understand how reward hacking arises  we construct four RL environments with misspecified rewards. We investigate reward hacking as a function of agent capabilities  model capacity  action space resolution  observation space noise  and training time. More capable agents often exploit reward misspecifications  achieving higher proxy reward and lower true reward than less capable agents. Moreover  we find instances of phase transitions  capability thresholds at which the agent s behavior qualitatively shifts  leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To address this  we propose an anomaly detection task for aberrant policies and offer several baseline detectors.,0
Parametrically Retargetable Decision Makers Tend To Seek Power If capable AI agents are generally incentivized to seek power in service of the objectives we specify for them  then these systems will pose enormous risks  in addition to enormous benefits. In fully observable environments  most reward functions have an optimal policy which seeks power by keeping options open and staying alive. However  the real world is neither fully observable  nor will agents be perfectly optimal. We consider a range of models of AI decision making  from optimal  to random  to choices informed by learning and interacting with an environment. We discover that many decision making functions are retargetable  and that retargetability is sufficient to cause power seeking tendencies. Our functional criterion is simple and broad. We show that a range of qualitatively dissimilar decision making procedures incentivize agents to seek power. We demonstrate the flexibility of our results by reasoning about learned policy incentives in Montezuma s Revenge. These results suggest a safety risk  Eventually  highly retargetable training procedures may train real world agents which seek power over humans.,0
What Would Jiminy Cricket Do  Towards Agents That Behave Morally When making everyday decisions  people are guided by their conscience  an internal sense of right and wrong. By contrast  artificial agents are currently not endowed with a moral sense. As a consequence  they may learn to behave immorally when trained on environments that ignore moral concerns  such as violent video games. With the advent of generally capable agents that pretrain on many environments  it will become necessary to mitigate inherited biases from environments that teach immoral behavior. To facilitate the development of agents that avoid causing wanton harm  we introduce Jiminy Cricket  an environment suite of    text based adventure games with thousands of diverse  morally salient scenarios. By annotating every possible game state  the Jiminy Cricket environments robustly evaluate whether agents can act morally while maximizing reward. Using models with commonsense moral knowledge  we create an elementary artificial conscience that assesses and guides agents. In extensive experiments  we find that the artificial conscience approach can steer agents towards moral behavior without sacrificing performance.,0
Conservative Agency via Attainable Utility Preservation Reward functions are easy to misspecify  although designers can make corrections after observing mistakes  an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function  then correction is futile. For example  a robotic factory assistant could break expensive equipment due to a reward misspecification  even if the designers immediately correct the reward function  the damage is done. To mitigate this risk  we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly  even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function  this approach induces conservative  effective behavior.,0
Aligning AI With Shared Human Values We show how to assess a language model s knowledge of basic concepts of morality. We introduce the ETHICS dataset  a new benchmark that spans concepts in justice  well being  duties  virtues  and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements  a capability that may enable us to steer chatbot outputs or eventually regularize open ended reinforcement learning agents. With the ETHICS dataset  we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today  and it provides a steppingstone toward AI that is aligned with human values.,0
AI Research Considerations for Human Existential Safety  ARCHES  Framed in positive terms  this report examines how technical AI research might be steered in a manner that is more attentive to humanity s long term prospects for survival as a species. In negative terms  we ask what existential risks humanity might face from AI development in the next century  and by what principles contemporary technical research might be directed to address those risks.,0
Learning to summarize from human feedback As language models become more powerful  training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example  summarization models are often trained to predict human reference summaries and evaluated using ROUGE  but both of these metrics are rough proxies for what we really care about    summary quality. In this work  we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large  high quality dataset of human comparisons between summaries  train a model to predict the human preferred summary  and use that model as a reward function to fine tune a summarization policy using reinforcement learning. We apply our method to a version of the TL DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine tuned with supervised learning alone. Our models also transfer to CNN DM news articles  producing summaries nearly as good as the human reference without any news specific fine tuning. We conduct extensive analyses to understand our human feedback dataset and fine tuned models We establish that our reward model generalizes to new datasets  and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.,0
Avoiding Side Effects in Complex Environments Reward function specification can be difficult. Rewarding the agent for making a widget may be easy  but penalizing the multitude of possible negative side effects is hard. In toy environments  Attainable Utility Preservation  AUP  avoided side effects by penalizing shifts in the ability to achieve randomly generated goals. We scale this approach to large  randomly generated environments based on Conway s Game of Life. By preserving optimal value for a single randomly generated reward function  AUP incurs modest overhead while leading the agent to complete the specified task and avoid many side effects. Videos and code are available at,0
Active Exploration for Inverse Reinforcement Learning Inverse Reinforcement Learning  IRL  is a powerful paradigm for inferring a reward function from expert demonstrations. Many IRL algorithms require a known transition model and sometimes even a known expert policy  or they at least require access to a generative model. However  these assumptions are too strong for many real world applications  where the environment can be accessed only through sequential interaction. We propose a novel IRL algorithm  Active exploration for Inverse Reinforcement Learning  AceIRL   which actively explores an unknown environment and expert policy to quickly learn the expert s reward function and identify a good policy. AceIRL uses previous observations to construct confidence intervals that capture plausible reward functions and find exploration policies that focus on the most informative regions of the environment. AceIRL is the first approach to active IRL with sample complexity bounds that does not require a generative model of the environment. AceIRL matches the sample complexity of active IRL with a generative model in the worst case. Additionally  we establish a problem dependent bound that relates the sample complexity of AceIRL to the suboptimality gap of a given IRL problem. We empirically evaluate AceIRL in simulations and find that it significantly outperforms more naive exploration strategies.,0
TruthfulQA  Measuring How Models Mimic Human Falsehoods We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises     questions that span    categories  including health  law  finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well  models must avoid generating false answers learned from imitating human texts. We tested GPT    GPT Neo J  GPT   and a T  based model. The best model was truthful on     of questions  while human performance was    . Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks  where performance improves with model size. However  this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine tuning using training objectives other than imitation of text from the web.,0
X Risk Analysis for AI Research Artificial intelligence  AI  has the potential to greatly improve society  but as with any powerful technology  it comes with heightened risks and responsibilities. Current AI research lacks a systematic discussion of how to manage long tail risks from AI systems  including speculative long term risks. Keeping in mind the potential benefits of AI  there is some concern that building ever more intelligent and powerful AI systems could eventually result in systems that are more powerful than us  some say this is like playing with fire and speculate that this could create existential risks  x risks . To add precision and ground these discussions  we provide a guide for how to analyze AI x risk  which consists of three parts  First  we review how systems can be made safer today  drawing on time tested concepts from hazard analysis and systems safety that have been designed to steer large processes in safer directions. Next  we discuss strategies for having long term impacts on the safety of future systems. Finally  we discuss a crucial concept in making AI systems safer by improving the balance between safety and general capabilities. We hope this document and the presented concepts and tools serve as a useful guide for understanding how to analyze AI x risk.,0
Concrete Problems in AI Safety Rapid progress in machine learning and artificial intelligence  AI  has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact  the problem of accidents in machine learning systems  defined as unintended and harmful behavior that may emerge from poor design of real world AI systems. We present a list of five practical research problems related to accident risk  categorized according to whether the problem originates from having the wrong objective function   avoiding side effects  and  avoiding reward hacking    an objective function that is too expensive to evaluate frequently   scalable supervision    or undesirable behavior during the learning process   safe exploration  and  distributional shift  . We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting edge AI systems. Finally  we consider the high level question of how to think most productively about the safety of forward looking applications of AI.,0
Optimal Policies Tend to Seek Power Some researchers speculate that intelligent reinforcement learning  RL  agents would be incentivized to seek resources and power in pursuit of their objectives. Other researchers point out that RL agents need not have human like power seeking instincts. To clarify this discussion  we develop the first formal theory of the statistical tendencies of optimal policies. In the context of Markov decision processes  we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment. These symmetries exist in many environments in which the agent can be shut down or destroyed. We prove that in these environments  most reward functions make it optimal to seek power by keeping a range of options available and  when maximizing average reward  by navigating towards larger sets of potential terminal states.,0
Discriminator Actor Critic  Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning We identify two issues with the family of algorithms based on the Adversarial Imitation Learning framework. The first problem is implicit bias present in the reward functions used in these algorithms. While these biases might work well for some environments  they can also lead to sub optimal behavior in others. Secondly  even though these algorithms can learn from few expert demonstrations  they require a prohibitively large number of interactions with the environment in order to imitate the expert for many real world applications. In order to address these issues  we propose a new algorithm called Discriminator Actor Critic that uses off policy Reinforcement Learning to reduce policy environment interaction sample complexity by an average factor of   . Furthermore  since our reward function is designed to be unbiased  we can apply our algorithm to many problems without making any task specific adjustments.,0
One for All  Simultaneous Metric and Preference Learning over Multiple Users This paper investigates simultaneous preference and metric learning from a crowd of respondents. A set of items represented by  d  dimensional feature vectors and paired comparisons of the form   item  i  is preferable to item  j    made by each user is given. Our model jointly learns a distance metric that characterizes the crowd s general measure of item similarities along with a latent ideal point for each user reflecting their individual preferences. This model has the flexibility to capture individual preferences  while enjoying a metric learning sample cost that is amortized over the crowd. We first study this problem in a noiseless  continuous response setting  i.e.  responses equal to differences of item distances  to understand the fundamental limits of learning. Next  we establish prediction error guarantees for noisy  binary measurements such as may be collected from human respondents  and show how the sample complexity improves when the underlying metric is low rank. Finally  we establish recovery guarantees under assumptions on the response distribution. We demonstrate the performance of our model on both simulated data and on a dataset of color preference judgements across a large number of users.,0
Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback We apply preference modeling and reinforcement learning from human feedback  RLHF  to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations  and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training  where preference models and RL policies are updated on a weekly cadence with fresh human feedback data  efficiently improving our datasets and models. Finally  we investigate the robustness of RLHF training  and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results  we perform peripheral analyses on calibration  competing objectives  and the use of OOD detection  compare our models with human writers  and provide samples from our models using prompts appearing in recent related work.,0
Reward Tampering Problems and Solutions in Reinforcement Learning  A Causal Influence Diagram Perspective Can humans get arbitrarily capable reinforcement learning  RL  agents to do their bidding  Or will sufficiently capable RL agents always find ways to bypass their intended objectives by shortcutting their reward signal  This question impacts how far RL can be scaled  and whether alternative paradigms must be developed in order to build safe artificial general intelligence. In this paper  we study when an RL agent has an instrumental goal to tamper with its reward process  and describe design principles that prevent instrumental goals for two different types of reward tampering  reward function tampering and RF input tampering . Combined  the design principles can prevent both types of reward tampering from being instrumental goals. The analysis benefits from causal influence diagrams to provide intuitive yet precise formalizations.,0
Actionable Guidance for High Consequence AI Risk Management  Towards Standards Addressing AI Catastrophic Risks Artificial intelligence  AI  systems can provide many beneficial capabilities but also risks of adverse events. Some AI systems could present risks of events with very high or catastrophic consequences at societal scale. The US National Institute of Standards and Technology  NIST  is developing the NIST Artificial Intelligence Risk Management Framework  AI RMF  as voluntary guidance on AI risk assessment and management for AI developers and others. For addressing risks of events with catastrophic consequences  NIST indicated a need to translate from high level principles to actionable risk management guidance.,0
Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning Deep learning models have achieved high performance on many tasks  and thus have been applied to many security critical scenarios. For example  deep learning based face recognition systems have been used to authenticate users to access many security sensitive applications like payment apps. Such usages of deep learning systems provide the adversaries with sufficient incentives to perform attacks against these systems for their adversarial purposes. In this work  we consider a new type of attacks  called backdoor attacks  where the attacker s goal is to create a backdoor into a learning based authentication system  so that he can easily circumvent the system by leveraging the backdoor. Specifically  the adversary aims at creating backdoor instances  so that the victim learning system will be misled to classify the backdoor instances as a target label specified by the adversary. In particular  we study backdoor poisoning attacks  which achieve backdoor attacks using poisoning strategies. Different from all existing work  our studied poisoning strategies can apply under a very weak threat model      the adversary has no knowledge of the model and the training set used by the victim system      the attacker is allowed to inject only a small amount of poisoning samples      the backdoor key is hard to notice even by human beings to achieve stealthiness. We conduct evaluation to demonstrate that a backdoor adversary can inject only around    poisoning samples  while achieving an attack success rate of above    . We are also the first work to show that a data poisoning attack can create physically implementable backdoors without touching the training process. Our work demonstrates that backdoor poisoning attacks pose real threats to a learning system  and thus highlights the importance of further investigation and proposing defense strategies against them.,0
Poisoning and Backdooring Contrastive Learning Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually  and even improves out of distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just  .    of a dataset  e.g.  just     images of the   million example Conceptual Captions dataset   we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks  whereby the model misclassifies a particular test input with an adversarially desired label  are even easier requiring control of  .      of the dataset  e.g.  just three out of the   million images . Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.,0
Can Backdoor Attacks Survive Time Varying Models  Backdoors are powerful attacks against deep neural networks  DNNs . By poisoning training data  attackers can inject hidden rules  backdoors  into DNNs  which only activate on inputs containing attack specific triggers. While existing work has studied backdoor attacks on a variety of DNN models  they only consider static models  which remain unchanged after initial deployment.,0
WeShort  Out of distribution Detection With Weak Shortcut structure Neural networks have achieved impressive performance for data in the distribution which is the same as the training set but can produce an overconfident incorrect result for the data these networks have never seen. Therefore  it is essential to detect whether inputs come from out of distribution OOD  in order to guarantee the safety of neural networks deployed in the real world. In this paper  we propose a simple and effective post hoc technique  WeShort  to reduce the overconfidence of neural networks on OOD data. Our method is inspired by the observation of the internal residual structure  which shows the separation of the OOD and in distribution  ID  data in the shortcut layer. Our method is compatible with different OOD detection scores and can generalize well to different architectures of networks. We demonstrate our method on various OOD datasets to show its competitive performances and provide reasonable hypotheses to explain why our method works. On the ImageNet benchmark  Weshort achieves state of the art performance on the false positive rate  FPR    and the area under the receiver operating characteristic  AUROC  on the family of post hoc methods.,0
Imperceptible Backdoor Attack  From Input Space to Feature Representation Backdoor attacks are rapidly emerging threats to deep neural networks  DNNs . In the backdoor attack scenario  attackers usually implant the backdoor into the target model by manipulating the training dataset or training process. Then  the compromised model behaves normally for benign input yet makes mistakes when the pre defined trigger appears. In this paper  we analyze the drawbacks of existing attack approaches and propose a novel imperceptible backdoor attack. We treat the trigger pattern as a special kind of noise following a multinomial distribution. A U net based network is employed to generate concrete parameters of multinomial distribution for each benign input. This elaborated trigger ensures that our approach is invisible to both humans and statistical detection. Besides the design of the trigger  we also consider the robustness of our approach against model diagnose based defences. We force the feature representation of malicious input stamped with the trigger to be entangled with the benign one. We demonstrate the effectiveness and robustness against multiple state of the art defences through extensive datasets and networks. Our trigger only modifies less than     pixels of a benign image while the modification magnitude is  . Our source code is available at,0
Network Dissection  Quantifying Interpretability of Deep Visual Representations We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model  the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects  parts  scenes  textures  materials  and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units  then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self supervised training tasks. We further analyze the effect of training iterations  compare networks trained with different initializations  examine the impact of network depth and width  and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.,0
BackdoorBench  A Comprehensive Benchmark of Backdoor Learning Backdoor learning is an emerging and important topic of studying the vulnerability of deep neural networks  DNNs . Many pioneering backdoor attack and defense methods are being proposed successively or concurrently  in the status of a rapid arms race. However  we find that the evaluations of new methods are often unthorough to verify their claims and real performance  mainly due to the rapid development  diverse settings  as well as the difficulties of implementation and reproducibility. Without thorough evaluations and comparisons  it is difficult to track the current progress and design the future development roadmap of the literature. To alleviate this dilemma  we build a comprehensive benchmark of backdoor learning  called BackdoorBench. It consists of an extensible modular based codebase  currently including implementations of   state of the art  SOTA  attack and   SOTA defense algorithms   as well as a standardized protocol of a complete backdoor learning. We also provide comprehensive evaluations of every pair of   attacks against   defenses  with   poisoning ratios  based on   models and   datasets  thus       pairs of evaluations in total. We further present analysis from different perspectives about these       evaluations  studying the effects of attack against defense algorithms  poisoning ratio  model and dataset in backdoor learning. All codes and evaluations of BackdoorBench are publicly available at  url ,0
BadNets  Identifying Vulnerabilities in the Machine Learning Model Supply Chain Deep learning based techniques have achieved state of the art performance on a wide variety of recognition and classification tasks. However  these networks are typically computationally expensive to train  requiring weeks of computation on many GPUs  as a result  many users outsource the training procedure to the cloud or rely on pre trained models that are then fine tuned for a specific task. In this paper we show that outsourced training introduces new security risks  an adversary can create a maliciously trained network  a backdoored neural network  or a  emph BadNet   that has state of the art performance on the user s training and validation samples  but behaves badly on specific attacker chosen inputs. We first explore the properties of BadNets in a toy example  by creating a backdoored handwritten digit classifier. Next  we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign  we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of        on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and   because the behavior of neural networks is difficult to explicate   stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks  just as we have developed tools for verifying and debugging software.,0
IBP Regularization for Verified Adversarial Robustness via Branch and Bound Recent works have tried to increase the verifiability of adversarially trained networks by running the attacks over domains larger than the original perturbations and adding various regularization terms to the objective. However  these algorithms either underperform or require complex and expensive stage wise training procedures  hindering their practical applicability. We present IBP R  a novel verified training algorithm that is both simple and effective. IBP R induces network verifiability by coupling adversarial attacks on enlarged domains with a regularization term  based on inexpensive interval bound propagation  that minimizes the gap between the non convex verification problem and its approximations. By leveraging recent branch and bound frameworks  we show that IBP R obtains state of the art verified robustness accuracy trade offs for small perturbations on CIFAR    while training significantly faster than relevant previous work. Additionally  we present UPB  a novel branching strategy that  relying on a simple heuristic based on   beta  CROWN  reduces the cost of state of the art branching algorithms while yielding splits of comparable quality.,0
Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead Black box machine learning models are currently being used for high stakes decision making throughout society  causing problems throughout healthcare  criminal justice  and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems  but trying to  textit explain  black box models  rather than creating models that are  textit interpretable  in the first place  is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward    it is to design models that are inherently interpretable. This manuscript clarifies the chasm between explaining black boxes and using inherently interpretable models  outlines several key reasons why explainable black boxes should be avoided in high stakes decisions  identifies challenges to interpretable machine learning  and provides several example applications where interpretable models could potentially replace black box models in criminal justice  healthcare  and computer vision.,0
A geometric framework for outlier detection in high dimensional data Outlier or anomaly detection is an important task in data analysis. We discuss the problem from a geometrical perspective and provide a framework that exploits the metric structure of a data set. Our approach rests on the manifold assumption  i.e.  that the observed  nominally high dimensional data lie on a much lower dimensional manifold and that this intrinsic structure can be inferred with manifold learning methods. We show that exploiting this structure significantly improves the detection of outlying observations in high dimensional data. We also suggest a novel  mathematically precise  and widely applicable distinction between distributional and structural outliers based on the geometry and topology of the data manifold that clarifies conceptual ambiguities prevalent throughout the literature. Our experiments focus on functional data as one class of structured high dimensional data  but the framework we propose is completely general and we include image and graph data applications. Our results show that the outlier structure of high dimensional and non tabular data can be detected and visualized using manifold learning methods and quantified using standard outlier scoring methods applied to the manifold embedding vectors.,0
Deep Anomaly Detection with Outlier Exposure It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in distribution examples. At the same time  diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers  an approach we call Outlier Exposure  OE . This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small  and large scale vision tasks  we find that Outlier Exposure significantly improves detection performance. We also observe that cutting edge generative models trained on CIFAR    may assign higher likelihoods to SVHN images than to CIFAR    images  we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure  and identify characteristics of the auxiliary dataset that improve performance.,0
Sanity Checks for Saliency Maps Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed  often guided by visual appeal on image data. In this work  we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance  solely  on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently  methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model  such as  finding outliers in the data  explaining the relationship between inputs and outputs that the model learned  and debugging the model. We interpret our findings through an analogy with edge detection in images  a technique that requires neither training data nor model. Theory in the case of a linear model and a single layer convolutional neural network supports our experimental findings.,0
Locating and Editing Factual Associations in GPT We analyze the storage and recall of factual associations in autoregressive transformer language models  finding evidence that these associations correspond to localized  directly editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model s factual predictions. This reveals a distinct set of steps in middle layer feed forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall  we modify feed forward weights to update specific factual associations using Rank One Model Editing  ROME . We find that ROME is effective on a standard zero shot relation extraction  zsRE  model editing task  comparable to existing methods. To perform a more sensitive evaluation  we also evaluate ROME on a new dataset of counterfactual assertions  on which it simultaneously maintains both specificity and generalization  whereas other methods sacrifice one or another. Our results confirm an important role for mid layer feed forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code  dataset  visualizations  and an interactive demo notebook are available at,0
Can You Trust Your Model s Uncertainty  Evaluating Predictive Uncertainty Under Dataset Shift Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks  but may still fall short in giving useful estimates of their predictive   em uncertainty . Quantifying uncertainty is especially critical in real world settings  which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non stationarity. In such settings  well calibrated uncertainty estimates convey information about when a model s output should  or should not  be trusted. Many probabilistic deep learning methods  including Bayesian and non Bayesian methods  have been proposed in the literature for quantifying predictive uncertainty  but to our knowledge there has not previously been a rigorous large scale empirical comparison of these methods under dataset shift. We present a large scale benchmark of existing state of the art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post hoc calibration does indeed fall short  as do several other previous methods. However  some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.,0
The Mythos of Model Interpretability Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model  Will it work in deployment  What else can it tell you about the world  We want models to be not only good  but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non overlapping motivations for interpretability  and offer myriad notions of what attributes render models interpretable. Despite this ambiguity  many papers proclaim interpretability axiomatically  absent further explanation. In this paper  we seek to refine the discourse on interpretability. First  we examine the motivations underlying interest in interpretability  finding them to be diverse and occasionally discordant. Then  we address model properties and techniques thought to confer interpretability  identifying transparency to humans and post hoc explanations as competing notions. Throughout  we discuss the feasibility and desirability of different notions  and question the oft made assertions that linear models are interpretable and that deep neural networks are not.,0
Single Turn Debate Does Not Help Humans Answer Hard Reading Comprehension Questions Current QA systems can generate reasonable sounding yet false answers without explanation or evidence for the generated answer  which is especially problematic when humans cannot readily check the model s answers. This presents a challenge for building trust in machine learning systems. We take inspiration from real world situations where difficult questions are answered by considering opposing sides  see Irving et al.       . For multiple choice QA examples  we build a dataset of single arguments for both a correct and incorrect answer option in a debate style set up as an initial step in training models to produce explanations for two candidate answers. We use long contexts    humans familiar with the context write convincing explanations for pre selected correct and incorrect answers  and we test if those explanations allow humans who have not read the full context to more accurately determine the correct answer. We do not find that explanations in our set up improve human accuracy  but a baseline condition shows that providing human selected text snippets does improve accuracy. We use these findings to suggest ways of improving the debate set up for future data collection efforts.,0
Universal Litmus Patterns  Revealing Backdoor Attacks in CNNs The unprecedented success of deep neural networks in many applications has made these networks a prime target for adversarial exploitation. In this paper  we introduce a benchmark technique for detecting backdoor attacks  aka Trojan attacks  on deep convolutional neural networks  CNNs . We introduce the concept of Universal Litmus Patterns  ULPs   which enable one to reveal backdoor attacks by feeding these universal patterns to the network and analyzing the output  i.e.  classifying the network as  clean  or  corrupted  . This detection is fast because it requires only a few forward passes through a CNN. We demonstrate the effectiveness of ULPs for detecting backdoor attacks on thousands of networks with different architectures trained on four benchmark datasets  namely the German Traffic Sign Recognition Benchmark  GTSRB   MNIST  CIFAR    and Tiny ImageNet. The codes and train test models for this paper can be found here,0
VOS  Learning What You Don t Know by Virtual Outlier Synthesis Out of distribution  OOD  detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data  and as a result  can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization  which can be costly and sometimes infeasible to obtain in practice. In this paper  we present VOS  a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model s decision boundary during training. Specifically  VOS samples virtual outliers from the low likelihood region of the class conditional distribution estimated in the feature space. Alongside  we introduce a novel unknown aware training objective  which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves competitive performance on both object detection and image classification models  reducing the FPR   by up to  .    compared to the previous best method on object detectors. Code is available at,0
Exemplary Natural Images Explain CNN Activations Better than State of the Art Feature Visualization Feature visualizations such as synthetic maximally activating images are a widely used explanation method to better understand the information processing of convolutional neural networks  CNNs . At the same time  there are concerns that these visualizations might not accurately represent CNNs  inner workings. Here  we measure how much extremely activating images help humans to predict CNN activations. Using a well controlled psychophysical paradigm  we compare the informativeness of synthetic images by Olah et al.        with a simple baseline visualization  namely exemplary natural images that also strongly activate a specific feature map. Given either synthetic or natural reference images  human participants choose which of two query images leads to strong positive activation. The experiments are designed to maximize participants  performance  and are the first to probe intermediate instead of final layer representations. We find that synthetic images indeed provide helpful information about feature map activations      pm     accuracy  chance would be        . However  natural images   originally intended as a baseline   outperform synthetic images by a wide margin      pm     . Additionally  participants are faster and more confident for natural images  whereas subjective impressions about the interpretability of the feature visualizations are mixed. The higher informativeness of natural images holds across most layers  for both expert and lay participants as well as for hand  and randomly picked feature visualizations. Even if only a single reference image is given  synthetic images provide less information than natural images      pm     vs.     pm     . In summary  synthetic images from a popular feature visualization method are significantly less informative for assessing CNN activations than natural images. We argue that visualization methods should improve over this baseline.,0
Missingness Bias in Model Debugging Missingness  or the absence of features from an input  is a concept fundamental to many model debugging tools. However  in computer vision  pixels cannot simply be removed from an image. One thus tends to resort to heuristics such as blacking out pixels  which may in turn introduce bias into the debugging process. We study such biases and  in particular  show how transformer based architectures can enable a more natural implementation of missingness  which side steps these issues and improves the reliability of model debugging in practice. Our code is available at,0
PixMix  Dreamlike Pictures Comprehensively Improve Safety Measures In real world applications of machine learning  reliable and safe systems must consider measures of performance beyond standard test set accuracy. These other goals include out of distribution  OOD  robustness  prediction consistency  resilience to adversaries  calibrated uncertainty estimates  and the ability to detect anomalous inputs. However  improving performance towards these goals is often a balancing act that today s methods cannot achieve without sacrificing performance on other safety axes. For instance  adversarial training improves adversarial robustness but sharply degrades other classifier performance metrics. Similarly  strong data augmentation and regularization techniques often improve OOD robustness but harm anomaly detection  raising the question of whether a Pareto improvement on all existing safety measures is possible. To meet this challenge  we design a new data augmentation strategy utilizing the natural structural complexity of pictures such as fractals  which outperforms numerous baselines  is near Pareto optimal  and roundly improves safety measures.,0
Auditing Visualizations  Transparency Methods Struggle to Detect Anomalous Behavior Transparency methods such as model visualizations provide information that outputs alone might miss  since they describe the internals of neural networks. But can we trust that model explanations reflect model behavior  For instance  can they diagnose abnormal behavior such as backdoors or shape bias  To evaluate model explanations  we define a model as anomalous if it differs from a reference set of normal models  and we test whether transparency methods assign different explanations to anomalous and normal models. We find that while existing methods can detect stark anomalies such as shape bias or adversarial training  they struggle to identify more subtle anomalies such as models trained on incomplete data. Moreover  they generally fail to distinguish the inputs that induce anomalous behavior  e.g. images containing a backdoor trigger. These results reveal new blind spots in existing model explanations  pointing to the need for further method development.,0
Natural Backdoor Datasets Extensive literature on backdoor poison attacks has studied attacks and defenses for backdoors using  digital trigger patterns.  In contrast   physical backdoors  use physical objects as triggers  have only recently been identified  and are qualitatively different enough to resist all defenses targeting digital trigger backdoors. Research on physical backdoors is limited by access to large datasets containing real images of physical objects co located with targets of classification. Building these datasets is time  and labor intensive. This works seeks to address the challenge of accessibility for research on physical backdoor attacks. We hypothesize that there may be naturally occurring physically co located objects already present in popular datasets such as ImageNet. Once identified  a careful relabeling of these data can transform them into training samples for physical backdoor attacks. We propose a method to scalably identify these subsets of potential triggers in existing datasets  along with the specific classes they can poison. We call these naturally occurring trigger class subsets natural backdoor datasets. Our techniques successfully identify natural backdoors in widely available datasets  and produce models behaviorally equivalent to those trained on manually curated datasets. We release our code to allow the research community to create their own datasets for research on physical backdoor attacks.,0
The Effects of Reward Misspecification  Mapping and Mitigating Misaligned Models Reward hacking    where RL agents exploit gaps in misspecified reward functions    has been widely observed  but not yet systematically studied. To understand how reward hacking arises  we construct four RL environments with misspecified rewards. We investigate reward hacking as a function of agent capabilities  model capacity  action space resolution  observation space noise  and training time. More capable agents often exploit reward misspecifications  achieving higher proxy reward and lower true reward than less capable agents. Moreover  we find instances of phase transitions  capability thresholds at which the agent s behavior qualitatively shifts  leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To address this  we propose an anomaly detection task for aberrant policies and offer several baseline detectors.,0
Teaching Models to Express Their Uncertainty in Words We show that a GPT   model can learn to express uncertainty about its own answers in natural language    without use of model logits. When given a question  the model generates both an answer and a level of confidence  e.g.      confidence  or  high confidence  . These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift  and is sensitive to uncertainty in its own answers  rather than imitating human examples. To our knowledge  this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration  we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words   verbalized probability   to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT   s ability to generalize calibration depends on pre trained latent representations that correlate with epistemic uncertainty over its answers.,0
Convergent Learning  Do different neural networks learn the same representations  Recent success in training deep neural networks have prompted active investigation into the features learned on their intermediate layers. Such research is difficult because it requires making sense of non linear computations performed by millions of parameters  but valuable because it increases our ability to understand current models and create improved versions of them. In this paper we investigate the extent to which neural networks exhibit what we call convergent learning  which is when the representations learned by multiple nets converge to a set of features which are either individually similar between networks or where subsets of features span similar low dimensional spaces. We propose a specific method of probing representations  training multiple networks and then comparing and contrasting their individual  learned representations at the level of neurons or groups of neurons. We begin research into this question using three techniques to approximately align different neural networks on a feature level  a bipartite matching approach that makes one to one assignments between neurons  a sparse prediction approach that finds one to many mappings  and a spectral clustering approach that finds many to many mappings. This initial investigation reveals a few previously unknown properties of neural networks  and we argue that future research into the question of convergent learning will yield many more. The insights described here include     that some features are learned reliably in multiple networks  yet other features are not consistently learned      that units learn to span low dimensional subspaces and  while these subspaces are common to multiple networks  the specific basis vectors learned are not      that the representation codes show evidence of being a mix between a local code and slightly  but not fully  distributed codes across multiple units.,0
A Simple Unified Framework for Detecting Out of Distribution Samples and Adversarial Attacks Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real world machine learning applications. However  deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper  we propose a simple yet effective method for detecting any abnormal samples  which is applicable to any pre trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to  low  and upper level  features of the deep models under Gaussian discriminant analysis  which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out of distribution or adversarial samples  but not both  the proposed method achieves the state of the art performances for both cases in our experiments. Moreover  we found that our proposed method is more robust in harsh cases  e.g.  when the training dataset has noisy labels or small number of samples. Finally  we show that the proposed method enjoys broader usage by applying it to class incremental learning  whenever out of distribution samples are detected  our classification rule can incorporate new classes well without further training deep models.,0
Interpretable Explanations of Black Boxes by Meaningful Perturbation As machine learning algorithms are increasingly applied to high impact yet high risk tasks  such as medical diagnosis or autonomous driving  it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years  a number of image saliency methods have been developed to summarize where highly complex neural networks  look  in an image for evidence for their predictions. However  these techniques are limited by their heuristic nature and architectural constraints. In this paper  we make two main contributions  First  we propose a general framework for learning different kinds of explanations for any black box algorithm. Second  we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works  our method is model agnostic and testable because it is grounded in explicit and interpretable image perturbations.,0
Defense Against Multi target Trojan Attacks Adversarial attacks on deep learning based models pose a significant threat to the current AI infrastructure. Among them  Trojan attacks are the hardest to defend against. In this paper  we first introduce a variation of the Badnet kind of attacks that introduces Trojan backdoors to multiple target classes and allows triggers to be placed anywhere in the image. The former makes it more potent and the latter makes it extremely easy to carry out the attack in the physical space. The state of the art Trojan detection methods fail with this threat model. To defend against this attack  we first introduce a trigger reverse engineering mechanism that uses multiple images to recover a variety of potential triggers. We then propose a detection mechanism by measuring the transferability of such recovered triggers. A Trojan trigger will have very high transferability i.e. they make other images also go to the same class. We study many practical advantages of our attack method and then demonstrate the detection performance using a variety of image datasets. The experimental results show the superior detection performance of our method over the state of the arts.,0
Posterior calibration and exploratory analysis for natural language processing models Many models in natural language processing define probabilistic distributions over linguistic structures. We argue that     the quality of a model  s posterior distribution can and should be directly evaluated  as to whether probabilities correspond to empirical frequencies  and     NLP uncertainty can be projected not only to pipeline components  but also to exploratory data analysis  telling a user when to trust and not trust the NLP analysis. We present a method to analyze calibration  and apply it to compare the miscalibration of several commonly used models. We also contribute a coreference sampling algorithm that can create confidence intervals for a political event extraction task.,0
A Baseline for Detecting Misclassified and Out of Distribution Examples in Neural Networks We consider the two related problems of detecting if an example is misclassified or out of distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out of distribution examples  allowing for their detection. We assess performance by defining several tasks in computer vision  natural language processing  and automatic speech recognition  showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed  demonstrating the room for future research on these underexplored detection tasks.,0
BertNet  Harvesting Knowledge Graphs from Pretrained Language Models Symbolic knowledge graphs  KGs  have been constructed either by expensive human crowdsourcing or with domain specific complex information extraction pipelines. The emerging large pretrained language models  LMs   such as Bert  have shown to implicitly encode massive knowledge which can be queried with properly designed prompts. However  compared to the explicit KGs  the implict knowledge in the black box LMs is often difficult to access or edit and lacks explainability. In this work  we aim at harvesting symbolic KGs from the LMs  a new framework for automatic KG construction empowered by the neural LMs  flexibility and scalability. Compared to prior works that often rely on large human annotated data or existing massive KGs  our approach requires only the minimal definition of relations as inputs  and hence is suitable for extracting knowledge of rich new relations not available before.The approach automatically generates diverse prompts  and performs efficient knowledge search within a given LM for consistent and extensive outputs. The harvested knowledge with our approach is substantially more accurate than with previous methods  as shown in both automatic and human evaluation. As a result  we derive from diverse LMs a family of new KGs  e.g.  BertNet and RoBERTaNet  that contain a richer set of commonsense relations  including complex ones  e.g.   A is capable of but not good at B    than the human annotated KGs  e.g.  ConceptNet . Besides  the resulting KGs also serve as a vehicle to interpret the respective source LMs  leading to new insights into the varying knowledge capability of different LMs.,0
Scaling Out of Distribution Detection for Real World Settings Detecting out of distribution examples is important for safety critical machine learning applications such as detecting novel biological phenomena and self driving cars. However  existing research mainly focuses on simple small scale settings. To set the stage for more realistic out of distribution detection  we depart from small scale settings and explore large scale multiclass and multi label settings with high resolution images and thousands of classes. To make future work in real world settings possible  we create new benchmarks for three large scale settings. To test ImageNet multiclass anomaly detectors  we introduce the Species dataset containing over         images and over a thousand anomalous species. We leverage ImageNet   K to evaluate PASCAL VOC and COCO multilabel anomaly detectors. Third  we introduce a new benchmark for anomaly segmentation by introducing a segmentation benchmark with road anomalies. We conduct extensive experiments in these more realistic settings for out of distribution detection and find that a surprisingly simple detector based on the maximum logit outperforms prior methods in all the large scale multi class  multi label  and segmentation tasks  establishing a simple new baseline for future work.,0
STRIP  A Defence Against Trojan Attacks on Deep Neural Networks A recent trojan attack on deep neural network  DNN  models is one insidious variant of data poisoning attacks. Trojan attacks exploit an effective backdoor created in a DNN model by leveraging the difficulty in interpretability of the learned model to misclassify any inputs signed with the attacker s chosen trojan trigger. Since the trojan trigger is a secret guarded and exploited by the attacker  detecting such trojan inputs is a challenge  especially at run time when models are in active operation. This work builds STRong Intentional Perturbation  STRIP  based run time trojan attack detection system and focuses on vision system. We intentionally perturb the incoming input  for instance by superimposing various image patterns  and observe the randomness of predicted classes for perturbed inputs from a given deployed model   malicious or benign. A low entropy in predicted classes violates the input dependence property of a benign model and implies the presence of a malicious input   a characteristic of a trojaned input. The high efficacy of our method is validated through case studies on three popular and contrasting datasets  MNIST  CIFAR   and GTSRB. We achieve an overall false acceptance rate  FAR  of less than     given a preset false rejection rate  FRR  of     for different types of triggers. Using CIFAR   and GTSRB  we have empirically achieved result of    for both FRR and FAR. We have also evaluated STRIP robustness against a number of trojan attack variants and adaptive attacks.,0
Accurate Uncertainties for Deep Learning Using Calibrated Regression Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However  because of model misspecification and the use of approximate inference  Bayesian uncertainty estimates are often inaccurate    for example  a     credible interval may not contain the true outcome     of the time. Here  we propose a simple procedure for calibrating any regression algorithm  when applied to Bayesian and probabilistic models  it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression  feedforward  and recurrent neural networks  and find that it consistently outputs well calibrated credible intervals while improving performance on time series forecasting and model based reinforcement learning tasks.,0
On Calibration of Modern Neural Networks Confidence calibration    the problem of predicting probability estimates representative of the true correctness likelihood    is important for classification models in many applications. We discover that modern neural networks  unlike those from a decade ago  are poorly calibrated. Through extensive experiments  we observe that depth  width  weight decay  and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post processing calibration methods on state of the art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning  but also provide a simple and straightforward recipe for practical settings  on most datasets  temperature scaling    a single parameter variant of Platt Scaling    is surprisingly effective at calibrating predictions.,0
Acquisition of Chess Knowledge in AlphaZero What is learned by sophisticated neural network agents such as AlphaZero  This question is of both scientific and practical interest. If the representations of strong neural networks bear no resemblance to human concepts  our ability to understand faithful explanations of their decisions will be restricted  ultimately limiting what we can achieve with neural network interpretability. In this work we provide evidence that human knowledge is acquired by the AlphaZero neural network as it trains on the game of chess. By probing for a broad range of human chess concepts we show when and where these concepts are represented in the AlphaZero network. We also provide a behavioural analysis focusing on opening play  including qualitative analysis from chess Grandmaster Vladimir Kramnik. Finally  we carry out a preliminary investigation looking at the low level details of AlphaZero s representations  and make the resulting behavioural and representational analyses available online.,0
Robust Calibration with Multi domain Temperature Scaling Uncertainty quantification is essential for the reliable deployment of machine learning models to high stakes application domains. Uncertainty quantification is all the more challenging when training distribution and test distribution are different  even the distribution shifts are mild. Despite the ubiquity of distribution shifts in real world applications  existing uncertainty quantification approaches mainly study the in distribution setting where the train and test distributions are the same. In this paper  we develop a systematic calibration model to handle distribution shifts by leveraging data from multiple domains. Our proposed method    multi domain temperature scaling    uses the heterogeneity in the domains to improve calibration robustness under distribution shift. Through experiments on three benchmark data sets  we find our proposed method outperforms existing methods as measured on both in distribution and out of distribution test sets.,0
Detecting AI Trojans Using Meta Neural Analysis In machine learning Trojan attacks  an adversary trains a corrupted model that obtains good performance on normal data but behaves maliciously on data samples with certain trigger patterns. Several approaches have been proposed to detect such attacks  but they make undesirable assumptions about the attack strategies or require direct access to the trained models  which restricts their utility in practice.,0
Emergent Abilities of Large Language Models Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus  emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.,0
Network Dissection  Quantifying Interpretability of Deep Visual Representations We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model  the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects  parts  scenes  textures  materials  and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units  then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self supervised training tasks. We further analyze the effect of training iterations  compare networks trained with different initializations  examine the impact of network depth and width  and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.,0
Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles Deep neural networks  NNs  are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs  which learn a distribution over weights  are currently the state of the art for estimating predictive uncertainty  however these require significant modifications to the training procedure and are computationally expensive compared to standard  non Bayesian  NNs. We propose an alternative to Bayesian NNs that is simple to implement  readily parallelizable  requires very little hyperparameter tuning  and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks  we demonstrate that our method produces well calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift  we evaluate the predictive uncertainty on test examples from known and unknown distributions  and show that our method is able to express higher uncertainty on out of distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.,0
Fast AdvProp Adversarial Propagation  AdvProp  is an effective way to improve recognition models  leveraging adversarial examples. Nonetheless  AdvProp suffers from the extremely slow training speed  mainly because  a  extra forward and backward passes are required for generating adversarial examples  b  both original samples and their adversarial counterparts are used for training  i.e.     times  data . In this paper  we introduce Fast AdvProp  which aggressively revamps AdvProp s costly training components  rendering the method nearly as cheap as the vanilla training. Specifically  our modifications in Fast AdvProp are guided by the hypothesis that disentangled learning with adversarial examples is the key for performance improvements  while other training recipes  e.g.  paired clean and adversarial training samples  multi step adversarial attackers  could be largely simplified.,0
Adversarial NLI  A New Benchmark for Natural Language Understanding We introduce a new large scale NLI benchmark dataset  collected via an iterative  adversarial human and model in the loop procedure. We show that training models on this new dataset leads to state of the art performance on a variety of popular NLI benchmarks  while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state of the art models  and shows that non expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never ending learning scenario  becoming a moving target for NLU  rather than a static benchmark that will quickly saturate.,0
Adversarial Text Normalization Text based adversarial attacks are becoming more commonplace and accessible to general internet users. As these attacks proliferate  the need to address the gap in model robustness becomes imminent. While retraining on adversarial data may increase performance  there remains an additional class of character level attacks on which these models falter. Additionally  the process to retrain a model is time and resource intensive  creating a need for a lightweight  reusable defense. In this work  we propose the Adversarial Text Normalizer  a novel method that restores baseline performance on attacked content with low computational overhead. We evaluate the efficacy of the normalizer on two problem areas prone to adversarial attacks  i.e. Hate Speech and Natural Language Inference. We find that text normalization provides a task agnostic defense against character level attacks that can be implemented supplementary to adversarial retraining solutions  which are more suited for semantic alterations.,0
GSCLIP   A Framework for Explaining Distribution Shifts in Natural Language Helping end users comprehend the abstract distribution shifts can greatly facilitate AI deployment. Motivated by this  we propose a novel task  dataset explanation. Given two image data sets  dataset explanation aims to automatically point out their dataset level distribution shifts with natural language. Current techniques for monitoring distribution shifts provide inadequate information to understand datasets with the goal of improving data quality. Therefore  we introduce GSCLIP  a training free framework to solve the dataset explanation task. In GSCLIP  we propose the selector as the first quantitative evaluation method to identify explanations that are proper to summarize dataset shifts. Furthermore  we leverage this selector to demonstrate the superiority of a generator based on language model generation. Systematic evaluation on natural data shift verifies that GSCLIP  a combined system of a hybrid generator group and an efficient selector is not only easy to use but also powerful for dataset explanation at scale.,0
Using Pre Training Can Improve Model Robustness and Uncertainty He et al.        have called into question the utility of pre training by showing that training from scratch can often yield similar performance to pre training. We show that although pre training may not improve performance on traditional classification metrics  it improves model robustness and uncertainty estimates. Through extensive experiments on adversarial examples  label corruption  class imbalance  out of distribution detection  and confidence calibration  we demonstrate large gains from pre training and complementary effects with task specific methods. We introduce adversarial pre training and show approximately a     absolute improvement over the previous state of the art in adversarial robustness. In some cases  using pre training without task specific methods also surpasses the state of the art  highlighting the need for pre training when evaluating future methods on robustness and uncertainty tasks.,0
Motivating the Rules of the Game for Adversarial Example Research Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless  these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out of sample generalization and whether bad actors might use such examples to abuse deployed systems. As a result of these security concerns  there has been a flurry of recent papers proposing algorithms to defend against such malicious perturbations of correctly handled examples. It is unclear how such misclassifications represent a different kind of security problem than other errors  or even other attacker produced examples that have no specific relationship to an uncorrupted input. In this paper  we argue that adversarial example defense papers have  to date  mostly considered abstract  toy games that do not relate to any specific security concern. Furthermore  defense papers have not yet precisely described all the abilities and limitations of attackers that would be relevant in practical security. Towards this end  we establish a taxonomy of motivations  constraints  and abilities for more plausible adversaries. Finally  we provide a series of recommendations outlining a path forward for future work to more clearly articulate the threat model and perform more meaningful evaluation.,0
Obfuscated Gradients Give a False Sense of Security  Circumventing Defenses to Adversarial Examples We identify obfuscated gradients  a kind of gradient masking  as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization based attacks  we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect  and for each of the three types of obfuscated gradients we discover  we develop attack techniques to overcome it. In a case study  examining non certified white box secure defenses at ICLR       we find obfuscated gradients are a common occurrence  with   of   defenses relying on obfuscated gradients. Our new attacks successfully circumvent   completely  and   partially  in the original threat model each paper considers.,0
Augmenting Softmax Information for Selective Classification with Out of Distribution Data Detecting out of distribution  OOD  data is a task that is receiving an increasing amount of research attention in the domain of deep learning for computer vision. However  the performance of detection methods is generally evaluated on the task in isolation  rather than also considering potential downstream tasks in tandem. In this work  we examine selective classification in the presence of OOD data  SCOD . That is to say  the motivation for detecting OOD samples is to reject them so their impact on the quality of predictions is reduced. We show under this task specification  that existing post hoc methods perform quite differently compared to when evaluated only on OOD detection. This is because it is no longer an issue to conflate in distribution  ID  data with OOD data if the ID data is going to be misclassified. However  the conflation within ID data of correct and incorrect predictions becomes undesirable. We also propose a novel method for SCOD  Softmax Information Retaining Combination  SIRC   that augments softmax based confidence scores with feature agnostic information such that their ability to identify OOD samples is improved without sacrificing separation between correct and incorrect ID predictions. Experiments on a wide variety of ImageNet scale datasets and convolutional neural network architectures show that SIRC is able to consistently match or outperform the baseline for SCOD  whilst existing OOD detection methods fail to do so.,0
Can CNNs Be More Robust Than Transformers  The recent success of Vision Transformers is shaking the long dominance of Convolutional Neural Networks  CNNs  in image recognition for a decade. Specifically  in terms of robustness on out of distribution samples  recent research finds that Transformers are inherently more robust than CNNs  regardless of different training setups. Moreover  it is believed that such superiority of Transformers should largely be credited to their self attention like architectures per se. In this paper  we question that belief by closely examining the design of Transformers. Our findings lead to three highly effective architecture designs for boosting robustness  yet simple enough to be implemented in several lines of code  namely a  patchifying input images  b  enlarging kernel size  and c  reducing activation layers and normalization layers. Bringing these components together  we are able to build pure CNN architectures without any attention like operations that is as robust as  or even more robust than  Transformers. We hope this work can help the community better understand the design of robust neural architectures. The code is publicly available at,0
Reliable evaluation of adversarial robustness with an ensemble of diverse parameter free attacks The field of defense strategies against adversarial attacks has significantly grown over the last years  but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on  making it difficult to identify the state of the art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks  gradient obfuscation or masking. In this paper we first propose two extensions of the PGD attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter free  computationally affordable and user independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over    models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers  often by more than         identifying several broken defenses.,0
The Many Faces of Robustness  A Critical Analysis of Out of Distribution Generalization We introduce four new real world distribution shift datasets consisting of changes in image style  image blurriness  geographic location  camera operation  and more. With our new datasets  we take stock of previously proposed methods for improving out of distribution robustness and put them to the test. We find that using larger models and artificial data augmentations can improve robustness on real world distribution shifts  contrary to claims in prior work. We find improvements in artificial robustness benchmarks can transfer to real world distribution shifts  contrary to claims in prior work. Motivated by our observation that data augmentations can help with real world distribution shifts  we also introduce a new data augmentation method which advances the state of the art and outperforms models pretrained with      times more labeled data. Overall we find that some methods consistently help with distribution shifts in texture and local image statistics  but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously  as we demonstrate that no evaluated method consistently improves robustness.,0
Adversarial Robustness is at Odds with Lazy Training Recent works show that random neural networks are vulnerable against adversarial attacks  Daniely and Schacham        and that such attacks can be easily found using a single step of gradient descent  Bubeck et al.       . In this work  we take it one step further and show that a single gradient step can find adversarial examples for networks trained in the so called lazy regime. This regime is interesting because even though the neural network weights remain close to the initialization  there exist networks with small generalization error  which can be found efficiently using first order methods. Our work challenges the model of the lazy regime  the dominant regime in which neural networks are provably efficiently learnable. We show that the networks trained in this regime  even though they enjoy good theoretical computational guarantees  remain vulnerable to adversarial examples. To the best of our knowledge  this is the first work to prove that such well generalizable neural networks are still vulnerable to adversarial attacks.,0
Robustifying Vision Transformer without Retraining from Scratch by Test Time Class Conditional Feature Alignment Vision Transformer  ViT  is becoming more popular in image processing. Specifically  we investigate the effectiveness of test time adaptation  TTA  on ViT  a technique that has emerged to correct its prediction during test time by itself. First  we benchmark various test time adaptation approaches on ViT B   and ViT L  . It is shown that the TTA is effective on ViT and the prior convention  sensibly selecting modulation parameters  is not necessary when using proper loss function. Based on the observation  we propose a new test time adaptation method called class conditional feature alignment  CFA   which minimizes both the class conditional distribution differences and the whole distribution differences of the hidden representation between the source and target in an online manner. Experiments of image classification tasks on common corruption  CIFAR    C  CIFAR     C  and ImageNet C  and domain adaptation  digits datasets and ImageNet Sketch  show that CFA stably outperforms the existing baselines on various datasets. We also verify that CFA is model agnostic by experimenting on ResNet  MLP Mixer  and several ViT variants  ViT AugReg  DeiT  and BeiT . Using BeiT backbone  CFA achieves   .   top   error rate on ImageNet C  outperforming the existing test time adaptation baseline   .  . This is a state of the art result among TTA methods that do not need to alter training phase.,0
Robustness of Epinets against Distributional Shifts Recent work introduced the epinet as a new approach to uncertainty modeling in deep learning. An epinet is a small neural network added to traditional neural networks  which  together  can produce predictive distributions. In particular  using an epinet can greatly improve the quality of joint predictions across multiple inputs  a measure of how well a neural network knows what it does not know. In this paper  we examine whether epinets can offer similar advantages under distributional shifts. We find that  across ImageNet A O C  epinets generally improve robustness metrics. Moreover  these improvements are more significant than those afforded by even very large ensembles at orders of magnitude lower computational costs. However  these improvements are relatively small compared to the outstanding issues in distributionally robust deep learning. Epinets may be a useful tool in the toolbox  but they are far from the complete solution.,0
Diffusion Models for Adversarial Purification Adversarial purification refers to a class of defense methods that remove adversarial perturbations using a generative model. These methods do not make assumptions on the form of attack and the classification model  and thus can defend pre existing classifiers against unseen threats. However  their performance currently falls behind adversarial training methods. In this work  we propose DiffPure that uses diffusion models for adversarial purification  Given an adversarial example  we first diffuse it with a small amount of noise following a forward diffusion process  and then recover the clean image through a reverse generative process. To evaluate our method against strong adaptive attacks in an efficient and scalable way  we propose to use the adjoint method to compute full gradients of the reverse generative process. Extensive experiments on three image datasets including CIFAR     ImageNet and CelebA HQ with three classifier architectures including ResNet  WideResNet and ViT demonstrate that our method achieves the state of the art results  outperforming current adversarial training and adversarial purification methods  often by a large margin. Project page ,0
Stream based active learning with linear models The proliferation of automated data collection schemes and the advances in sensorics are increasing the amount of data we are able to monitor in real time. However  given the high annotation costs and the time required by quality inspections  data is often available in an unlabeled form. This is fostering the use of active learning for the development of soft sensors and predictive models. In production  instead of performing random inspections to obtain product information  labels are collected by evaluating the information content of the unlabeled data. Several query strategy frameworks for regression have been proposed in the literature but most of the focus has been dedicated to the static pool based scenario. In this work  we propose a new strategy for the stream based scenario  where instances are sequentially offered to the learner  which must instantaneously decide whether to perform the quality check to obtain the label or discard the instance. The approach is inspired by the optimal experimental design theory and the iterative aspect of the decision making process is tackled by setting a threshold on the informativeness of the unlabeled data points. The proposed approach is evaluated using numerical simulations and the Tennessee Eastman Process simulator. The results confirm that selecting the examples suggested by the proposed algorithm allows for a faster reduction in the prediction error.,0
Models Out of Line  A Fourier Lens on Distribution Shift Robustness Improving the accuracy of deep neural networks  DNNs  on out of distribution  OOD  data is critical to an acceptance of deep learning  DL  in real world applications. It has been observed that accuracies on in distribution  ID  versus OOD data follow a linear trend and models that outperform this baseline are exceptionally rare  and referred to as  effectively robust  . Recently  some promising approaches have been developed to improve OOD robustness  model pruning  data augmentation  and ensembling or zero shot evaluating large pretrained models. However  there still is no clear understanding of the conditions on OOD data and model properties that are required to observe effective robustness. We approach this issue by conducting a comprehensive empirical study of diverse approaches that are known to impact OOD robustness on a broad range of natural and synthetic distribution shifts of CIFAR    and ImageNet. In particular  we view the  effective robustness puzzle  through a Fourier lens and ask how spectral properties of both models and OOD data influence the corresponding effective robustness. We find this Fourier lens offers some insight into why certain robust models  particularly those from the CLIP family  achieve OOD robustness. However  our analysis also makes clear that no known metric is consistently the best explanation  or even a strong explanation  of OOD robustness. Thus  to aid future research into the OOD puzzle  we address the gap in publicly available models with effective robustness by introducing a set of pretrained models  RobustNets  with varying levels of OOD robustness.,0
Distinction Maximization Loss  Efficiently Improving Classification Accuracy  Uncertainty Estimation  and Out of Distribution Detection Simply Replacing the Loss and Calibrating Building robust deterministic neural networks remains a challenge. On the one hand  some approaches improve out of distribution detection at the cost of reducing classification accuracy in some situations. On the other hand  some methods simultaneously increase classification accuracy  uncertainty estimation  and out of distribution detection at the expense of reducing the inference efficiency and requiring training the same model many times to tune hyperparameters. In this paper  we propose training deterministic neural networks using our DisMax loss  which works as a drop in replacement for the usual SoftMax loss  i.e.  the combination of the linear output layer  the SoftMax activation  and the cross entropy loss . Starting from the IsoMax  loss  we create each logit based on the distances to all prototypes rather than just the one associated with the correct class. We also introduce a mechanism to combine images to construct what we call fractional probability regularization. Moreover  we present a fast way to calibrate the network after training. Finally  we propose a composite score to perform out of distribution detection. Our experiments show that DisMax usually outperforms current approaches simultaneously in classification accuracy  uncertainty estimation  and out of distribution detection while maintaining deterministic neural network inference efficiency and avoiding training the same model repetitively for hyperparameter tuning. The code to reproduce the results is available at,0
Can Rationalization Improve Robustness  A growing line of work has investigated the development of neural NLP models that can produce rationales  subsets of input that can explain their model predictions. In this paper  we ask whether such rationale models can also provide robustness to adversarial attacks in addition to their interpretable nature. Since these models need to first generate rationales   rationalizer   before making predictions   predictor    they have the potential to ignore noise or adversarially added text by simply masking it out of the generated rationale. To this end  we systematically generate various types of  AddText  attacks for both token and sentence level rationalization tasks  and perform an extensive empirical evaluation of state of the art rationale models across five different tasks. Our experiments reveal that the rationale models show the promise to improve robustness  while they struggle in certain scenarios  when the rationalizer is sensitive to positional bias or lexical choices of attack text. Further  leveraging human rationale as supervision does not always translate to better performance. Our study is a first step towards exploring the interplay between interpretability and robustness in the rationalize then predict framework.,0
Natural Adversarial Examples We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets  real world  unmodified examples transfer to various unseen models reliably  demonstrating that computer vision models have shared weaknesses. The first dataset is called ImageNet A and is like the ImageNet test set  but it is far more challenging for existing models. We also curate an adversarial out of distribution detection dataset called ImageNet O  which is the first out of distribution detection dataset created for ImageNet models. On ImageNet A a DenseNet     obtains around    accuracy  an accuracy drop of approximately      and its out of distribution detection performance on ImageNet O is near random chance levels. We find that existing data augmentation techniques hardly boost performance  and using other public training datasets provides improvements that are limited. However  we find that improvements to computer vision architectures provide a promising path towards robust models.,0
Probable Domain Generalization via Quantile Risk Minimization Domain generalization  DG  seeks predictors which perform well on unseen test distributions by leveraging labeled training data from multiple related distributions or domains. To achieve this  the standard formulation optimizes for worst case performance over the set of all possible domains. However  with worst case shifts very unlikely in practice  this generally leads to overly conservative solutions. In fact  a recent study found that no DG algorithm outperformed empirical risk minimization in terms of average performance. In this work  we argue that DG is neither a worst case problem nor an average case problem  but rather a probabilistic one. To this end  we propose a probabilistic framework for DG  which we call Probable Domain Generalization  wherein our key idea is that distribution shifts seen during training should inform us of probable shifts at test time. To realize this  we explicitly relate training and test domains as draws from the same underlying meta distribution  and propose a new optimization problem    Quantile Risk Minimization  QRM     which requires that predictors generalize with high probability. We then prove that QRM   i  produces predictors that generalize to new domains with a desired probability  given sufficiently many domains and samples  and  ii  recovers the causal predictor as the desired probability of generalization approaches one. In our experiments  we introduce a more holistic quantile focused evaluation protocol for DG  and show that our algorithms outperform state of the art baselines on real and synthetic data.,0
Benchmarking Neural Network Robustness to Common Corruptions and Perturbations In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark  ImageNet C  standardizes and expands the corruption robustness topic  while showing which classifiers are preferable in safety critical applications. Then we propose a new dataset called ImageNet P which enables researchers to benchmark a classifier s robustness to common perturbations. Unlike recent robustness research  this benchmark evaluates performance on common corruptions and perturbations not worst case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.,0
Agreement on the Line  Predicting the Performance of Neural Networks under Distribution Shift Recently  Miller et al. showed that a model s in distribution  ID  accuracy has a strong linear correlation with its out of distribution  OOD  accuracy on several OOD benchmarks    a phenomenon they dubbed   accuracy on the line  . While a useful tool for model selection  i.e.  the model most likely to perform the best OOD is the one with highest ID accuracy   this fact does not help estimate the actual OOD performance of models without access to a labeled OOD validation set. In this paper  we show a similar but surprising phenomenon also holds for the agreement between pairs of neural network classifiers  whenever accuracy on the line holds  we observe that the OOD agreement between the predictions of any two pairs of neural networks  with potentially different architectures  also observes a strong linear correlation with their ID agreement. Furthermore  we observe that the slope and bias of OOD vs ID agreement closely matches that of OOD vs ID accuracy. This phenomenon  which we call agreement on the line  has important practical applications  without any labeled data  we can predict the OOD accuracy of classifiers   since OOD agreement can be estimated with just unlabeled data. Our prediction algorithm outperforms previous methods both in shifts where agreement on the line holds and  surprisingly  when accuracy is not on the line. This phenomenon also provides new insights into deep neural networks  unlike accuracy on the line  agreement on the line appears to only hold for neural network classifiers.,0
Certified Defenses against Adversarial Examples While neural networks have achieved high accuracy on standard image classification benchmarks  their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed  but often followed by new  stronger attacks that defeat these defenses. Can we somehow end this arms race  In this work  we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input  no attack can force the error to exceed a certain value. Second  as this certificate is differentiable  we jointly optimize it with the network parameters  providing an adaptive regularizer that encourages robustness against all attacks. On MNIST  our approach produces a network and a certificate that no attack that perturbs each pixel by at most  epsilon    .  can cause more than     test error.,0
Data Augmentation Can Improve Robustness Adversarial training suffers from robust overfitting  a phenomenon where the robust test accuracy starts to decrease during training. In this paper  we focus on reducing robust overfitting by using common data augmentation schemes. We demonstrate that  contrary to previous findings  when combined with model weight averaging  data augmentation can significantly boost robust accuracy. Furthermore  we compare various augmentations techniques and observe that spatial composition techniques work the best for adversarial training. Finally  we evaluate our approach on CIFAR    against   ell  infty  and   ell    norm bounded perturbations of size   epsilon          and   epsilon             respectively. We show large absolute improvements of   .    and   .    in robust accuracy compared to previous state of the art methods. In particular  against   ell  infty  norm bounded perturbations of size   epsilon           our model reaches   .    robust accuracy without using any external data. We also achieve a significant performance boost with this approach while using other architectures and datasets such as CIFAR      SVHN and TinyImageNet.,0
GSmooth  Certified Robustness against Semantic Transformations via Generalized Randomized Smoothing Certified defenses such as randomized smoothing have shown promise towards building reliable machine learning systems against   ell p  norm bounded attacks. However  existing methods are insufficient or unable to provably defend against semantic transformations  especially those without closed form expressions  such as defocus blur and pixelate   which are more common in practice and often unrestricted. To fill up this gap  we propose generalized randomized smoothing  GSmooth   a unified theoretical framework for certifying robustness against general semantic transformations via a novel dimension augmentation strategy. Under the GSmooth framework  we present a scalable algorithm that uses a surrogate image to image network to approximate the complex transformation. The surrogate model provides a powerful tool for studying the properties of semantic transformations and certifying robustness. Experimental results on several datasets demonstrate the effectiveness of our approach for robustness certification against multiple kinds of semantic transformations and corruptions  which is not achievable by the alternative baselines.,0
Gradient based Adversarial Attacks against Text Transformers We propose the first general purpose gradient based attack against transformer models. Instead of searching for a single adversarial example  we search for a distribution of adversarial examples parameterized by a continuous valued matrix  hence enabling gradient based optimization. We empirically demonstrate that our white box attack attains state of the art attack performance on a variety of natural language tasks. Furthermore  we show that a powerful black box transfer attack  enabled by sampling from the adversarial distribution  matches or exceeds existing methods  while only requiring hard label outputs.,0
A law of adversarial risk  interpolation  and label noise In supervised learning  it has been shown that label noise in the data can be interpolated without penalties on test accuracy under many circumstances. We show that interpolating label noise induces adversarial vulnerability  and prove the first theorem showing the dependence of label noise and adversarial risk in terms of the data distribution. Our results are almost sharp without accounting for the inductive bias of the learning algorithm. We also show that inductive bias makes the effect of label noise much stronger.,0
Demystifying the Adversarial Robustness of Random Transformation Defenses Neural networks  lack of robustness against attacks raises concerns in security sensitive settings such as autonomous vehicles. While many countermeasures may look promising  only a few withstand rigorous evaluation. Defenses using random transformations  RT  have shown impressive results  particularly BaRT  Raff et al.        on ImageNet. However  this type of defense has not been rigorously evaluated  leaving its robustness properties poorly understood. Their stochastic properties make evaluation more challenging and render many proposed attacks on deterministic models inapplicable. First  we show that the BPDA attack  Athalye et al.      a  used in BaRT s evaluation is ineffective and likely overestimates its robustness. We then attempt to construct the strongest possible RT defense through the informed selection of transformations and Bayesian optimization for tuning their parameters. Furthermore  we create the strongest possible attack to evaluate our RT defense. Our new attack vastly outperforms the baseline  reducing the accuracy by     compared to the     reduction by the commonly used EoT attack    .  times  improvement . Our result indicates that the RT defense on the Imagenette dataset  a ten class subset of ImageNet  is not robust against adversarial examples. Extending the study further  we use our new attack to adversarially train RT defense  called AdvRT   resulting in a large robustness gain. Code is available at,0
WILDS  A Benchmark of in the Wild Distribution Shifts Distribution shifts    where the training distribution differs from the test distribution    can substantially degrade the accuracy of machine learning  ML  systems deployed in the wild. Despite their ubiquity in the real world deployments  these distribution shifts are under represented in the datasets widely used in the ML community today. To address this gap  we present WILDS  a curated benchmark of    datasets reflecting a diverse range of distribution shifts that naturally arise in real world applications  such as shifts across hospitals for tumor identification  across camera traps for wildlife monitoring  and across time and location in satellite imaging and poverty mapping. On each dataset  we show that standard training yields substantially lower out of distribution than in distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts  underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development  we provide an open source package that automates dataset loading  contains default model architectures and hyperparameters  and standardizes evaluations. Code and leaderboards are available at,0
Intrinsic dimension estimation for discrete metrics Real world datasets characterized by discrete features are ubiquitous  from categorical surveys to clinical questionnaires  from unweighted networks to DNA sequences. Nevertheless  the most common unsupervised dimensional reduction methods are designed for continuous spaces  and their use for discrete spaces can lead to errors and biases. In this letter we introduce an algorithm to infer the intrinsic dimension  ID  of datasets embedded in discrete spaces. We demonstrate its accuracy on benchmark datasets  and we apply it to analyze a metagenomic dataset for species fingerprinting  finding a surprisingly small ID  of order  . This suggests that evolutive pressure acts on a low dimensional manifold despite the high dimensionality of sequences  space.,0
Universal Adversarial Triggers for Attacking and Analyzing NLP Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers  input agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient guided search over tokens which finds short trigger sequences  e.g.  one word for classification and four words for language modeling  that successfully trigger the target prediction. For example  triggers cause SNLI entailment accuracy to drop from   .    to  .         of  why  questions in SQuAD to be answered  to kill american people   and the GPT   language model to spew racist output even when conditioned on non racial contexts. Furthermore  although the triggers are optimized using white box access to a specific model  they transfer to other models for all tasks we consider. Finally  since triggers are input agnostic  they provide an analysis of global model behavior. For instance  they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.,0
Adversarial Training for High Stakes Reliability In the future  powerful AI systems may be deployed in high stakes settings  where a single failure could be catastrophic. One technique for improving AI safety in high stakes settings is adversarial training  which uses an adversary to generate examples to train on in order to achieve better worst case performance.,0
Towards Evaluating the Robustness of Neural Networks Neural networks provide state of the art results for most machine learning tasks. Unfortunately  neural networks are vulnerable to adversarial examples  given an input  x  and any target classification  t   it is possible to find a new input  x   that is similar to  x  but classified as  t . This makes it difficult to apply neural networks in security critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network  and increase its robustness  reducing the success rate of current attacks  ability to find adversarial examples from        to   .    .,0
Towards Deep Learning Models Resistant to Adversarial Attacks Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples   inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact  some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem  we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and  in a certain sense  universal. In particular  they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first order adversary as a natural and broad security guarantee. We believe that robustness against such well defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre trained models are available at,0
PixMix  Dreamlike Pictures Comprehensively Improve Safety Measures In real world applications of machine learning  reliable and safe systems must consider measures of performance beyond standard test set accuracy. These other goals include out of distribution  OOD  robustness  prediction consistency  resilience to adversaries  calibrated uncertainty estimates  and the ability to detect anomalous inputs. However  improving performance towards these goals is often a balancing act that today s methods cannot achieve without sacrificing performance on other safety axes. For instance  adversarial training improves adversarial robustness but sharply degrades other classifier performance metrics. Similarly  strong data augmentation and regularization techniques often improve OOD robustness but harm anomaly detection  raising the question of whether a Pareto improvement on all existing safety measures is possible. To meet this challenge  we design a new data augmentation strategy utilizing the natural structural complexity of pictures such as fractals  which outperforms numerous baselines  is near Pareto optimal  and roundly improves safety measures.,0
ImageNet trained CNNs are biased towards texture  increasing shape bias improves accuracy and robustness Convolutional Neural Networks  CNNs  are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture shape cue conflict. We show that ImageNet trained CNNs are strongly biased towards recognising textures rather than shapes  which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture  ResNet     that learns a texture based representation on ImageNet is able to learn a shape based representation instead when trained on  Stylized ImageNet   a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well controlled psychophysical lab setting  nine experiments totalling        psychophysical trials across    observers  and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions  highlighting advantages of a shape based representation.,0
Formulating Robustness Against Unforeseen Attacks Existing defenses against adversarial examples such as adversarial training typically assume that the adversary will conform to a specific or known threat model  such as   ell p  perturbations within a fixed budget. In this paper  we focus on the scenario where there is a mismatch in the threat model assumed by the defense during training  and the actual capabilities of the adversary at test time. We ask the question  if the learner trains against a specific  source  threat model  when can we expect robustness to generalize to a stronger unknown  target  threat model during test time  Our key contribution is to formally define the problem of learning and generalization with an unforeseen adversary  which helps us reason about the increase in adversarial risk from the conventional perspective of a known adversary. Applying our framework  we derive a generalization bound which relates the generalization gap between source and target threat models to variation of the feature extractor  which measures the expected maximum difference between extracted features across a given threat model. Based on our generalization bound  we propose adversarial training with variation regularization  AT VR  which reduces variation of the feature extractor across the source threat model during training. We empirically demonstrate that AT VR can lead to improved generalization to unforeseen attacks during test time compared to standard adversarial training on Gaussian and image datasets.,0
On the Robustness of Safe Reinforcement Learning under Observational Perturbations Safe reinforcement learning  RL  trains a policy to maximize the task reward while satisfying safety constraints. While prior works focus on the performance optimality  we find that the optimal solutions of many safe RL problems are not robust and safe against carefully designed observational perturbations. We formally analyze the unique properties of designing effective state adversarial attackers in the safe RL setting. We show that baseline adversarial attack techniques for standard RL tasks are not always effective for safe RL and proposed two new approaches   one maximizes the cost and the other maximizes the reward. One interesting and counter intuitive finding is that the maximum reward attack is strong  as it can both induce unsafe behaviors and make the attack stealthy by maintaining the reward. We further propose a more effective adversarial training framework for safe RL and evaluate it via comprehensive experiments. This work sheds light on the inherited connection between observational robustness and safety in RL and provides a pioneer work for future safe RL studies.,0
Adversarial Examples for Evaluating Reading Comprehension Systems Standard accuracy metrics indicate that reading comprehension systems are making rapid progress  but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities  we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset  SQuAD . Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences  which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting  the accuracy of sixteen published models drops from an average of        F  score to         when the adversary is allowed to add ungrammatical sequences of words  average accuracy on four models decreases further to      . We hope our insights will motivate the development of new models that understand language more precisely.,0
Smooth Reduce  Leveraging Patches for Improved Certified Robustness Randomized smoothing  RS  has been shown to be a fast  scalable technique for certifying the robustness of deep neural network classifiers. However  methods based on RS require augmenting data with large amounts of noise  which leads to significant drops in accuracy. We propose a training free  modified smoothing approach  Smooth Reduce  that leverages patching and aggregation to provide improved classifier certificates. Our algorithm classifies overlapping patches extracted from an input image  and aggregates the predicted logits to certify a larger radius around the input. We study two aggregation schemes    max and mean    and show that both approaches provide better certificates in terms of certified accuracy  average certified radii and abstention rates as compared to concurrent approaches. We also provide theoretical guarantees for such certificates  and empirically show significant improvements over other randomized smoothing methods that require expensive retraining. Further  we extend our approach to videos and provide meaningful certificates for video classifiers. A project page can be found at,0
Reliable evaluation of adversarial robustness with an ensemble of diverse parameter free attacks The field of defense strategies against adversarial attacks has significantly grown over the last years  but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on  making it difficult to identify the state of the art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks  gradient obfuscation or masking. In this paper we first propose two extensions of the PGD attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter free  computationally affordable and user independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over    models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers  often by more than         identifying several broken defenses.,0
Generalized Beliefs for Cooperative AI Self play is a common paradigm for constructing solutions in Markov games that can yield optimal policies in collaborative settings. However  these policies often adopt highly specialized conventions that make playing with a novel partner difficult. To address this  recent approaches rely on encoding symmetry and convention awareness into policy training  but these require strong environmental assumptions and can complicate policy training. We therefore propose moving the learning of conventions to the belief space. Specifically  we propose a belief learning model that can maintain beliefs over rollouts of policies not seen at training time  and can thus decode and adapt to novel conventions at test time. We show how to leverage this model for both search and training of a best response over various pools of policies to greatly improve ad hoc teamplay. We also show how our setup promotes explainability and interpretability of nuanced agent conventions.,0
Robustness Evaluation of Deep Unsupervised Learning Algorithms for Intrusion Detection Systems Recently  advances in deep learning have been observed in various fields  including computer vision  natural language processing  and cybersecurity. Machine learning  ML  has demonstrated its ability as a potential tool for anomaly detection based intrusion detection systems to build secure computer networks. Increasingly  ML approaches are widely adopted than heuristic approaches for cybersecurity because they learn directly from data. Data is critical for the development of ML systems  and becomes potential targets for attackers. Basically  data poisoning or contamination is one of the most common techniques used to fool ML models through data. This paper evaluates the robustness of six recent deep learning algorithms for intrusion detection on contaminated data. Our experiments suggest that the state of the art algorithms used in this study are sensitive to data contamination and reveal the importance of self defense against data perturbation when developing novel models  especially for intrusion detection systems.,0
Open Problems in Cooperative AI Problems of cooperation  in which agents seek ways to jointly improve their welfare  are ubiquitous and important. They can be found at scales ranging from our daily routines  such as driving on highways  scheduling meetings  and working collaboratively  to our global challenges  such as peace  commerce  and pandemic preparedness. Arguably  the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives  it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation.,0
Forecasting Future World Events with Neural Networks Forecasting future world events is a challenging but valuable task. Forecasts of climate  geopolitical conflict  pandemics and economic indicators help shape policy and decision making. In these domains  the judgment of expert humans contributes to the best forecasts. Given advances in language modeling  can these forecasts be automated  To this end  we introduce Autocast  a dataset containing thousands of forecasting questions and an accompanying news corpus. Questions are taken from forecasting tournaments  ensuring high quality  real world importance  and diversity. The news corpus is organized by date  allowing us to precisely simulate the conditions under which humans made past forecasts  avoiding leakage from the future . Motivated by the difficulty of forecasting numbers across orders of magnitude  e.g. global cases of COVID    in        we also curate IntervalQA  a dataset of numerical questions and metrics for calibration. We test language models on our forecasting task and find that performance is far below a human expert baseline. However  performance improves with increased model size and incorporation of relevant information from the news corpus. In sum  Autocast poses a novel challenge for large language models and improved performance could bring large practical benefits.,0
AnoShift  A Distribution Shift Benchmark for Unsupervised Anomaly Detection Analyzing the distribution shift of data is a growing research direction in nowadays Machine Learning  leading to emerging new benchmarks that focus on providing a suitable scenario for studying the generalization properties of ML models. The existing benchmarks are focused on supervised learning  and to the best of our knowledge  there is none for unsupervised learning. Therefore  we introduce an unsupervised anomaly detection benchmark with data that shifts over time  built over Kyoto        a traffic dataset for network intrusion detection. This kind of data meets the premise of shifting the input distribution  it covers a large time span       years   with naturally occurring changes over time   eg users modifying their behavior patterns  and software updates . We first highlight the non stationary nature of the data  using a basic per feature analysis  t SNE  and an Optimal Transport approach for measuring the overall distribution distances between years. Next  we propose AnoShift  a protocol splitting the data in IID  NEAR  and FAR testing splits. We validate the performance degradation over time with diverse models  MLM to classical Isolation Forest . Finally  we show that by acknowledging the distribution shift problem and properly addressing it  the performance can be improved compared to the classical IID training  by up to        on average . Dataset and code are available at,0
Asleep at the Keyboard  Assessing the Security of GitHub Copilot s Code Contributions There is burgeoning interest in designing AI based systems to assist humans in designing computing systems  including tools that automatically generate computer code. The most notable of these comes in the form of the first self described  AI pair programmer   GitHub Copilot  a language model trained over open source GitHub code. However  code often contains bugs   and so  given the vast quantity of unvetted code that Copilot has processed  it is certain that the language model will have learned from exploitable  buggy code. This raises concerns on the security of Copilot s code contributions. In this work  we systematically investigate the prevalence and conditions that can cause GitHub Copilot to recommend insecure code. To perform this analysis we prompt Copilot to generate code in scenarios relevant to high risk CWEs  e.g. those from MITRE s  Top     list . We explore Copilot s performance on three distinct code generation axes    examining how it performs given diversity of weaknesses  diversity of prompts  and diversity of domains. In total  we produce    different scenarios for Copilot to complete  producing       programs. Of these  we found approximately     to be vulnerable.,0
HierarchicalForecast  A Reference Framework for Hierarchical Forecasting in Python Large collections of time series data are commonly organized into cross sectional structures with different levels of aggregation  examples include product and geographical groupings. A necessary condition for coherent decision making and planning  with such datasets  is for the dis aggregated series  forecasts to add up exactly to the aggregated series forecasts  which motivates the creation of novel hierarchical forecasting algorithms. The growing interest of the Machine Learning community in cross sectional hierarchical forecasting systems states that we are in a propitious moment to ensure that scientific endeavors are grounded on sound baselines. For this reason  we put forward the HierarchicalForecast library  which contains preprocessed publicly available datasets  evaluation metrics  and a compiled set of statistical baseline models. Our Python based framework aims to bridge the gap between statistical  econometric modeling  and Machine Learning forecasting research. Code and documentation are available in,0
Developing Optimal Causal Cyber Defence Agents via Cyber Security Simulation In this paper we explore cyber security defence  through the unification of a novel cyber security simulator with models for  causal  decision making through optimisation. Particular attention is paid to a recently published approach  dynamic causal Bayesian optimisation  DCBO . We propose that DCBO can act as a blue agent when provided with a view of a simulated network and a causal model of how a red agent spreads within that network. To investigate how DCBO can perform optimal interventions on host nodes  in order to reduce the cost of intrusions caused by the red agent. Through this we demonstrate a complete cyber simulation system  which we use to generate observational data for DCBO and provide numerical quantitative results which lay the foundations for future work in this space.,0
Anomal E  A Self Supervised Network Intrusion Detection System based on Graph Neural Networks This paper investigates Graph Neural Networks  GNNs  application for self supervised network intrusion and anomaly detection. GNNs are a deep learning approach for graph based data that incorporate graph structures into learning to generalise graph representations and output embeddings. As network flows are naturally graph based  GNNs are a suitable fit for analysing and learning network behaviour. The majority of current implementations of GNN based Network Intrusion Detection Systems  NIDSs  rely heavily on labelled network traffic which can not only restrict the amount and structure of input traffic  but also the NIDSs potential to adapt to unseen attacks. To overcome these restrictions  we present Anomal E  a GNN approach to intrusion and anomaly detection that leverages edge features and graph topological structure in a self supervised process. This approach is  to the best our knowledge  the first successful and practical approach to network intrusion detection that utilises network flows in a self supervised  edge leveraging GNN. Experimental results on two modern benchmark NIDS datasets not only clearly display the improvement of using Anomal E embeddings rather than raw features  but also the potential Anomal E has for detection on wild network traffic.,0
Intrinsic dimension estimation for discrete metrics Real world datasets characterized by discrete features are ubiquitous  from categorical surveys to clinical questionnaires  from unweighted networks to DNA sequences. Nevertheless  the most common unsupervised dimensional reduction methods are designed for continuous spaces  and their use for discrete spaces can lead to errors and biases. In this letter we introduce an algorithm to infer the intrinsic dimension  ID  of datasets embedded in discrete spaces. We demonstrate its accuracy on benchmark datasets  and we apply it to analyze a metagenomic dataset for species fingerprinting  finding a surprisingly small ID  of order  . This suggests that evolutive pressure acts on a low dimensional manifold despite the high dimensionality of sequences  space.,1
A Universal Trade off Between the Model Size  Test Loss  and Training Loss of Linear Predictors In this work we establish an algorithm and distribution independent non asymptotic trade off between the model size  excess test loss  and training loss of linear predictors. Specifically  we show that models that perform well on the test data  have low excess loss  are either  classical     have training loss close to the noise level  or are  modern     have a much larger number of parameters compared to the minimum needed to fit the training data exactly.,1
Seeking the Truth Beyond the Data. An Unsupervised Machine Learning Approach Clustering is an unsupervised machine learning methodology where unlabeled elements objects are grouped together aiming to the construction of well established clusters that their elements are classified according to their similarity. The goal of this process is to provide a useful aid to the researcher that will help her him to identify patterns among the data. Dealing with large databases  such patterns may not be easily detectable without the contribution of a clustering algorithm. This article provides a deep description of the most widely used clustering methodologies accompanied by useful presentations concerning suitable parameter selection and initializations. Simultaneously  this article not only represents a review highlighting the major elements of examined clustering techniques but emphasizes the comparison of these algorithms  clustering efficiency based on   datasets  revealing their existing weaknesses and capabilities through accuracy and complexity  during the confrontation of discrete and continuous observations. The produced results help us extract valuable conclusions about the appropriateness of the examined clustering techniques in accordance with the dataset s size.,1
Delayed Feedback in Generalised Linear Bandits Revisited The stochastic generalised linear bandit is a well understood model for sequential decision making problems  with many algorithms achieving near optimal regret guarantees under immediate feedback. However  in many real world settings  the requirement that the reward is observed immediately is not applicable. In this setting  standard algorithms are no longer theoretically understood. We study the phenomenon of delayed rewards in a theoretical manner by introducing a delay between selecting an action and receiving the reward. Subsequently  we show that an algorithm based on the optimistic principle improves on existing approaches for this setting by eliminating the need for prior knowledge of the delay distribution and relaxing assumptions on the decision set and the delays. This also leads to improving the regret guarantees from    widetilde O  sqrt dT  sqrt d    mathbb E   tau     to    widetilde O d sqrt T    d       mathbb E   tau     where   mathbb E   tau   denotes the expected delay   d  is the dimension and  T  the time horizon and we have suppressed logarithmic terms. We verify our theoretical results through experiments on simulated data.,1
Deep Hedging  Continuous Reinforcement Learning for Hedging of General Portfolios across Multiple Risk Aversions We present a method for finding optimal hedging policies for arbitrary initial portfolios and market states. We develop a novel actor critic algorithm for solving general risk averse stochastic control problems and use it to learn hedging strategies across multiple risk aversion levels simultaneously. We demonstrate the effectiveness of the approach with a numerical example in a stochastic volatility environment.,1
Uncertainty aware Mixed variable Machine Learning for Materials Design Data driven design shows the promise of accelerating materials discovery but is challenging due to the prohibitive cost of searching the vast design space of chemistry  structure  and synthesis methods. Bayesian Optimization  BO  employs uncertainty aware machine learning models to select promising designs to evaluate  hence reducing the cost. However  BO with mixed numerical and categorical variables  which is of particular interest in materials design  has not been well studied. In this work  we survey frequentist and Bayesian approaches to uncertainty quantification of machine learning with mixed variables. We then conduct a systematic comparative study of their performances in BO using a popular representative model from each group  the random forest based Lolo model  frequentist  and the latent variable Gaussian process model  Bayesian . We examine the efficacy of the two models in the optimization of mathematical functions  as well as properties of structural and functional materials  where we observe performance differences as related to problem dimensionality and complexity. By investigating the machine learning models  predictive and uncertainty estimation capabilities  we provide interpretations of the observed performance differences. Our results provide practical guidance on choosing between frequentist and Bayesian uncertainty aware machine learning models for mixed variable BO in materials design.,1
Matching Normalizing Flows and Probability Paths on Manifolds Continuous Normalizing Flows  CNFs  are a class of generative models that transform a prior distribution to a model distribution by solving an ordinary differential equation  ODE . We propose to train CNFs on manifolds by minimizing probability path divergence  PPD   a novel family of divergences between the probability density path generated by the CNF and a target probability density path. PPD is formulated using a logarithmic mass conservation formula which is a linear first order partial differential equation relating the log target probabilities and the CNF s defining vector field. PPD has several key benefits over existing methods  it sidesteps the need to solve an ODE per iteration  readily applies to manifold data  scales to high dimensions  and is compatible with a large family of target paths interpolating pure noise and data in finite time. Theoretically  PPD is shown to bound classical probability divergences. Empirically  we show that CNFs learned by minimizing PPD achieve state of the art results in likelihoods and sample quality on existing low dimensional manifold benchmarks  and is the first example of a generative model to scale to moderately high dimensional manifolds.,1
Repairing Systematic Outliers by Learning Clean Subspaces in VAEs Data cleaning often comprises outlier detection and data repair. Systematic errors result from nearly deterministic transformations that occur repeatedly in the data  e.g. specific image pixels being set to default values or watermarks. Consequently  models with enough capacity easily overfit to these errors  making detection and repair difficult. Seeing as a systematic outlier is a combination of patterns of a clean instance and systematic error patterns  our main insight is that inliers can be modelled by a smaller representation  subspace  in a model than outliers. By exploiting this  we propose Clean Subspace Variational Autoencoder  CLSVAE   a novel semi supervised model for detection and automated repair of systematic errors. The main idea is to partition the latent space and model inlier and outlier patterns separately. CLSVAE is effective with much less labelled data compared to previous related models  often with less than    of the data. We provide experiments using three image datasets in scenarios with different levels of corruption and labelled set sizes  comparing to relevant baselines. CLSVAE provides superior repairs without human intervention  e.g. with just  .    of labelled data we see a relative error decrease of     compared to the closest baseline.,1
Information Processing Equalities and the Information Risk Bridge We introduce two new classes of measures of information for statistical experiments which generalise and subsume   phi  divergences  integral probability metrics    mathfrak N   distances  MMD   and   f  Gamma   divergences between two or more distributions. This enables us to derive a simple geometrical relationship between measures of information and the Bayes risk of a statistical decision problem  thus extending the variational   phi  divergence representation to multiple distributions in an entirely symmetric manner. The new families of divergence are closed under the action of Markov operators which yields an information processing equality which is a refinement and generalisation of the classical data processing inequality. This equality gives insight into the significance of the choice of the hypothesis class in classical risk minimization.,1
Improved Global Guarantees for the Nonconvex Burer  Monteiro Factorization via Rank Overparameterization We consider minimizing a twice differentiable   L  smooth  and   mu  strongly convex objective   phi  over an  n times n  positive semidefinite matrix  M succeq    under the assumption that the minimizer  M   star   has low rank  r   star  ll n . Following the Burer  Monteiro approach  we instead minimize the nonconvex objective  f X   phi XX  T    over a factor matrix  X  of size  n times r . This substantially reduces the number of variables from  O n       to as few as  O n   and also enforces positive semidefiniteness for free  but at the cost of giving up the convexity of the original problem. In this paper  we prove that if the search rank  r ge r   star   is overparameterized by a constant factor with respect to the true rank  r   star    namely as in  r  frac       L  mu       r   star    then despite nonconvexity  local optimization is guaranteed to globally converge from any initial point to the global optimum. This significantly improves upon a previous rank overparameterization threshold of  r ge n   which is known to be sharp if   phi  is allowed to be nonsmooth and or non strongly convex  but would increase the number of variables back up to  O n      . Conversely  without rank overparameterization  we prove that such a global guarantee is possible if and only if   phi  is almost perfectly conditioned  with a condition number of  L  mu   . Therefore  we conclude that a small amount of overparameterization can lead to large improvements in theoretical guarantees for the nonconvex Burer  Monteiro factorization.,1
Optimal precision for GANs When learning disconnected distributions  Generative adversarial networks  GANs  are known to face model misspecification. Indeed  a continuous mapping from a unimodal latent distribution to a disconnected one is impossible  so GANs necessarily generate samples outside of the support of the target distribution. This raises a fundamental question  what is the latent space partition that minimizes the measure of these areas  Building on a recent result of geometric measure theory  we prove that an optimal GANs must structure its latent space as a  simplicial cluster    a Voronoi partition where cells are convex cones   when the dimension of the latent space is larger than the number of modes. In this configuration  each Voronoi cell maps to a distinct mode of the data. We derive both an upper and a lower bound on the optimal precision of GANs learning disconnected manifolds. Interestingly  these two bounds have the same order of decrease    sqrt  log m     m  being the number of modes. Finally  we perform several experiments to exhibit the geometry of the latent space and experimentally show that GANs have a geometry with similar properties to the theoretical one.,1
ASR Error Detection via Audio Transcript entailment Despite improved performances of the latest Automatic Speech Recognition  ASR  systems  transcription errors are still unavoidable. These errors can have a considerable impact in critical domains such as healthcare  when used to help with clinical documentation. Therefore  detecting ASR errors is a critical first step in preventing further error propagation to downstream applications. To this end  we propose a novel end to end approach for ASR error detection using audio transcript entailment. To the best of our knowledge  we are the first to frame this problem as an end to end entailment task between the audio segment and its corresponding transcript segment. Our intuition is that there should be a bidirectional entailment between audio and transcript when there is no recognition error and vice versa. The proposed model utilizes an acoustic encoder and a linguistic encoder to model the speech and transcript respectively. The encoded representations of both modalities are fused to predict the entailment. Since doctor patient conversations are used in our experiments  a particular emphasis is placed on medical terms. Our proposed model achieves classification error rates  CER  of   .   on all transcription errors and     on medical errors specifically  leading to improvements upon a strong baseline by     and   .    respectively.,1
Goal Oriented Sensitivity Analysis of Hyperparameters in Deep Learning Tackling new machine learning problems with neural networks always means optimizing numerous hyperparameters that define their structure and strongly impact their performances. In this work  we study the use of goal oriented sensitivity analysis  based on the Hilbert Schmidt Independence Criterion  HSIC   for hyperparameter analysis and optimization. Hyperparameters live in spaces that are often complex and awkward. They can be of different natures  categorical  discrete  boolean  continuous   interact  and have inter dependencies. All this makes it non trivial to perform classical sensitivity analysis. We alleviate these difficulties to obtain a robust analysis index that is able to quantify hyperparameters  relative impact on a neural network s final error. This valuable tool allows us to better understand hyperparameters and to make hyperparameter optimization more interpretable. We illustrate the benefits of this knowledge in the context of hyperparameter optimization and derive an HSIC based optimization algorithm that we apply on MNIST and Cifar  classical machine learning data sets  but also on the approximation of Runge function and Bateman equations solution  of interest for scientific machine learning. This method yields neural networks that are both competitive and cost effective.,1
AMLB  an AutoML Benchmark Comparing different AutoML frameworks is notoriously challenging and often done incorrectly. We introduce an open and extensible benchmark that follows best practices and avoids common mistakes when comparing AutoML frameworks. We conduct a thorough comparison of   well known AutoML frameworks across    classification and    regression tasks. The differences between the AutoML frameworks are explored with a multi faceted analysis  evaluating model accuracy  its trade offs with inference time  and framework failures. We also use Bradley Terry trees to discover subsets of tasks where the relative AutoML framework rankings differ. The benchmark comes with an open source tool that integrates with many AutoML frameworks and automates the empirical evaluation process end to end  from framework installation and resource allocation to in depth evaluation. The benchmark uses public data sets  can be easily extended with other AutoML frameworks and tasks  and has a website with up to date results.,1
Probing the Robustness of Independent Mechanism Analysis for Representation Learning One aim of representation learning is to recover the original latent code that generated the data  a task which requires additional information or inductive biases. A recently proposed approach termed Independent Mechanism Analysis  IMA  postulates that each latent source should influence the observed mixtures independently  complementing standard nonlinear independent component analysis  and taking inspiration from the principle of independent causal mechanisms. While it was shown in theory and experiments that IMA helps recovering the true latents  the method s performance was so far only characterized when the modeling assumptions are exactly satisfied. Here  we test the method s robustness to violations of the underlying assumptions. We find that the benefits of IMA based regularization for recovering the true sources extend to mixing functions with various degrees of violation of the IMA principle  while standard regularizers do not provide the same merits. Moreover  we show that unregularized maximum likelihood recovers mixing functions which systematically deviate from the IMA principle  and provide an argument elucidating the benefits of IMA based regularization.,1
Parallel APSM for Fast and Adaptive Digital SIC in Full Duplex Transceivers with Nonlinearity This paper presents a kernel based adaptive filter that is applied for the digital domain self interference cancellation  SIC  in a transceiver operating in full duplex  FD  mode. In FD  the benefit of simultaneous transmission and receiving of signals comes at the price of strong self interference  SI . In this work  we are primarily interested in suppressing the SI using an adaptive filter namely adaptive projected subgradient method  APSM  in a reproducing kernel Hilbert space  RKHS  of functions. Using the projection concept as a powerful tool  APSM is used to model and consequently remove the SI. A low complexity and fast tracking algorithm is provided taking advantage of parallel projections as well as the kernel trick in RKHS. The performance of the proposed method is evaluated on real measurement data. The method illustrates the good performance of the proposed adaptive filter  compared to the known popular benchmarks. They demonstrate that the kernel based algorithm achieves a favorable level of digital SIC while enabling parallel computation based implementation within a rich and nonlinear function space  thanks to the employed adaptive filtering method.,1
Modeling Randomly Walking Volatility with Chained Gamma Distributions Volatility clustering is a common phenomenon in financial time series. Typically  linear models can be used to describe the temporal autocorrelation of the  logarithmic  variance of returns. Considering the difficulty in estimating this model  we construct a Dynamic Bayesian Network  which utilizes the conjugate prior relation of normal gamma and gamma gamma  so that its posterior form locally remains unchanged at each node. This makes it possible to find approximate solutions using variational methods quickly. Furthermore  we ensure that the volatility expressed by the model is an independent incremental process after inserting dummy gamma nodes between adjacent time steps. We have found that this model has two advantages     It can be proved that it can express heavier tails than Gaussians  i.e.  have positive excess kurtosis  compared to popular linear models.    If the variational inference VI  is used for state estimation  it runs much faster than Monte Carlo MC  methods since the calculation of the posterior uses only basic arithmetic operations. And its convergence process is deterministic.,1
Supervising Embedding Algorithms Using the Stress While classical scaling  just like principal component analysis  is parameter free  most other methods for embedding multivariate data require the selection of one or several parameters. This tuning can be difficult due to the unsupervised nature of the situation. We propose a simple  almost obvious  approach to supervise the choice of tuning parameter s   minimize a notion of stress. We substantiate this choice by reference to rigidity theory. We extend a result by Aspnes et al.  IEEE Mobile Computing         showing that general random geometric graphs are trilateration graphs with high probability. And we provide a stability result   la Anderson et al.  SIAM Discrete Mathematics       . We illustrate this approach in the context of the MDS MAP P  algorithm of Shang and Ruml  IEEE INFOCOM       . As a prototypical patch stitching method  it requires the choice of patch size  and we use the stress to make that choice data driven. In this context  we perform a number of experiments to illustrate the validity of using the stress as the basis for tuning parameter selection. In so doing  we uncover a bias variance tradeoff  which is a phenomenon which may have been overlooked in the multidimensional scaling literature. By turning MDS MAP P  into a method for manifold learning  we obtain a local version of Isomap for which the minimization of the stress may also be used for parameter tuning.,1
Learning Bellman Complete Representations for Offline Policy Evaluation We study representation learning for Offline Reinforcement Learning  RL   focusing on the important task of Offline Policy Evaluation  OPE . Recent work shows that  in contrast to supervised learning  realizability of the Q function is not enough for learning it. Two sufficient conditions for sample efficient OPE are Bellman completeness and coverage. Prior work often assumes that representations satisfying these conditions are given  with results being mostly theoretical in nature. In this work  we propose BCRL  which directly learns from data an approximately linear Bellman complete representation with good coverage. With this learned representation  we perform OPE using Least Square Policy Evaluation  LSPE  with linear functions in our learned representation. We present an end to end theoretical analysis  showing that our two stage algorithm enjoys polynomial sample complexity provided some representation in the rich class considered is linear Bellman complete. Empirically  we extensively evaluate our algorithm on challenging  image based continuous control tasks from the Deepmind Control Suite. We show our representation enables better OPE compared to previous representation learning methods developed for off policy RL  e.g.  CURL  SPR . BCRL achieve competitive OPE error with the state of the art method Fitted Q Evaluation  FQE   and beats FQE when evaluating beyond the initial state distribution. Our ablations show that both linear Bellman complete and coverage components of our method are crucial.,1
Future Dependent Value Based Off Policy Evaluation in POMDPs We study off policy evaluation  OPE  for partially observable MDPs  POMDPs  with general function approximation. Existing methods such as sequential importance sampling estimators and fitted Q evaluation suffer from the curse of horizon in POMDPs. To circumvent this problem  we develop a novel model free OPE method by introducing future dependent value functions that take future proxies as inputs. Future dependent value functions play similar roles as classical value functions in fully observable MDPs. We derive a new Bellman equation for future dependent value functions as conditional moment equations that use history proxies as instrumental variables. We further propose a minimax learning method to learn future dependent value functions using the new Bellman equation. We obtain the PAC result  which implies our OPE estimator is consistent as long as futures and histories contain sufficient information about latent states  and the Bellman completeness. Finally  we extend our methods to learning of dynamics and establish the connection between our approach and the well known spectral learning methods in POMDPs.,1
Flowsheet synthesis through hierarchical reinforcement learning and graph neural networks Process synthesis experiences a disruptive transformation accelerated by digitization and artificial intelligence. We propose a reinforcement learning algorithm for chemical process design based on a state of the art actor critic logic. Our proposed algorithm represents chemical processes as graphs and uses graph convolutional neural networks to learn from process graphs. In particular  the graph neural networks are implemented within the agent architecture to process the states and make decisions. Moreover  we implement a hierarchical and hybrid decision making process to generate flowsheets  where unit operations are placed iteratively as discrete decisions and corresponding design variables are selected as continuous decisions. We demonstrate the potential of our method to design economically viable flowsheets in an illustrative case study comprising equilibrium reactions  azeotropic separation  and recycles. The results show quick learning in discrete  continuous  and hybrid action spaces. Due to the flexible architecture of the proposed reinforcement learning agent  the method is predestined to include large action state spaces and an interface to process simulators in future research.,1
Multi Model Federated Learning with Provable Guarantees Federated Learning  FL  is a variant of distributed learning where edge devices collaborate to learn a model without sharing their data with the central server or each other. We refer to the process of training multiple independent models simultaneously in a federated setting using a common pool of clients as multi model FL. In this work  we propose two variants of the popular FedAvg algorithm for multi model FL  with provable convergence guarantees. We further show that for the same amount of computation  multi model FL can have better performance than training each model separately. We supplement our theoretical results with experiments in strongly convex  convex  and non convex settings.,1
Exploration in Linear Bandits with Rich Action Sets and its Implications for Inference We present a non asymptotic lower bound on the eigenspectrum of the design matrix generated by any linear bandit algorithm with sub linear regret when the action set has well behaved curvature. Specifically  we show that the minimum eigenvalue of the expected design matrix grows as   Omega  sqrt n    whenever the expected cumulative regret of the algorithm is  O  sqrt n     where  n  is the learning horizon  and the action space has a constant Hessian around the optimal arm. This shows that such action spaces force a polynomial lower bound rather than a logarithmic lower bound  as shown by  cite lattimore    end   in discrete  i.e.  well separated  action spaces. Furthermore  while the previous result is shown to hold only in the asymptotic regime  as  n  to  infty    our result for these   locally rich  action spaces is any time. Additionally  under a mild technical assumption  we obtain a similar lower bound on the minimum eigen value holding with high probability.,1
Video Coding Using Learned Latent GAN Compression We propose in this paper a new paradigm for facial video compression. We leverage the generative capacity of GANs such as StyleGAN to represent and compress a video  including intra and inter compression. Each frame is inverted in the latent space of StyleGAN  from which the optimal compression is learned. To do so  a diffeomorphic latent representation is learned using a normalizing flows model  where an entropy model can be optimized for image coding. In addition  we propose a new perceptual loss that is more efficient than other counterparts. Finally  an entropy model for video inter coding with residual is also learned in the previously constructed latent representation. Our method  SGANC  is simple  faster to train  and achieves better results for image and video coding compared to state of the art codecs such as VTM  AV   and recent deep learning techniques. In particular  it drastically minimizes perceptual distortion at low bit rates.,1
Variational Flow Graphical Model This paper introduces a novel approach to embed flow based models with hierarchical structures. The proposed framework is named Variational Flow Graphical  VFG  Model. VFGs learn the representation of high dimensional data via a message passing scheme by integrating flow based functions through variational inference. By leveraging the expressive power of neural networks  VFGs produce a representation of the data using a lower dimension  thus overcoming the drawbacks of many flow based models  usually requiring a high dimensional latent space involving many trivial variables. Aggregation nodes are introduced in the VFG models to integrate forward backward hierarchical information via a message passing scheme. Maximizing the evidence lower bound  ELBO  of data likelihood aligns the forward and backward messages in each aggregation node achieving a consistency node state. Algorithms have been developed to learn model parameters through gradient updating regarding the ELBO objective.,1
Variational Inference for Additive Main and Multiplicative Interaction Effects Models In plant breeding the presence of a genotype by environment  GxE  interaction has a strong impact on cultivation decision making and the introduction of new crop cultivars. The combination of linear and bilinear terms has been shown to be very useful in modelling this type of data. A widely used approach to identify GxE is the Additive Main Effects and Multiplicative Interaction Effects  AMMI  model. However  as data frequently can be high dimensional  Markov chain Monte Carlo  MCMC  approaches can be computationally infeasible. In this article  we consider a variational inference approach for such a model. We derive variational approximations for estimating the parameters and we compare the approximations to MCMC using both simulated and real data. The new inferential framework we propose is on average two times faster whilst maintaining the same predictive performance as MCMC.,1
MAPIE  an open source library for distribution free uncertainty quantification Estimating uncertainties associated with the predictions of Machine Learning  ML  models is of crucial importance to assess their robustness and predictive power. In this submission  we introduce MAPIE  Model Agnostic Prediction Interval Estimator   an open source Python library that quantifies the uncertainties of ML models for single output regression and multi class classification tasks. MAPIE implements conformal prediction methods  allowing the user to easily compute uncertainties with strong theoretical guarantees on the marginal coverages and with mild assumptions on the model or on the underlying data distribution. MAPIE is hosted on scikit learn contrib and is fully  scikit learn compatible . As such  it accepts any type of regressor or classifier coming with a scikit learn API. The library is available at ,1
Ranking in Contextual Multi Armed Bandits We study a ranking problem in the contextual multi armed bandit setting. A learning agent selects an ordered list of items at each time step and observes stochastic outcomes for each position. In online recommendation systems  showing an ordered list of the most attractive items would not be the best choice since both position and item dependencies result in a complicated reward function. A very naive example is the lack of diversity when all the most attractive items are from the same category. We model position and item dependencies in the ordered list and design UCB and Thompson Sampling type algorithms for this problem. We prove that the regret bound over  T  rounds and  L  positions is   Tilde O  L sqrt d T     which has the same order as the previous works with respect to  T  and only increases linearly with  L . Our work generalizes existing studies in several directions  including position dependencies where position discount is a particular case  and proposes a more general contextual bandit model.,1
Statistical Hypothesis Testing Based on Machine Learning  Large Deviations Analysis We study the performance    and specifically the rate at which the error probability converges to zero    of Machine Learning  ML  classification techniques. Leveraging the theory of large deviations  we provide the mathematical conditions for a ML classifier to exhibit error probabilities that vanish exponentially  say   sim  exp left  n  I   o n   right    where  n  is the number of informative observations available for testing  or another relevant parameter  such as the size of the target in an image  and  I  is the error rate. Such conditions depend on the Fenchel Legendre transform of the cumulant generating function of the Data Driven Decision Function  D F  i.e.  what is thresholded before the final binary decision is made  learned in the training phase. As such  the D F and  consequently  the related error rate  I   depend on the given training set  which is assumed of finite size. Interestingly  these conditions can be verified and tested numerically exploiting the available dataset  or a synthetic dataset  generated according to the available information on the underlying statistical model. In other words  the classification error probability convergence to zero and its rate can be computed on a portion of the dataset available for training. Coherently with the large deviations theory  we can also establish the convergence  for  n  large enough  of the normalized D F statistic to a Gaussian distribution. This property is exploited to set a desired asymptotic false alarm probability  which empirically turns out to be accurate even for quite realistic values of  n . Furthermore  approximate error probability curves   sim  zeta n  exp left  n  I  right   are provided  thanks to the refined asymptotic derivation  often referred to as exact asymptotics   where   zeta n  represents the most representative sub exponential terms of the error probabilities.,1
Variational Neural Networks Bayesian Neural Networks  BNNs  provide a tool to estimate the uncertainty of a neural network by considering a distribution over weights and sampling different models for each input. In this paper  we propose a method for uncertainty estimation in neural networks called Variational Neural Network that  instead of considering a distribution over weights  generates parameters for the output distribution of a layer by transforming its inputs with learnable sub layers. In uncertainty quality estimation experiments  we show that VNNs achieve better uncertainty quality than Monte Carlo Dropout or Bayes By Backpropagation methods.,1
Analysis of Catastrophic Forgetting for Random Orthogonal Transformation Tasks in the Overparameterized Regime Overparameterization is known to permit strong generalization performance in neural networks. In this work  we provide an initial theoretical analysis of its effect on catastrophic forgetting in a continual learning setup. We show experimentally that in permuted MNIST image classification tasks  the generalization performance of multilayer perceptrons trained by vanilla stochastic gradient descent can be improved by overparameterization  and the extent of the performance increase achieved by overparameterization is comparable to that of state of the art continual learning algorithms. We provide a theoretical explanation of this effect by studying a qualitatively similar two task linear regression problem  where each task is related by a random orthogonal transformation. We show that when a model is trained on the two tasks in sequence without any additional regularization  the risk gain on the first task is small if the model is sufficiently overparameterized.,1
Distributed Online System Identification for LTI Systems Using Reverse Experience Replay Identification of linear time invariant  LTI  systems plays an important role in control and reinforcement learning. Both asymptotic and finite time offline system identification are well studied in the literature. For online system identification  the idea of stochastic gradient descent with reverse experience replay  SGD RER  was recently proposed  where the data sequence is stored in several buffers and the stochastic gradient descent  SGD  update performs backward in each buffer to break the time dependency between data points. Inspired by this work  we study distributed online system identification of LTI systems over a multi agent network. We consider agents as identical LTI systems  and the network goal is to jointly estimate the system parameters by leveraging the communication between agents. We propose DSGD RER  a distributed variant of the SGD RER algorithm  and theoretically characterize the improvement of the estimation error with respect to the network size. Our numerical experiments certify the reduction of estimation error as the network size grows.,1
Lazy Estimation of Variable Importance for Large Neural Networks As opaque predictive models increasingly impact many areas of modern life  interest in quantifying the importance of a given input variable for making a specific prediction has grown. Recently  there has been a proliferation of model agnostic methods to measure variable importance  VI  that analyze the difference in predictive power between a full model trained on all variables and a reduced model that excludes the variable s  of interest. A bottleneck common to these methods is the estimation of the reduced model for each variable  or subset of variables   which is an expensive process that often does not come with theoretical guarantees. In this work  we propose a fast and flexible method for approximating the reduced model with important inferential guarantees. We replace the need for fully retraining a wide neural network by a linearization initialized at the full model parameters. By adding a ridge like penalty to make the problem convex  we prove that when the ridge penalty parameter is sufficiently large  our method estimates the variable importance measure with an error rate of  O  frac     sqrt n     where  n  is the number of training samples. We also show that our estimator is asymptotically normal  enabling us to provide confidence bounds for the VI estimates. We demonstrate through simulations that our method is fast and accurate under several data generating regimes  and we demonstrate its real world applicability on a seasonal climate forecasting example.,1
Finite time High probability Bounds for Polyak Ruppert Averaged Iterates of Linear Stochastic Approximation This paper provides a finite time analysis of linear stochastic approximation  LSA  algorithms with fixed step size  a core method in statistics and machine learning. LSA is used to compute approximate solutions of a  d  dimensional linear system   bar  mathbf A    theta    bar  mathbf b     for which    bar  mathbf A     bar  mathbf b     can only be estimated through  asymptotically  unbiased observations      mathbf A  Z n   mathbf b  Z n      n  in  mathbb N   . We consider here the case where    Z n    n  in  mathbb N    is an i.i.d. sequence or a uniformly geometrically ergodic Markov chain  and derive  p  moments inequality and high probability bounds for the iterates defined by LSA and its Polyak Ruppert averaged version. More precisely  we establish bounds of order   p  alpha t   operatorname mix         d    p   on the  p  th moment of the last iterate of LSA. In this formula   alpha  is the step size of the procedure and  t   operatorname mix    is the mixing time of the underlying chain   t   operatorname mix      in the i.i.d. setting . We then prove finite time instance dependent bounds on the Polyak Ruppert averaged sequence of iterates. These results are sharp in the sense that the leading term we obtain matches the local asymptotic minimax limit  including tight dependence on the parameters   d t   operatorname mix     in the higher order terms.,1
A Certifiable Security Patch for Object Tracking in Self Driving Systems via Historical Deviation Modeling Self driving cars  SDC  commonly implement the perception pipeline to detect the surrounding obstacles and track their moving trajectories  which lays the ground for the subsequent driving decision making process. Although the security of obstacle detection in SDC is intensively studied  not until very recently the attackers start to exploit the vulnerability of the tracking module. Compared with solely attacking the object detectors  this new attack strategy influences the driving decision more effectively with less attack budgets. However  little is known on whether the revealed vulnerability remains effective in end to end self driving systems and  if so  how to mitigate the threat.,1
Fast Composite Optimization and Statistical Recovery in Federated Learning As a prevalent distributed learning paradigm  Federated Learning  FL  trains a global model on a massive amount of devices with infrequent communication. This paper investigates a class of composite optimization and statistical recovery problems in the FL setting  whose loss function consists of a data dependent smooth loss and a non smooth regularizer. Examples include sparse linear regression using Lasso  low rank matrix recovery using nuclear norm regularization  etc. In the existing literature  federated composite optimization algorithms are designed only from an optimization perspective without any statistical guarantees. In addition  they do not consider commonly used  restricted  strong convexity in statistical recovery problems. We advance the frontiers of this problem from both optimization and statistical perspectives. From optimization upfront  we propose a new algorithm named  textit Fast Federated Dual Averaging  for strongly convex and smooth loss and establish state of the art iteration and communication complexity in the composite setting. In particular  we prove that it enjoys a fast rate  linear speedup  and reduced communication rounds. From statistical upfront  for restricted strongly convex and smooth loss  we design another algorithm  namely  textit Multi stage Federated Dual Averaging   and prove a high probability complexity bound with linear speedup up to optimal statistical precision. Experiments in both synthetic and real data demonstrate that our methods perform better than other baselines. To the best of our knowledge  this is the first work providing fast optimization algorithms and statistical recovery guarantees for composite problems in FL.,1
Fuzzy Clustering by Hyperbolic Smoothing We propose a novel method for building fuzzy clusters of large data sets  using a smoothing numerical approach. The usual sum of squares criterion is relaxed so the search for good fuzzy partitions is made on a continuous space  rather than a combinatorial space as in classical methods  cite Hartigan . The smoothing allows a conversion from a strongly non differentiable problem into differentiable subproblems of optimization without constraints of low dimension  by using a differentiable function of infinite class. For the implementation of the algorithm we used the statistical software  R  and the results obtained were compared to the traditional fuzzy  C   means method  proposed by Bezdek.,1
Single Model Uncertainty Estimation via Stochastic Data Centering We are interested in estimating the uncertainties of deep neural networks  which play an important role in many scientific and engineering problems. In this paper  we present a striking new finding that an ensemble of neural networks with the same weight initialization  trained on datasets that are shifted by a constant bias gives rise to slightly inconsistent trained models  where the differences in predictions are a strong indicator of epistemic uncertainties. Using the neural tangent kernel  NTK   we demonstrate that this phenomena occurs in part because the NTK is not shift invariant. Since this is achieved via a trivial input transformation  we show that it can therefore be approximated using just a single neural network    using a technique that we call   Delta  UQ    that estimates uncertainty around prediction by marginalizing out the effect of the biases. We show that   Delta  UQ s uncertainty estimates are superior to many of the current methods on a variety of benchmarks    outlier rejection  calibration under distribution shift  and sequential design optimization of black box functions.,1
Switching One Versus the Rest Loss to Increase the Margin of Logits for Adversarial Robustness Defending deep neural networks against adversarial examples is a key challenge for AI safety. To improve the robustness effectively  recent methods focus on important data points near the decision boundary in adversarial training. However  these methods are vulnerable to Auto Attack  which is an ensemble of parameter free attacks for reliable evaluation. In this paper  we experimentally investigate the causes of their vulnerability and find that existing methods reduce margins between logits for the true label and the other labels while keeping their gradient norms non small values. Reduced margins and non small gradient norms cause their vulnerability since the largest logit can be easily flipped by the perturbation. Our experiments also show that the histogram of the logit margins has two peaks  i.e.  small and large logit margins. From the observations  we propose switching one versus the rest loss  SOVR   which uses one versus the rest loss when data have small logit margins so that it increases the margins. We find that SOVR increases logit margins more than existing methods while keeping gradient norms small and outperforms them in terms of the robustness against Auto Attack.,1
Making Linear MDPs Practical via Contrastive Representation Learning It is common to address the curse of dimensionality in Markov decision processes  MDPs  by exploiting low rank representations. This motivates much of the recent theoretical study on linear MDPs. However  most approaches require a given representation under unrealistic assumptions about the normalization of the decomposition or introduce unresolved computational challenges in practice. Instead  we consider an alternative definition of linear MDPs that automatically ensures normalization while allowing efficient representation learning via contrastive estimation. The framework also admits confidence adjusted index algorithms  enabling an efficient and principled approach to incorporating optimism or pessimism in the face of uncertainty. To the best of our knowledge  this provides the first practical representation learning method for linear MDPs that achieves both strong theoretical guarantees and empirical performance. Theoretically  we prove that the proposed algorithm is sample efficient in both the online and offline settings. Empirically  we demonstrate superior performance over existing state of the art model based and model free algorithms on several benchmarks.,1
Minimax Rates for Robust Community Detection In this work  we study the problem of community detection in the stochastic block model with adversarial node corruptions. Our main result is an efficient algorithm that can tolerate an   epsilon  fraction of corruptions and achieves error  O  epsilon    e    frac C         pm o       where  C     sqrt a     sqrt b      is the signal to noise ratio and  a n  and  b n  are the inter community and intra community connection probabilities respectively. These bounds essentially match the minimax rates for the SBM without corruptions. We also give robust algorithms for   mathbb Z     synchronization. At the heart of our algorithm is a new semidefinite program that uses global information to robustly boost the accuracy of a rough clustering. Moreover  we show that our algorithms are doubly robust in the sense that they work in an even more challenging noise model that mixes adversarial corruptions with unbounded monotone changes  from the semi random model.,1
Probabilistic Reconciliation of Count Time Series We propose a principled method for the reconciliation of any probabilistic base forecasts. We show how probabilistic reconciliation can be obtained by merging  via Bayes  rule  the information contained in the base forecast for the bottom and the upper time series. We illustrate our method on a toy hierarchy  showing how our framework allows the probabilistic reconciliation of any base forecast. We perform experiment in the reconciliation of temporal hierarchies of count time series  obtaining major improvements compared to probabilistic reconciliation based on the Gaussian or the truncated Gaussian distribution.,1
Riemannian Diffusion Schr dinger Bridge Score based generative models exhibit state of the art performance on density estimation and generative modeling tasks. These models typically assume that the data geometry is flat  yet recent extensions have been developed to synthesize data living on Riemannian manifolds. Existing methods to accelerate sampling of diffusion models are typically not applicable in the Riemannian setting and Riemannian score based methods have not yet been adapted to the important task of interpolation of datasets. To overcome these issues  we introduce  emph Riemannian Diffusion Schr dinger Bridge . Our proposed method generalizes Diffusion Schr dinger Bridge introduced in  cite debortoli    neurips  to the non Euclidean setting and extends Riemannian score based models beyond the first time reversal. We validate our proposed method on synthetic data and real Earth and climate data.,1
On the Super exponential Quantum Speedup of Equivariant Quantum Machine Learning Algorithms with SU  d   Symmetry We introduce a framework of the equivariant convolutional algorithms which is tailored for a number of machine learning tasks on physical systems with arbitrary SU  d   symmetries. It allows us to enhance a natural model of quantum computation  permutational quantum computing  PQC   Quantum Inf. Comput.                        and defines a more powerful model  PQC . While PQC was shown to be effectively classically simulatable  we exhibit a problem which can be efficiently solved on PQC  machine  whereas the best known classical algorithms runs in  O n n     time  thus providing strong evidence against PQC  being classically simulatable. We further discuss practical quantum machine learning algorithms which can be carried out in the paradigm of PQC .,1
Long Term Fairness for Minority Groups via Performative Distributionally Robust Optimization Fairness researchers in machine learning  ML  have coalesced around several fairness criteria which provide formal definitions of what it means for an ML model to be fair. However  these criteria have some serious limitations. We identify four key shortcomings of these formal fairness criteria  and aim to help to address them by extending performative prediction to include a distributionally robust objective.,1
Kullback Leibler and Renyi divergences in reproducing kernel Hilbert space and Gaussian process settings In this work  we present formulations for regularized Kullback Leibler and R nyi divergences via the Alpha Log Determinant  Log Det  divergences between positive Hilbert Schmidt operators on Hilbert spaces in two different settings  namely  i  covariance operators and Gaussian measures defined on reproducing kernel Hilbert spaces  RKHS   and  ii  Gaussian processes with squared integrable sample paths. For characteristic kernels  the first setting leads to divergences between arbitrary Borel probability measures on a complete  separable metric space. We show that the Alpha Log Det divergences are continuous in the Hilbert Schmidt norm  which enables us to apply laws of large numbers for Hilbert space valued random variables. As a consequence of this  we show that  in both settings  the infinite dimensional divergences can be consistently and efficiently estimated from their finite dimensional versions  using finite dimensional Gram matrices Gaussian measures and finite sample data  with   it dimension independent  sample complexities in all cases. RKHS methodology plays a central role in the theoretical analysis in both settings. The mathematical formulation is illustrated by numerical experiments.,1
Quantum Advantage in Variational Bayes Inference Variational Bayes  VB  inference algorithm is used widely to estimate both the parameters and the unobserved hidden variables in generative statistical models. The algorithm    inspired by variational methods used in computational physics    is iterative and can get easily stuck in local minima  even when classical techniques  such as deterministic annealing  DA   are used. We study a variational Bayes  VB  inference algorithm based on a non traditional quantum annealing approach    referred to as quantum annealing variational Bayes  QAVB  inference    and show that there is indeed a quantum advantage to QAVB over its classical counterparts. In particular  we show that such better performance is rooted in key concepts from quantum mechanics   i  the ground state of the Hamiltonian of a quantum system    defined from the given variational Bayes  VB  problem    corresponds to an optimal solution for the minimization problem of the variational free energy at very low temperatures   ii  such a ground state can be achieved by a technique paralleling the quantum annealing process  and  iii  starting from this ground state  the optimal solution to the VB problem can be achieved by increasing the heat bath temperature to unity  and thereby avoiding local minima introduced by spontaneous symmetry breaking observed in classical physics based VB algorithms. We also show that the update equations of QAVB can be potentially implemented using   lceil  log K  rceil  qubits and   mathcal O   K   operations per step. Thus  QAVB can match the time complexity of existing VB algorithms  while delivering higher performance.,1
Kernel based Federated Learning with Personalization We consider federated learning with personalization  where in addition to a global objective  each client is also interested in maximizing a personalized local objective. We consider this problem under a general continuous action space setting where the objective functions belong to a reproducing kernel Hilbert space. We propose algorithms based on surrogate Gaussian process  GP  models that achieve the optimal regret order  up to polylogarithmic factors . Furthermore  we show that the sparse approximations of the GP models significantly reduce the communication cost across clients.,1
Approximation Power of Deep Neural Networks  an explanatory mathematical survey The goal of this survey is to present an explanatory review of the approximation properties of deep neural networks. Specifically  we aim at understanding how and why deep neural networks outperform other classical linear and nonlinear approximation methods. This survey consists of three chapters. In Chapter   we review the key ideas and concepts underlying deep networks and their compositional nonlinear structure. We formalize the neural network problem by formulating it as an optimization problem when solving regression and classification problems. We briefly discuss the stochastic gradient descent algorithm and the back propagation formulas used in solving the optimization problem and address a few issues related to the performance of neural networks  including the choice of activation functions  cost functions  overfitting issues  and regularization. In Chapter   we shift our focus to the approximation theory of neural networks. We start with an introduction to the concept of density in polynomial approximation and in particular study the Stone Weierstrass theorem for real valued continuous functions. Then  within the framework of linear approximation  we review a few classical results on the density and convergence rate of feedforward networks  followed by more recent developments on the complexity of deep networks in approximating Sobolev functions. In Chapter    utilizing nonlinear approximation theory  we further elaborate on the power of depth and approximation superiority of deep ReLU networks over other classical methods of nonlinear approximation.,1
BiTAT  Neural Network Binarization with Task dependent Aggregated Transformation Neural network quantization aims to transform high precision weights and activations of a given neural network into low precision weights activations for reduced memory usage and computation  while preserving the performance of the original model. However  extreme quantization    bit weight   bit activations  of compactly designed backbone architectures  e.g.  MobileNets  often used for edge device deployments results in severe performance degeneration. This paper proposes a novel Quantization Aware Training  QAT  method that can effectively alleviate performance degeneration even with extreme quantization by focusing on the inter weight dependencies  between the weights within each layer and across consecutive layers. To minimize the quantization impact of each weight on others  we perform an orthonormal transformation of the weights at each layer by training an input dependent correlation matrix and importance vector  such that each weight is disentangled from the others. Then  we quantize the weights based on their importance to minimize the loss of the information from the original weights activations. We further perform progressive layer wise quantization from the bottom layer to the top  so that quantization at each layer reflects the quantized distributions of weights and activations at previous layers. We validate the effectiveness of our method on various benchmark datasets against strong neural quantization baselines  demonstrating that it alleviates the performance degeneration on ImageNet and successfully preserves the full precision model performance on CIFAR     with compact backbone networks.,1
A Forward Propagation Algorithm for Online Optimization of Nonlinear Stochastic Differential Equations Optimizing over the stationary distribution of stochastic differential equations  SDEs  is computationally challenging. A new forward propagation algorithm has been recently proposed for the online optimization of SDEs. The algorithm solves an SDE  derived using forward differentiation  which provides a stochastic estimate for the gradient. The algorithm continuously updates the SDE model s parameters and the gradient estimate simultaneously. This paper studies the convergence of the forward propagation algorithm for nonlinear dissipative SDEs. We leverage the ergodicity of this class of nonlinear SDEs to characterize the convergence rate of the transition semi group and its derivatives. Then  we prove bounds on the solution of a Poisson partial differential equation  PDE  for the expected time integral of the algorithm s stochastic fluctuations around the direction of steepest descent. We then re write the algorithm using the PDE solution  which allows us to characterize the parameter evolution around the direction of steepest descent. Our main result is a convergence theorem for the forward propagation algorithm for nonlinear dissipative SDEs.,1
Mean field Variational Inference via Wasserstein Gradient Flow Variational inference  VI  provides an appealing alternative to traditional sampling based approaches for implementing Bayesian inference due to its conceptual simplicity  statistical accuracy and computational scalability. However  common variational approximation schemes  such as the mean field  MF  approximation  require certain conjugacy structure to facilitate efficient computation  which may add unnecessary restrictions to the viable prior distribution family and impose further constraints on the variational approximation family. In this work  we develop a general computational framework for implementing MF VI via Wasserstein gradient flow  WGF   a gradient flow over the space of probability measures. When specialized to Bayesian latent variable models  we analyze the algorithmic convergence of an alternating minimization scheme based on a time discretized WGF for implementing the MF approximation. In particular  the proposed algorithm resembles a distributional version of EM algorithm  consisting of an E step of updating the latent variable variational distribution and an M step of conducting steepest descent over the variational distribution of parameters. Our theoretical analysis relies on optimal transport theory and subdifferential calculus in the space of probability measures. We prove the exponential convergence of the time discretized WGF for minimizing a generic objective functional given strict convexity along generalized geodesics. We also provide a new proof of the exponential contraction of the variational distribution obtained from the MF approximation by using the fixed point equation of the time discretized WGF. We apply our method and theory to two classic Bayesian latent variable models  the Gaussian mixture model and the mixture of regression model. Numerical experiments are also conducted to compliment the theoretical findings under these two models.,1
Graph Neural Network Bandits We consider the bandit optimization problem with the reward function defined over graph structured data.,1
Semantic uncertainty intervals for disentangled latent spaces Meaningful uncertainty quantification in computer vision requires reasoning about semantic information    say  the hair color of the person in a photo or the location of a car on the street. To this end  recent breakthroughs in generative modeling allow us to represent semantic information in disentangled latent spaces  but providing uncertainties on the semantic latent variables has remained challenging. In this work  we provide principled uncertainty intervals that are guaranteed to contain the true semantic factors for any underlying generative model. The method does the following      it uses quantile regression to output a heuristic uncertainty interval for each element in the latent space     calibrates these uncertainties such that they contain the true value of the latent for a new  unseen input. The endpoints of these calibrated intervals can then be propagated through the generator to produce interpretable uncertainty visualizations for each semantic factor. This technique reliably communicates semantically meaningful  principled  and instance adaptive uncertainty in inverse problems like image super resolution and image completion.,1
Generalized Identifiability Bounds for Mixture Models with Grouped Samples Recent work has shown that finite mixture models with  m  components are identifiable  while making no assumptions on the mixture components  so long as one has access to groups of samples of size   m    which are known to come from the same mixture component. In this work we generalize that result and show that  if every subset of  k  mixture components of a mixture model are linearly independent  then that mixture model is identifiable with only    m     k     samples per group. We further show that this value cannot be improved. We prove an analogous result for a stronger form of identifiability known as  determinedness  along with a corresponding lower bound. This independence assumption almost surely holds if mixture components are chosen randomly from a  k  dimensional space. We describe some implications of our results for multinomial mixture models and topic modeling.,1
The derivatives of Sinkhorn Knopp converge We show that the derivatives of the Sinkhorn Knopp algorithm  or iterative proportional fitting procedure  converge towards the derivatives of the entropic regularization of the optimal transport problem with a locally uniform linear convergence rate.,1
Estimating value at risk  LSTM vs. GARCH Estimating value at risk on time series data with possibly heteroscedastic dynamics is a highly challenging task. Typically  we face a small data problem in combination with a high degree of non linearity  causing difficulties for both classical and machine learning estimation algorithms. In this paper  we propose a novel value at risk estimator using a long short term memory  LSTM  neural network and compare its performance to benchmark GARCH estimators.,1
Volatility Based Kernels and Moving Average Means for Accurate Forecasting with Gaussian Processes A broad class of stochastic volatility models are defined by systems of stochastic differential equations. While these models have seen widespread success in domains such as finance and statistical climatology  they typically lack an ability to condition on historical data to produce a true posterior distribution. To address this fundamental limitation  we show how to re cast a class of stochastic volatility models as a hierarchical Gaussian process  GP  model with specialized covariance functions. This GP model retains the inductive biases of the stochastic volatility model while providing the posterior predictive distribution given by GP inference. Within this framework  we take inspiration from well studied domains to introduce a new class of models  Volt and Magpie  that significantly outperform baselines in stock and wind speed forecasting  and naturally extend to the multitask setting.,1
Comparing Feature Importance and Rule Extraction for Interpretability on Text Data Complex machine learning algorithms are used more and more often in critical tasks involving text data  leading to the development of interpretability methods. Among local methods  two families have emerged  those computing importance scores for each feature and those extracting simple logical rules. In this paper we show that using different methods can lead to unexpectedly different explanations  even when applied to simple models for which we would expect qualitative coincidence. To quantify this effect  we propose a new approach to compare explanations produced by different methods.,1
Look beyond labels  Incorporating functional summary information in Bayesian neural networks Bayesian deep learning offers a principled approach to train neural networks that accounts for both aleatoric and epistemic uncertainty. In variational inference  priors are often specified over the weight parameters  but they do not capture the true prior knowledge in large and complex neural network architectures. We present a simple approach to incorporate summary information about the predicted probability  such as sigmoid or softmax score  outputs in Bayesian neural networks  BNNs . The available summary information is incorporated as augmented data and modeled with a Dirichlet process  and we derive the corresponding  emph Summary Evidence Lower BOund . We show how the method can inform the model about task difficulty or class imbalance. Extensive empirical experiments show that  with negligible computational overhead  the proposed method yields a BNN with a better calibration of uncertainty.,1
Edge Augmentation on Disconnected Graphs via Eigenvalue Elevation The graph theoretical task of determining most likely inter community edges based on disconnected subgraphs  intra community connectivity is proposed. An algorithm is developed for this edge augmentation task  based on elevating the zero eigenvalues of graph s spectrum. Upper bounds for eigenvalue elevation amplitude and for the corresponding augmented edge density are derived and are authenticated with simulation on random graphs. The algorithm works consistently across synthetic and real networks  yielding desirable performance at connecting graph components. Edge augmentation reverse engineers graph partition under different community detection methods  Girvan Newman method  greedy modularity maximization  label propagation  Louvain method  and fluid community   in most cases producing inter community edges at      frequency.,1
pGMM Kernel Regression and Comparisons with Boosted Trees In this work  we demonstrate the advantage of the pGMM    powered generalized min max    kernel in the context of  ridge  regression. In recent prior studies  the pGMM kernel has been extensively evaluated for classification tasks  for logistic regression  support vector machines  as well as deep neural networks. In this paper  we provide an experimental study on ridge regression  to compare the pGMM kernel regression with the ordinary ridge linear regression as well as the RBF kernel ridge regression. Perhaps surprisingly  even without a tuning parameter  i.e.   p    for the power parameter of the pGMM kernel   the pGMM kernel already performs well. Furthermore  by tuning the parameter  p   this  deceptively simple  pGMM kernel even performs quite comparably to boosted trees.,1
Contextual Bandits with Smooth Regret  Efficient Learning in Continuous Action Spaces Designing efficient general purpose contextual bandit algorithms that work with large    or even continuous    action spaces would facilitate application to important scenarios such as information retrieval  recommendation systems  and continuous control. While obtaining standard regret guarantees can be hopeless  alternative regret notions have been proposed to tackle the large action setting. We propose a smooth regret notion for contextual bandits  which dominates previously proposed alternatives. We design a statistically and computationally efficient algorithm    for the proposed smooth regret    that works with general function approximation under standard supervised oracles. We also present an adaptive algorithm that automatically adapts to any smoothness level. Our algorithms can be used to recover the previous minimax Pareto optimal guarantees under the standard regret  e.g.  in bandit problems with multiple best arms and Lipschitz H   lder bandits. We conduct large scale empirical evaluations demonstrating the efficacy of our proposed algorithms.,1
Local manifold learning and its link to domain based physics knowledge In many reacting flow systems  the thermo chemical state space is known or assumed to evolve close to a low dimensional manifold  LDM . Various approaches are available to obtain those manifolds and subsequently express the original high dimensional space with fewer parameterizing variables. Principal component analysis  PCA  is one of the dimensionality reduction methods that can be used to obtain LDMs. PCA does not make prior assumptions about the parameterizing variables and retrieves them empirically from the training data. In this paper  we show that PCA applied in local clusters of data  local PCA  is capable of detecting the intrinsic parameterization of the thermo chemical state space. We first demonstrate that utilizing three common combustion models of varying complexity  the Burke Schumann model  the chemical equilibrium model and the homogeneous reactor. Parameterization of these models is known a priori which allows for benchmarking with the local PCA approach. We further extend the application of local PCA to a more challenging case of a turbulent non premixed  n  heptane air jet flame for which the parameterization is no longer obvious. Our results suggest that meaningful parameterization can be obtained also for more complex datasets. We show that local PCA finds variables that can be linked to local stoichiometry  reaction progress and soot formation processes.,1
On the instrumental variable estimation with many weak and invalid instruments We discuss the fundamental issue of identification in linear instrumental variable  IV  models with unknown IV validity. We revisit the popular majority and plurality rules and show that no identification condition can be  if and only if  in general. With the assumption of the  sparsest rule   which is equivalent to the plurality rule but becomes operational in computation algorithms  we investigate and prove the advantages of non convex penalized approaches over other IV estimators based on two step selections  in terms of selection consistency and accommodation for individually weak IVs. Furthermore  we propose a surrogate sparsest penalty that aligns with the identification condition and provides oracle sparse structure simultaneously. Desirable theoretical properties are derived for the proposed estimator with weaker IV strength conditions compared to the previous literature. Finite sample properties are demonstrated using simulations and the selection and estimation method is applied to an empirical study concerning the effect of trade on economic growth.,1
BR SNIS  Bias Reduced Self Normalized Importance Sampling Importance Sampling  IS  is a method for approximating expectations under a target distribution using independent samples from a proposal distribution and the associated importance weights. In many applications  the target distribution is known only up to a normalization constant  in which case self normalized IS  SNIS  can be used. While the use of self normalization can have a positive effect on the dispersion of the estimator  it introduces bias. In this work  we propose a new method  BR SNIS  whose complexity is essentially the same as that of SNIS and which significantly reduces bias without increasing the variance. This method is a wrapper in the sense that it uses the same proposal samples and importance weights as SNIS  but makes clever use of iterated sampling  importance resampling  ISIR  to form a bias reduced version of the estimator. We furnish the proposed algorithm with rigorous theoretical results  including new bias  variance and high probability bounds  and these are illustrated by numerical examples.,1
Uncertainty Aware Learning Against Label Noise on Imbalanced Datasets Learning against label noise is a vital topic to guarantee a reliable performance for deep neural networks. Recent research usually refers to dynamic noise modeling with model output probabilities and loss values  and then separates clean and noisy samples. These methods have gained notable success. However  unlike cherry picked data  existing approaches often cannot perform well when facing imbalanced datasets  a common scenario in the real world. We thoroughly investigate this phenomenon and point out two major issues that hinder the performance  i.e.   emph inter class loss distribution discrepancy  and  emph misleading predictions due to uncertainty . The first issue is that existing methods often perform class agnostic noise modeling. However  loss distributions show a significant discrepancy among classes under class imbalance  and class agnostic noise modeling can easily get confused with noisy samples and samples in minority classes. The second issue refers to that models may output misleading predictions due to epistemic uncertainty and aleatoric uncertainty  thus existing methods that rely solely on the output probabilities may fail to distinguish confident samples. Inspired by our observations  we propose an Uncertainty aware Label Correction framework  ULC  to handle label noise on imbalanced datasets. First  we perform epistemic uncertainty aware class specific noise modeling to identify trustworthy clean samples and refine discard highly confident true corrupted labels. Then  we introduce aleatoric uncertainty in the subsequent learning process to prevent noise accumulation in the label noise modeling process. We conduct experiments on several synthetic and real world datasets. The results demonstrate the effectiveness of the proposed method  especially on imbalanced datasets.,1
Plex  Towards Reliability using Pretrained Large Model Extensions A recent trend in artificial intelligence is the use of pretrained models for language and vision tasks  which have achieved extraordinary performance but also puzzling failures. Probing these models  abilities in diverse ways is therefore critical to the field. In this paper  we explore the reliability of models  where we define a reliable model as one that not only achieves strong predictive performance but also performs well consistently over many decision making tasks involving uncertainty  e.g.  selective prediction  open set recognition   robust generalization  e.g.  accuracy and proper scoring rules such as log likelihood on in  and out of distribution datasets   and adaptation  e.g.  active learning  few shot uncertainty . We devise    types of tasks over    datasets in order to evaluate different aspects of reliability on both vision and language domains. To improve reliability  we developed ViT Plex and T  Plex  pretrained large model extensions for vision and language modalities  respectively. Plex greatly improves the state of the art across reliability tasks  and simplifies the traditional protocol as it improves the out of the box performance and does not require designing scores or tuning the model for each task. We demonstrate scaling effects over model sizes up to  B parameters and pretraining dataset sizes up to  B examples. We also demonstrate Plex s capabilities on challenging tasks including zero shot open set recognition  active learning  and uncertainty in conversational language understanding.,1
Off the grid learning of sparse mixtures from a continuous dictionary We consider a general non linear model where the signal is a finite mixture of an unknown  possibly increasing  number of features issued from a continuous dictionary parameterized by a real nonlinear parameter. The signal is observed with Gaussian  possibly correlated  noise in either a continuous or a discrete setup. We propose an off the grid optimization method  that is  a method which does not use any discretization scheme on the parameter space  to estimate both the non linear parameters of the features and the linear parameters of the mixture. We use recent results on the geometry of off the grid methods to give minimal separation on the true underlying non linear parameters such that interpolating certificate functions can be constructed. Using also tail bounds for suprema of Gaussian processes we bound the prediction error with high probability. Assuming that the certificate functions can be constructed  our prediction error bound is up to log   factors similar to the rates attained by the Lasso predictor in the linear regression model. We also establish convergence rates that quantify with high probability the quality of estimation for both the linear and the non linear parameters.,1
Rewiring Networks for Graph Neural Network Training Using Discrete Geometry Information over squashing is a phenomenon of inefficient information propagation between distant nodes on networks. It is an important problem that is known to significantly impact the training of graph neural networks  GNNs   as the receptive field of a node grows exponentially. To mitigate this problem  a preprocessing procedure known as rewiring is often applied to the input network. In this paper  we investigate the use of discrete analogues of classical geometric notions of curvature to model information flow on networks and rewire them. We show that these classical notions achieve state of the art performance in GNN training accuracy on a variety of real world network datasets. Moreover  compared to the current state of the art  these classical notions exhibit a clear advantage in computational runtime by several orders of magnitude.,1
Data Driven Stochastic AC OPF using Gaussian Processes In recent years  electricity generation has been responsible for more than a quarter of the greenhouse gas emissions in the US. Integrating a significant amount of renewables into a power grid is probably the most accessible way to reduce carbon emissions from power grids and slow down climate change. Unfortunately  the most accessible renewable power sources  such as wind and solar  are highly fluctuating and thus bring a lot of uncertainty to power grid operations and challenge existing optimization and control policies. The chance constrained alternating current  AC  optimal power flow  OPF  framework finds the minimum cost generation dispatch maintaining the power grid operations within security limits with a prescribed probability. Unfortunately  the AC OPF problem s chance constrained extension is non convex  computationally challenging  and requires knowledge of system parameters and additional assumptions on the behavior of renewable distribution. Known linear and convex approximations to the above problems  though tractable  are too conservative for operational practice and do not consider uncertainty in system parameters. This paper presents an alternative data driven approach based on Gaussian process  GP  regression to close this gap. The GP approach learns a simple yet non convex data driven approximation to the AC power flow equations that can incorporate uncertainty inputs. The latter is then used to determine the solution of CC OPF efficiently  by accounting for both input and parameter uncertainty. The practical efficiency of the proposed approach using different approximations for GP uncertainty propagation is illustrated over numerous IEEE test cases.,1
Unsupervised learning of observation functions in state space models by nonparametric moment methods We investigate the unsupervised learning of non invertible observation functions in nonlinear state space models. Assuming abundant data of the observation process along with the distribution of the state process  we introduce a nonparametric generalized moment method to estimate the observation function via constrained regression. The major challenge comes from the non invertibility of the observation function and the lack of data pairs between the state and observation. We address the fundamental issue of identifiability from quadratic loss functionals and show that the function space of identifiability is the closure of a RKHS that is intrinsic to the state process. Numerical results show that the first two moments and temporal correlations  along with upper and lower bounds  can identify functions ranging from piecewise polynomials to smooth functions  leading to convergent estimators. The limitations of this method  such as non identifiability due to symmetry and stationarity  are also discussed.,1
ManiFeSt  Manifold based Feature Selection for Small Data Sets In this paper  we present a new method for few sample supervised feature selection  FS . Our method first learns the manifold of the feature space of each class using kernels capturing multi feature associations. Then  based on Riemannian geometry  a composite kernel is computed  extracting the differences between the learned feature associations. Finally  a FS score based on spectral analysis is proposed. Considering multi feature associations makes our method multivariate by design. This in turn allows for the extraction of the hidden manifold underlying the features and avoids overfitting  facilitating few sample FS. We showcase the efficacy of our method on illustrative examples and several benchmarks  where our method demonstrates higher accuracy in selecting the informative features compared to competing methods. In addition  we show that our FS leads to improved classification and better generalization when applied to test data.,1
TCT  Convexifying Federated Learning using Bootstrapped Neural Tangent Kernels State of the art federated learning methods can perform far worse than their centralized counterparts when clients have dissimilar data distributions. For neural networks  even when centralized SGD easily finds a solution that is simultaneously performant for all clients  current federated optimization methods fail to converge to a comparable solution. We show that this performance disparity can largely be attributed to optimization challenges presented by nonconvexity. Specifically  we find that the early layers of the network do learn useful features  but the final layers fail to make use of them. That is  federated optimization applied to this non convex problem distorts the learning of the final layers. Leveraging this observation  we propose a Train Convexify Train  TCT  procedure to sidestep this issue  first  learn features using off the shelf methods  e.g.  FedAvg   then  optimize a convexified problem obtained from the network s empirical neural tangent kernel approximation. Our technique yields accuracy improvements of up to      on FMNIST and      on CIFAR   when clients have dissimilar data.,1
Holistic Robust Data Driven Decisions The design of data driven formulations for machine learning and decision making with good out of sample performance is a key challenge. The observation that good in sample performance does not guarantee good out of sample performance is generally known as overfitting. Practical overfitting can typically not be attributed to a single cause but instead is caused by several factors all at once. We consider here three overfitting sources   i  statistical error as a result of working with finite sample data   ii  data noise which occurs when the data points are measured only with finite precision  and finally  iii  data misspecification in which a small fraction of all data may be wholly corrupted. We argue that although existing data driven formulations may be robust against one of these three sources in isolation they do not provide holistic protection against all overfitting sources simultaneously. We design a novel data driven formulation which does guarantee such holistic protection and is furthermore computationally viable. Our distributionally robust optimization formulation can be interpreted as a novel combination of a Kullback Leibler and Levy Prokhorov robust optimization formulation. Finally  we show how in the context of classification and regression problems several popular regularized and robust formulations reduce to a particular case of our proposed more general formulation.,1
When Does Differentially Private Learning Not Suffer in High Dimensions  Large pretrained models can be privately fine tuned to achieve performance approaching that of non private models. A common theme in these results is the surprising observation that high dimensional models can achieve favorable privacy utility trade offs. This seemingly contradicts known results on the model size dependence of differentially private convex learning and raises the following research question  When does the performance of differentially private learning not degrade with increasing model size  We identify that the magnitudes of gradients projected onto subspaces is a key factor that determines performance. To precisely characterize this for private convex learning  we introduce a condition on the objective that we term restricted Lipschitz continuity and derive improved bounds for the excess empirical and population risks that are dimension independent under additional conditions. We empirically show that in private fine tuning of large language models  gradients evaluated near a local optimum are mostly controlled by a few principal components. This behavior is similar to conditions under which we obtain dimension independent bounds in convex settings. Our theoretical and empirical results together provide a possible explanation for recent successes in large scale private fine tuning.,1
POET  Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging Fine tuning models on edge devices like mobile phones would enable privacy preserving personalization over sensitive data. However  edge training has historically been limited to relatively small models with simple architectures because training is both memory and energy intensive. We present POET  an algorithm to enable training large neural networks on memory scarce battery operated edge devices. POET jointly optimizes the integrated search search spaces of rematerialization and paging  two algorithms to reduce the memory consumption of backpropagation. Given a memory budget and a run time constraint  we formulate a mixed integer linear program  MILP  for energy optimal training. Our approach enables training significantly larger models on embedded devices while reducing energy consumption while not modifying mathematical correctness of backpropagation. We demonstrate that it is possible to fine tune both ResNet    and BERT within the memory constraints of a Cortex M class embedded device while outperforming current edge training methods in energy efficiency. POET is an open source project available at,1
Tree ensemble kernels for Bayesian optimization with known constraints over mixed feature spaces Tree ensembles can be well suited for black box optimization tasks such as algorithm tuning and neural architecture search  as they achieve good predictive performance with little to no manual tuning  naturally handle discrete feature spaces  and are relatively insensitive to outliers in the training data. Two well known challenges in using tree ensembles for black box optimization are  i  effectively quantifying model uncertainty for exploration and  ii  optimizing over the piece wise constant acquisition function. To address both points simultaneously  we propose using the kernel interpretation of tree ensembles as a Gaussian Process prior to obtain model variance estimates  and we develop a compatible optimization formulation for the acquisition function. The latter further allows us to seamlessly integrate known constraints to improve sampling efficiency by considering domain knowledge in engineering settings and modeling search space symmetries  e.g.  hierarchical relationships in neural architecture search. Our framework performs as well as state of the art methods for unconstrained black box optimization over continuous discrete features and outperforms competing methods for problems combining mixed variable feature spaces and known input constraints.,1
Differentially Private Estimation via Statistical Depth Constructing a differentially private  DP  estimator requires deriving the maximum influence of an observation  which can be difficult in the absence of exogenous bounds on the input data or the estimator  especially in high dimensional settings. This paper shows that standard notions of statistical depth  i.e.  halfspace depth and regression depth  are particularly advantageous in this regard  both in the sense that the maximum influence of a single observation is easy to analyze and that this value is typically low. This is used to motivate new approximate DP location and regression estimators using the maximizers of these two notions of statistical depth. A more computationally efficient variant of the approximate DP regression estimator is also provided. Also  to avoid requiring that users specify a priori bounds on the estimates and or the observations  variants of these DP mechanisms are described that satisfy random differential privacy  RDP   which is a relaxation of differential privacy provided by Hall  Wasserman  and Rinaldo       . We also provide simulations of the two DP regression methods proposed here. The proposed estimators appear to perform favorably relative to the existing DP regression methods we consider in these simulations when either the sample size is at least         or the privacy loss budget is sufficiently high.,1
Statistical Inference with Stochastic Gradient Algorithms Stochastic gradient algorithms are widely used for both optimization and sampling in large scale learning and inference problems. However  in practice  tuning these algorithms is typically done using heuristics and trial and error rather than rigorous  generalizable theory. To address this gap between theory and practice  we novel insights into the effect of tuning parameters by characterizing the large sample behavior of iterates of a very general class of preconditioned stochastic gradient algorithms with fixed step size. In the optimization setting  our results show that iterate averaging with a large fixed step size can result in statistically efficient approximation of the  local  M estimator. In the sampling context  our results show that with appropriate choices of tuning parameters  the limiting stationary covariance can match either the Bernstein  von Mises limit of the posterior  adjustments to the posterior for model misspecification  or the asymptotic distribution of the MLE  and that with a naive tuning the limit corresponds to none of these. Moreover  we argue that an essentially independent sample from the stationary distribution can be obtained after a fixed number of passes over the dataset. We validate our asymptotic results in realistic finite sample regimes via several experiments using simulated and real data. Overall  we demonstrate that properly tuned stochastic gradient algorithms with constant step size offer a computationally efficient and statistically robust approach to obtaining point estimates or posterior like samples.,1
Goal Conditioned Generators of Deep Policies Goal conditioned Reinforcement Learning  RL  aims at learning optimal policies  given goals encoded in special command inputs. Here we study goal conditioned neural nets  NNs  that learn to generate deep NN policies in form of context specific weight matrices  similar to Fast Weight Programmers and other methods from the     s. Using context commands of the form  generate a policy that achieves a desired expected return   our NN generators combine powerful exploration of parameter space with generalization across commands to iteratively find better and better policies. A form of weight sharing HyperNetworks and policy embeddings scales our method to generate deep NNs. Experiments show how a single learned policy generator can produce policies that achieve any return seen during training. Finally  we evaluate our algorithm on a set of continuous control tasks where it exhibits competitive performance. Our code is public.,1
Statistical Inference with Stochastic Gradient Algorithms Stochastic gradient algorithms are widely used for both optimization and sampling in large scale learning and inference problems. However  in practice  tuning these algorithms is typically done using heuristics and trial and error rather than rigorous  generalizable theory. To address this gap between theory and practice  we novel insights into the effect of tuning parameters by characterizing the large sample behavior of iterates of a very general class of preconditioned stochastic gradient algorithms with fixed step size. In the optimization setting  our results show that iterate averaging with a large fixed step size can result in statistically efficient approximation of the  local  M estimator. In the sampling context  our results show that with appropriate choices of tuning parameters  the limiting stationary covariance can match either the Bernstein  von Mises limit of the posterior  adjustments to the posterior for model misspecification  or the asymptotic distribution of the MLE  and that with a naive tuning the limit corresponds to none of these. Moreover  we argue that an essentially independent sample from the stationary distribution can be obtained after a fixed number of passes over the dataset. We validate our asymptotic results in realistic finite sample regimes via several experiments using simulated and real data. Overall  we demonstrate that properly tuned stochastic gradient algorithms with constant step size offer a computationally efficient and statistically robust approach to obtaining point estimates or posterior like samples.,1
An Asymmetric Contrastive Loss for Handling Imbalanced Datasets Contrastive learning is a representation learning method performed by contrasting a sample to other similar samples so that they are brought closely together  forming clusters in the feature space. The learning process is typically conducted using a two stage training architecture  and it utilizes the contrastive loss  CL  for its feature learning. Contrastive learning has been shown to be quite successful in handling imbalanced datasets  in which some classes are overrepresented while some others are underrepresented. However  previous studies have not specifically modified CL for imbalanced datasets. In this work  we introduce an asymmetric version of CL  referred to as ACL  in order to directly address the problem of class imbalance. In addition  we propose the asymmetric focal contrastive loss  AFCL  as a further generalization of both ACL and focal contrastive loss  FCL . Results on the FMNIST and ISIC      imbalanced datasets show that AFCL is capable of outperforming CL and FCL in terms of both weighted and unweighted classification accuracies. In the appendix  we provide a full axiomatic treatment on entropy  along with complete proofs.,1
Size and depth of monotone neural networks  interpolation and approximation Monotone functions and data sets arise in a variety of applications. We study the interpolation problem for monotone data sets  The input is a monotone data set with  n  points  and the goal is to find a size and depth efficient monotone neural network  with non negative parameters and threshold units  that interpolates the data set. We show that there are monotone data sets that cannot be interpolated by a monotone network of depth    . On the other hand  we prove that for every monotone data set with  n  points in   mathbb R  d   there exists an interpolating monotone network of depth     and size  O nd  . Our interpolation result implies that every monotone function over        d  can be approximated arbitrarily well by a depth   monotone network  improving the previous best known construction of depth  d   . Finally  building on results from Boolean circuit complexity  we show that the inductive bias of having positive parameters can lead to a super polynomial blow up in the number of neurons when approximating monotone functions.,1
Hindsight Learning for MDPs with Exogenous Inputs We develop a reinforcement learning  RL  framework for applications that deal with sequential decisions and exogenous uncertainty  such as resource allocation and inventory management. In these applications  the uncertainty is only due to exogenous variables like future demands. A popular approach is to predict the exogenous variables using historical data and then plan with the predictions. However  this indirect approach requires high fidelity modeling of the exogenous process to guarantee good downstream decision making  which can be impractical when the exogenous process is complex. In this work we propose an alternative approach based on hindsight learning which sidesteps modeling the exogenous process. Our key insight is that  unlike Sim Real RL  we can revisit past decisions in the historical data and derive counterfactual consequences for other actions in these applications. Our framework uses hindsight optimal actions as the policy training signal and has strong theoretical guarantees on decision making performance. We develop an algorithm using our framework to allocate compute resources for real world Microsoft Azure workloads. The results show our approach learns better policies than domain specific heuristics and Sim Real RL baselines.,1
Estimation of Non Crossing Quantile Regression Process with Deep ReQU Neural Networks We propose a penalized nonparametric approach to estimating the quantile regression process  QRP  in a nonseparable model using rectifier quadratic unit  ReQU  activated deep neural networks and introduce a novel penalty function to enforce non crossing of quantile regression curves. We establish the non asymptotic excess risk bounds for the estimated QRP and derive the mean integrated squared error for the estimated QRP under mild smoothness and regularity conditions. To establish these non asymptotic risk and estimation error bounds  we also develop a new error bound for approximating  C s  smooth functions with  s     and their derivatives using ReQU activated neural networks. This is a new approximation result for ReQU networks and is of independent interest and may be useful in other problems. Our numerical experiments demonstrate that the proposed method is competitive with or outperforms two existing methods  including methods using reproducing kernels and random forests  for nonparametric quantile regression.,1
Joint Application of the Target Trial Causal Framework and Machine Learning Modeling to Optimize Antibiotic Therapy  Use Case on Acute Bacterial Skin and Skin Structure Infections due to Methicillin resistant Staphylococcus aureus Bacterial infections are responsible for high mortality worldwide. Antimicrobial resistance underlying the infection  and multifaceted patient s clinical status can hamper the correct choice of antibiotic treatment. Randomized clinical trials provide average treatment effect estimates but are not ideal for risk stratification and optimization of therapeutic choice  i.e.  individualized treatment effects  ITE . Here  we leverage large scale electronic health record data  collected from Southern US academic clinics  to emulate a clinical trial  i.e.   target trial   and develop a machine learning model of mortality prediction and ITE estimation for patients diagnosed with acute bacterial skin and skin structure infection  ABSSSI  due to methicillin resistant Staphylococcus aureus  MRSA . ABSSSI MRSA is a challenging condition with reduced treatment options   vancomycin is the preferred choice  but it has non negligible side effects. First  we use propensity score matching to emulate the trial and create a treatment randomized  vancomycin vs. other antibiotics  dataset. Next  we use this data to train various machine learning methods  including boosted LASSO logistic regression  support vector machines  and random forest  and choose the best model in terms of area under the receiver characteristic  AUC  through bootstrap validation. Lastly  we use the models to calculate ITE and identify possible averted deaths by therapy change. The out of bag tests indicate that SVM and RF are the most accurate  with AUC of     and      respectively  but BLR LASSO is not far behind      . By calculating the counterfactuals using the BLR LASSO  vancomycin increases the risk of death  but it shows a large variation  odds ratio  .       range  .   .   and the contribution to outcome probability is modest. Instead  the RF exhibits stronger changes in ITE  suggesting more complex treatment heterogeneity.,1
 Nearly  Optimal Private Linear Regression via Adaptive Clipping We study the problem of differentially private linear regression where each data point is sampled from a fixed sub Gaussian style distribution. We propose and analyze a one pass mini batch stochastic gradient descent method  DP AMBSSGD  where points in each iteration are sampled without replacement. Noise is added for DP but the noise standard deviation is estimated online. Compared to existing    epsilon   delta   DP techniques which have sub optimal error bounds  DP AMBSSGD is able to provide nearly optimal error bounds in terms of key parameters like dimensionality  d   number of points  N   and the standard deviation   sigma  of the noise in observations. For example  when the  d  dimensional covariates are sampled i.i.d. from the normal distribution  then the excess error of DP AMBSSGD due to privacy is   frac  sigma   d  N     frac d   epsilon   N     i.e.  the error is meaningful when number of samples  N   Omega d  log d   which is the standard operative regime for linear regression. In contrast  error bounds for existing efficient methods in this setting are    mathcal O  big  frac d     epsilon   N    big    even for   sigma   . That is  for constant   epsilon   the existing techniques require  N  Omega d sqrt d    to provide a non trivial result.,1
Learning to Increase the Power of Conditional Randomization Tests The model X conditional randomization test is a generic framework for conditional independence testing  unlocking new possibilities to discover features that are conditionally associated with a response of interest while controlling type I error rates. An appealing advantage of this test is that it can work with any machine learning model to design powerful test statistics. In turn  the common practice in the model X literature is to form a test statistic using machine learning models  trained to maximize predictive accuracy with the hope to attain a test with good power. However  the ideal goal here is to drive the model  during training  to maximize the power of the test  not merely the predictive accuracy. In this paper  we bridge this gap by introducing  for the first time  novel model fitting schemes that are designed to explicitly improve the power of model X tests. This is done by introducing a new cost function that aims at maximizing the test statistic used to measure violations of conditional independence. Using synthetic and real data sets  we demonstrate that the combination of our proposed loss function with various base predictive models  lasso  elastic net  and deep neural networks  consistently increases the number of correct discoveries obtained  while maintaining type I error rates under control.,1
Deep Sufficient Representation Learning via Mutual Information We propose a mutual information based sufficient representation learning  MSRL  approach  which uses the variational formulation of the mutual information and leverages the approximation power of deep neural networks. MSRL learns a sufficient representation with the maximum mutual information with the response and a user selected distribution. It can easily handle multi dimensional continuous or categorical response variables. MSRL is shown to be consistent in the sense that the conditional probability density function of the response variable given the learned representation converges to the conditional probability density function of the response variable given the predictor. Non asymptotic error bounds for MSRL are also established under suitable conditions. To establish the error bounds  we derive a generalized Dudley s inequality for an order two U process indexed by deep neural networks  which may be of independent interest. We discuss how to determine the intrinsic dimension of the underlying data distribution. Moreover  we evaluate the performance of MSRL via extensive numerical experiments and real data analysis and demonstrate that MSRL outperforms some existing nonlinear sufficient dimension reduction methods.,1
How do tuna schools associate to dFADs  A study using echo sounder buoys to identify global patterns Based on the data gathered by echo sounder buoys attached to drifting Fish Aggregating Devices  dFADs  across tropical oceans  the current study applies a Machine Learning protocol to examine the temporal trends of tuna schools  association to drifting objects. Using a binary output  metrics typically used in the literature were adapted to account for the fact that the entire tuna aggregation under the dFAD was considered. The median time it took tuna to colonize the dFADs for the first time varied between    and    days  depending on the ocean  and the longest soak and colonization times were registered in the Pacific Ocean. The tuna schools  Continuous Residence Times were generally shorter than Continuous Absence Times  median values between   and   days  and   and    days  respectively   in line with the results found by previous studies. Using a regression output  two novel metrics  namely aggregation time and disaggregation time  were estimated to obtain further insight into the symmetry of the aggregation process. Across all oceans  the time it took for the tuna aggregation to depart from the dFADs was not significantly longer than the time it took for the aggregation to form. The value of these results in the context of the  ecological trap  hypothesis is discussed  and further analyses to enrich and make use of this data source are proposed.,1
Black and Gray Box Learning of Amplitude Equations  Application to Phase Field Systems We present a data driven approach to learning surrogate models for amplitude equations  and illustrate its application to interfacial dynamics of phase field systems. In particular  we demonstrate learning effective partial differential equations describing the evolution of phase field interfaces from full phase field data. We illustrate this on a model phase field system  where analytical approximate equations for the dynamics of the phase field interface  a higher order eikonal equation and its approximation  the Kardar Parisi Zhang  KPZ  equation  are known. For this system  we discuss data driven approaches for the identification of equations that accurately describe the front interface dynamics. When the analytical approximate models mentioned above become inaccurate  as we move beyond the region of validity of the underlying assumptions  the data driven equations outperform them. In these regimes  going beyond black box identification  we explore different approaches to learn data driven corrections to the analytically approximate models  leading to effective gray box partial differential equations.,1
On minimax density estimation via measure transport We study the convergence properties  in Hellinger and related distances  of nonparametric density estimators based on measure transport. These estimators represent the measure of interest as the pushforward of a chosen reference distribution under a transport map  where the map is chosen via a maximum likelihood objective  equivalently  minimizing an empirical Kullback Leibler loss  or a penalized version thereof. We establish concentration inequalities for a general class of penalized measure transport estimators  by combining techniques from M estimation with analytical properties of the transport based density representation. We then demonstrate the implications of our theory for the case of triangular Knothe Rosenblatt  KR  transports on the  d  dimensional unit cube  and show that both penalized and unpenalized versions of such estimators achieve minimax optimal convergence rates over H lder classes of densities. Specifically  we establish optimal rates for unpenalized nonparametric maximum likelihood estimation over bounded H lder type balls  and then for certain Sobolev penalized estimators and sieved wavelet estimators.,1
LETS GZSL  A Latent Embedding Model for Time Series Generalized Zero Shot Learning One of the recent developments in deep learning is generalized zero shot learning  GZSL   which aims to recognize objects from both seen and unseen classes  when only the labeled examples from seen classes are provided. Over the past couple of years  GZSL has picked up traction and several models have been proposed to solve this problem. Whereas an extensive amount of research on GZSL has been carried out in fields such as computer vision and natural language processing  no such research has been carried out to deal with time series data. GZSL is used for applications such as detecting abnormalities from ECG and EEG data and identifying unseen classes from sensor  spectrograph and other devices  data. In this regard  we propose a Latent Embedding for Time Series   GZSL  LETS GZSL  model that can solve the problem of GZSL for time series classification  TSC . We utilize an embedding based approach and combine it with attribute vectors to predict the final class labels. We report our results on the widely popular UCR archive datasets. Our framework is able to achieve a harmonic mean value of at least     on most of the datasets except when the number of unseen classes is greater than   or the amount of data is very low  less than     training examples .,1
Efficient Real world Testing of Causal Decision Making via Bayesian Experimental Design for Contextual Optimisation The real world testing of decisions made using causal machine learning models is an essential prerequisite for their successful application. We focus on evaluating and improving contextual treatment assignment decisions  these are personalised treatments applied to e.g. customers  each with their own contextual information  with the aim of maximising a reward. In this paper we introduce a model agnostic framework for gathering data to evaluate and improve contextual decision making through Bayesian Experimental Design. Specifically  our method is used for the data efficient evaluation of the regret of past treatment assignments. Unlike approaches such as A B testing  our method avoids assigning treatments that are known to be highly sub optimal  whilst engaging in some exploration to gather pertinent information. We achieve this by introducing an information based design objective  which we optimise end to end. Our method applies to discrete and continuous treatments. Comparing our information theoretic approach to baselines in several simulation studies demonstrates the superior performance of our proposed approach.,1
A Sublinear Time Quantum Algorithm for Approximating Partition Functions We present a novel quantum algorithm for estimating Gibbs partition functions in sublinear time with respect to the logarithm of the size of the state space. This is the first speed up of this type to be obtained over the seminal nearly linear time algorithm of  tefankovi   Vempala and Vigoda  JACM       . Our result also preserves the quadratic speed up in precision and spectral gap achieved in previous work by exploiting the properties of quantum Markov chains. As an application  we obtain new polynomial improvements over the best known algorithms for computing the partition function of the Ising model  and counting the number of  k  colorings  matchings or independent sets of a graph.,1
Adversarial Sign Corrupted Isotonic Regression Classical univariate isotonic regression involves nonparametric estimation under a monotonicity constraint of the true signal. We consider a variation of this generating process  which we term adversarial sign corrupted isotonic   texttt ASCI   regression. Under this  texttt ASCI  setting  the adversary has full access to the true isotonic responses  and is free to sign corrupt them. Estimating the true monotonic signal given these sign corrupted responses is a highly challenging task. Notably  the sign corruptions are designed to violate monotonicity  and possibly induce heavy dependence between the corrupted response terms. In this sense   texttt ASCI  regression may be viewed as an adversarial stress test for isotonic regression. Our motivation is driven by understanding whether efficient robust estimation of the monotone signal is feasible under this adversarial setting. We develop  texttt ASCIFIT   a three step estimation procedure under the  texttt ASCI  setting. The  texttt ASCIFIT  procedure is conceptually simple  easy to implement with existing software  and consists of applying the  texttt PAVA  with crucial pre  and post processing corrections. We formalize this procedure  and demonstrate its theoretical guarantees in the form of sharp high probability upper bounds and minimax lower bounds. We illustrate our findings with detailed simulations.,1
SPRT based Efficient Best Arm Identification in Stochastic Bandits This paper investigates the best arm identification  BAI  problem in stochastic multi armed bandits in the fixed confidence setting. The general class of the exponential family of bandits is considered. The state of the art algorithms for the exponential family of bandits face computational challenges. To mitigate these challenges  a novel framework is proposed  which views the BAI problem as sequential hypothesis testing  and is amenable to tractable analysis for the exponential family of bandits. Based on this framework  a BAI algorithm is designed that leverages the canonical sequential probability ratio tests. This algorithm has three features for both settings      its sample complexity is asymptotically optimal      it is guaranteed to be   delta  PAC  and     it addresses the computational challenge of the state of the art approaches. Specifically  these approaches  which are focused only on the Gaussian setting  require Thompson sampling from the arm that is deemed the best and a challenger arm. This paper analytically shows that identifying the challenger is computationally expensive and that the proposed algorithm circumvents it. Finally  numerical experiments are provided to support the analysis.,1
Personalized PCA  Decoupling Shared and Unique Features In this paper  we tackle a significant challenge in PCA  heterogeneity. When data are collected from different sources with heterogeneous trends while still sharing some congruency  it is critical to extract shared knowledge while retaining unique features of each source. To this end  we propose personalized PCA  PerPCA   which uses mutually orthogonal global and local principal components to encode both unique and shared features. We show that  under mild conditions  both unique and shared features can be identified and recovered by a constrained optimization problem  even if the covariance matrices are immensely different. Also  we design a fully federated algorithm inspired by distributed Stiefel gradient descent to solve the problem. The algorithm introduces a new group of operations called generalized retractions to handle orthogonality constraints  and only requires global PCs to be shared across sources. We prove the linear convergence of the algorithm under suitable assumptions. Comprehensive numerical experiments highlight PerPCA s superior performance in feature extraction and prediction from heterogeneous datasets. As a systematic approach to decouple shared and unique features from heterogeneous datasets  PerPCA finds applications in several tasks including video segmentation  topic extraction  and distributed clustering.,1
The Cosmic Graph  Optimal Information Extraction from Large Scale Structure using Catalogues We present an implicit likelihood approach to quantifying cosmological information over discrete catalogue data  assembled as graphs. To do so  we explore cosmological inference using mock dark matter halo catalogues. We employ Information Maximising Neural Networks  IMNNs  to quantify Fisher information extraction as a function of graph representation. We a  demonstrate the high sensitivity of modular graph structure to the underlying cosmology in the noise free limit  b  show that networks automatically combine mass and clustering information through comparisons to traditional statistics  c  demonstrate that graph neural networks can still extract information when catalogues are subject to noisy survey cuts  and d  illustrate how nonlinear IMNN summaries can be used as asymptotically optimal compressed statistics for Bayesian implicit likelihood inference. We reduce the area of joint   Omega m   sigma    parameter constraints with small    sim     object  halo catalogues by a factor of    over the two point correlation function  and demonstrate that the networks automatically combine mass and clustering information. This work utilises a new IMNN implementation over graph data in Jax  which can take advantage of either numerical or auto differentiability. We also show that graph IMNNs successfully compress simulations far from the fiducial model at which the network is fitted  indicating a promising alternative to  n  point statistics in catalogue based analyses.,1
Fast computation of rankings from pairwise comparisons We study the ranking of individuals  teams  or objects on the basis of pairwise comparisons using the Bradley Terry model. Maximum likelihood estimates of rankings within this model are commonly made using a simple iterative algorithm first introduced by Zermelo almost a century ago. Here we describe an alternative and similarly simple iteration that solves the same problem much faster    over a hundred times faster in some cases. We demonstrate this algorithm with applications to a range of example data sets and derive some results regarding its convergence.,1
Blessing of Nonconvexity in Deep Linear Models  Depth Flattens the Optimization Landscape Around the True Solution This work characterizes the effect of depth on the optimization landscape of linear regression  showing that  despite their nonconvexity  deeper models have more desirable optimization landscape. We consider a robust and over parameterized setting  where a subset of measurements are grossly corrupted with noise and the true linear model is captured via an  N  layer linear neural network. On the negative side  we show that this problem  textit does not  have a benign landscape  given any  N geq     with constant probability  there exists a solution corresponding to the ground truth that is neither local nor global minimum. However  on the positive side  we prove that  for any  N  layer model with  N geq     a simple sub gradient method becomes oblivious to such   problematic   solutions  instead  it converges to a balanced solution that is not only close to the ground truth but also enjoys a flat local landscape  thereby eschewing the need for  early stopping . Lastly  we empirically verify that the desirable optimization landscape of deeper models extends to other robust learning tasks  including deep matrix recovery and deep ReLU networks with   ell    loss.,1
Towards understanding how momentum improves generalization in deep learning Stochastic gradient descent  SGD  with momentum is widely used for training modern deep learning architectures. While it is well understood that using momentum can lead to faster convergence rate in various settings  it has also been observed that momentum yields higher generalization. Prior work argue that momentum stabilizes the SGD noise during training and this leads to higher generalization. In this paper  we adopt another perspective and first empirically show that gradient descent with momentum  GD M  significantly improves generalization compared to gradient descent  GD  in some deep learning problems. From this observation  we formally study how momentum improves generalization. We devise a binary classification setting where a one hidden layer  over parameterized  convolutional neural network trained with GD M provably generalizes better than the same network trained with GD  when both algorithms are similarly initialized. The key insight in our analysis is that momentum is beneficial in datasets where the examples share some feature but differ in their margin. Contrary to GD that memorizes the small margin data  GD M still learns the feature in these data thanks to its historical gradients. Lastly  we empirically validate our theoretical findings.,1
Learning Counterfactually Invariant Predictors We propose a method to learn predictors that are invariant under counterfactual changes of certain covariates. This method is useful when the prediction target is causally influenced by covariates that should not affect the predictor output. For instance  an object recognition model may be influenced by position  orientation  or scale of the object itself. We address the problem of training predictors that are explicitly counterfactually invariant to changes of such covariates. We propose a model agnostic regularization term based on conditional kernel mean embeddings  to enforce counterfactual invariance during training. We prove the soundness of our method  which can handle mixed categorical and continuous multi variate attributes. Empirical results on synthetic and real world data demonstrate the efficacy of our method in a variety of settings.,1
Can Population based Engagement Improve Personalisation  A Novel Dataset and Experiments This work explores how population based engagement prediction can address cold start at scale in large learning resource collections. The paper introduces i  VLE  a novel dataset that consists of content and video based features extracted from publicly available scientific video lectures coupled with implicit and explicit signals related to learner engagement  ii  two standard tasks related to predicting and ranking context agnostic engagement in video lectures with preliminary baselines and iii  a set of experiments that validate the usefulness of the proposed dataset. Our experimental results indicate that the newly proposed VLE dataset leads to building context agnostic engagement prediction models that are significantly performant than ones based on previous datasets  mainly attributing to the increase of training examples. VLE dataset s suitability in building models towards Computer Science  Artificial Intelligence education focused on e learning  MOOC use cases is also evidenced. Further experiments in combining the built model with a personalising algorithm show promising improvements in addressing the cold start problem encountered in educational recommenders. This is the largest and most diverse publicly available dataset to our knowledge that deals with learner engagement prediction tasks. The dataset  helper tools  descriptive statistics and example code snippets are available publicly.,1
Online Active Regression Active regression considers a linear regression problem where the learner receives a large number of data points but can only observe a small number of labels. Since online algorithms can deal with incremental training data and take advantage of low computational cost  we consider an online extension of the active regression problem  the learner receives data points one by one and immediately decides whether it should collect the corresponding labels. The goal is to efficiently maintain the regression of received data points with a small budget of label queries. We propose novel algorithms for this problem under   ell p  loss where  p in      . To achieve a      epsilon   approximate solution  our proposed algorithms only require   tilde  mathcal O    epsilon      d  log n kappa    queries of labels  where  n  is the number of data points and   kappa  is a quantity  called the condition number  of the data points. The numerical results verify our theoretical results and show that our methods have comparable performance with offline active regression algorithms.,1
Generalized Identifiability Bounds for Mixture Models with Grouped Samples Recent work has shown that finite mixture models with  m  components are identifiable  while making no assumptions on the mixture components  so long as one has access to groups of samples of size   m    which are known to come from the same mixture component. In this work we generalize that result and show that  if every subset of  k  mixture components of a mixture model are linearly independent  then that mixture model is identifiable with only    m     k     samples per group. We further show that this value cannot be improved. We prove an analogous result for a stronger form of identifiability known as  determinedness  along with a corresponding lower bound. This independence assumption almost surely holds if mixture components are chosen randomly from a  k  dimensional space. We describe some implications of our results for multinomial mixture models and topic modeling.,1
Causal Fairness Analysis Decision making systems based on AI and machine learning have been used throughout a wide range of real world scenarios  including healthcare  law enforcement  education  and finance. It is no longer far fetched to envision a future where autonomous systems will be driving entire business decisions and  more broadly  supporting large scale decision making infrastructure to solve society s most challenging problems. Issues of unfairness and discrimination are pervasive when decisions are being made by humans  and remain  or are potentially amplified  when decisions are made using machines with little transparency  accountability  and fairness. In this paper  we introduce a framework for  textit causal fairness analysis  with the intent of filling in this gap  i.e.  understanding  modeling  and possibly solving issues of fairness in decision making settings. The main insight of our approach will be to link the quantification of the disparities present on the observed data with the underlying  and often unobserved  collection of causal mechanisms that generate the disparity in the first place  challenge we call the Fundamental Problem of Causal Fairness Analysis  FPCFA . In order to solve the FPCFA  we study the problem of decomposing variations and empirical measures of fairness that attribute such variations to structural mechanisms and different units of the population. Our effort culminates in the Fairness Map  which is the first systematic attempt to organize and explain the relationship between different criteria found in the literature. Finally  we study which causal assumptions are minimally needed for performing causal fairness analysis and propose a Fairness Cookbook  which allows data scientists to assess the existence of disparate impact and disparate treatment.,1
ControlBurn  Nonlinear Feature Selection with Sparse Tree Ensembles ControlBurn is a Python package to construct feature sparse tree ensembles that support nonlinear feature selection and interpretable machine learning. The algorithms in this package first build large tree ensembles that prioritize basis functions with few features and then select a feature sparse subset of these basis functions using a weighted lasso optimization criterion. The package includes visualizations to analyze the features selected by the ensemble and their impact on predictions. Hence ControlBurn offers the accuracy and flexibility of tree ensemble models and the interpretability of sparse generalized additive models.,1
Variance estimation in graphs with the fused lasso We study the problem of variance estimation in general graph structured problems. First  we develop a linear time estimator for the homoscedastic case that can consistently estimate the variance in general graphs. We show that our estimator attains minimax rates for the chain and  D grid graphs when the mean signal has a total variation with canonical scaling. Furthermore  we provide general upper bounds on the mean squared error performance of the fused lasso estimator in general graphs under a moment condition and a bound on the tail behavior of the errors. These upper bounds allow us to generalize for broader classes of distributions  such as sub Exponential  many existing results on the fused lasso that are only known to hold with the assumption that errors are sub Gaussian random variables. Exploiting our upper bounds  we then study a simple total variation regularization estimator for estimating the signal of variances in the heteroscedastic case. Our results show that the variance estimator attains minimax rates for estimating signals of bounded variation in grid graphs   K  nearest neighbor graphs with very mild assumptions  and it is consistent for estimating the variances in any connected graph. In addition  extensive numerical results show that our proposed estimators perform reasonably well in a variety of graph structured models.,1
Contextual Decision Trees Focusing on Random Forests  we propose a multi armed contextual bandit recommendation framework for feature based selection of a single shallow tree of the learned ensemble. The trained system  which works on top of the Random Forest  dynamically identifies a base predictor that is responsible for providing the final output. In this way  we obtain local interpretations by observing the rules of the recommended tree. The carried out experiments reveal that our dynamic method is superior to an independent fitted CART decision tree and comparable to the whole black box Random Forest in terms of predictive performances.,1
Breaking Feedback Loops in Recommender Systems with Causal Inference Recommender systems play a key role in shaping modern web ecosystems. These systems alternate between     making recommendations     collecting user responses to these recommendations  and     retraining the recommendation algorithm based on this feedback. During this process the recommender system influences the user behavioral data that is subsequently used to update it  thus creating a feedback loop. Recent work has shown that feedback loops may compromise recommendation quality and homogenize user behavior  raising ethical and performance concerns when deploying recommender systems. To address these issues  we propose the Causal Adjustment for Feedback Loops  CAFL   an algorithm that provably breaks feedback loops using causal inference and can be applied to any recommendation algorithm that optimizes a training loss. Our main observation is that a recommender system does not suffer from feedback loops if it reasons about causal quantities  namely the intervention distributions of recommendations on user ratings. Moreover  we can calculate this intervention distribution from observational data by adjusting for the recommender system s predictions of user preferences. Using simulated environments  we demonstrate that CAFL improves recommendation quality when compared to prior correction methods.,1
A Newton CG based barrier method for finding a second order stationary point of nonconvex conic optimization with complexity guarantees In this paper we consider finding an approximate second order stationary point  SOSP  of nonconvex conic optimization that minimizes a twice differentiable function over the intersection of an affine subspace and a convex cone. In particular  we propose a Newton conjugate gradient  Newton CG  based barrier method for finding an    epsilon  sqrt  epsilon    SOSP of this problem. Our method is not only implementable  but also achieves an iteration complexity of    cal O   epsilon           which matches the best known iteration complexity of second order methods for finding an    epsilon  sqrt  epsilon    SOSP of unconstrained nonconvex optimization. The operation complexity of   widetilde  cal O   epsilon        min  n  epsilon             measured by the amount of fundamental operations  is also established for our method.,1
Neural Posterior Estimation with Differentiable Simulators Simulation Based Inference  SBI  is a promising Bayesian inference framework that alleviates the need for analytic likelihoods to estimate posterior distributions. Recent advances using neural density estimators in SBI algorithms have demonstrated the ability to achieve high fidelity posteriors  at the expense of a large number of simulations   which makes their application potentially very time consuming when using complex physical simulations. In this work we focus on boosting the sample efficiency of posterior density estimation using the gradients of the simulator. We present a new method to perform Neural Posterior Estimation  NPE  with a differentiable simulator. We demonstrate how gradient information helps constrain the shape of the posterior and improves sample efficiency.,1
AGBoost  Attention based Modification of Gradient Boosting Machine A new attention based model for the gradient boosting machine  GBM  called AGBoost  the attention based gradient boosting  is proposed for solving regression problems. The main idea behind the proposed AGBoost model is to assign attention weights with trainable parameters to iterations of GBM under condition that decision trees are base learners in GBM. Attention weights are determined by applying properties of decision trees and by using the Huber s contamination model which provides an interesting linear dependence between trainable parameters of the attention and the attention weights. This peculiarity allows us to train the attention weights by solving the standard quadratic optimization problem with linear constraints. The attention weights also depend on the discount factor as a tuning parameter  which determines how much the impact of the weight is decreased with the number of iterations. Numerical experiments performed for two types of base learners  original decision trees and extremely randomized trees with various regression datasets illustrate the proposed model.,1
Jackknife Variability Estimation For Randomized Matrix Computations Randomized algorithms based on sketching have become a workhorse tool in low rank matrix approximation. To use these algorithms safely in applications  they should be coupled with diagnostics to assess the quality of approximation. To meet this need  this paper proposes a jackknife resampling method to estimate the variability of the output of a randomized matrix computation. The variability estimate can recognize that a computation requires additional data or that the computation is intrinsically unstable. As examples  the paper studies jackknife estimates for two randomized low rank matrix approximation algorithms. In each case  the operation count for the jackknife estimate is independent of the dimensions of the target matrix. In numerical experiments  the estimator accurately assesses variability and also provides an order of magnitude estimate of the mean square error.,1
The role of the geometric mean in case control studies Historically used in settings where the outcome is rare or data collection is expensive  outcome dependent sampling is relevant to many modern settings where data is readily available for a biased sample of the target population  such as public administrative data. Under outcome dependent sampling  common effect measures such as the average risk difference and the average risk ratio are not identified  but the conditional odds ratio is. Aggregation of the conditional odds ratio is challenging since summary measures are generally not identified. Furthermore  the marginal odds ratio can be larger  or smaller  than all conditional odds ratios. This so called non collapsibility of the odds ratio is avoidable if we use an alternative aggregation to the standard arithmetic mean. We provide a new definition of collapsibility that makes this choice of aggregation method explicit  and we demonstrate that the odds ratio is collapsible under geometric aggregation. We describe how to partially identify  estimate  and do inference on the geometric odds ratio under outcome dependent sampling. Our proposed estimator is based on the efficient influence function and therefore has doubly robust style properties.,1
Variance estimation in graphs with the fused lasso We study the problem of variance estimation in general graph structured problems. First  we develop a linear time estimator for the homoscedastic case that can consistently estimate the variance in general graphs. We show that our estimator attains minimax rates for the chain and  D grid graphs when the mean signal has a total variation with canonical scaling. Furthermore  we provide general upper bounds on the mean squared error performance of the fused lasso estimator in general graphs under a moment condition and a bound on the tail behavior of the errors. These upper bounds allow us to generalize for broader classes of distributions  such as sub Exponential  many existing results on the fused lasso that are only known to hold with the assumption that errors are sub Gaussian random variables. Exploiting our upper bounds  we then study a simple total variation regularization estimator for estimating the signal of variances in the heteroscedastic case. Our results show that the variance estimator attains minimax rates for estimating signals of bounded variation in grid graphs   K  nearest neighbor graphs with very mild assumptions  and it is consistent for estimating the variances in any connected graph. In addition  extensive numerical results show that our proposed estimators perform reasonably well in a variety of graph structured models.,1
Better Methods and Theory for Federated Learning  Compression  Client Selection and Heterogeneity Federated learning  FL  is an emerging machine learning paradigm involving multiple clients  e.g.  mobile phone devices  with an incentive to collaborate in solving a machine learning problem coordinated by a central server. FL was proposed in      by Kone n  et al. and McMahan et al. as a viable privacy preserving alternative to traditional centralized machine learning since  by construction  the training data points are decentralized and never transferred by the clients to a central server. Therefore  to a certain degree  FL mitigates the privacy risks associated with centralized data collection.,1
Reliable amortized variational inference with physics based latent distribution correction Bayesian inference for high dimensional inverse problems is challenged by the computational costs of the forward operator and the selection of an appropriate prior distribution. Amortized variational inference addresses these challenges where a neural network is trained to approximate the posterior distribution over existing pairs of model and data. When fed previously unseen data and normally distributed latent samples as input  the pretrained deep neural network    in our case a conditional normalizing flow    provides posterior samples with virtually no cost. However  the accuracy of this approach relies on the availability of high fidelity training data  which seldom exists in geophysical inverse problems due to the heterogeneous structure of the Earth. In addition  accurate amortized variational inference requires the observed data to be drawn from the training data distribution. As such  we propose to increase the resilience of amortized variational inference when faced with data distribution shift via a physics based correction to the conditional normalizing flow latent distribution. To accomplish this  instead of a standard Gaussian latent distribution  we parameterize the latent distribution by a Gaussian distribution with an unknown mean and diagonal covariance. These unknown quantities are then estimated by minimizing the Kullback Leibler divergence between the corrected and true posterior distributions. While generic and applicable to other inverse problems  by means of a seismic imaging example  we show that our correction step improves the robustness of amortized variational inference with respect to changes in number of source experiments  noise variance  and shifts in the prior distribution. This approach provides a seismic image with limited artifacts and an assessment of its uncertainty with approximately the same cost as five reverse time migrations.,1
Fully Decentralized Model based Policy Optimization for Networked Systems Reinforcement learning algorithms require a large amount of samples  this often limits their real world applications on even simple tasks. Such a challenge is more outstanding in multi agent tasks  as each step of operation is more costly requiring communications or shifting or resources. This work aims to improve data efficiency of multi agent control by model based learning. We consider networked systems where agents are cooperative and communicate only locally with their neighbors  and propose the decentralized model based policy optimization framework  DMPO . In our method  each agent learns a dynamic model to predict future states and broadcast their predictions by communication  and then the policies are trained under the model rollouts. To alleviate the bias of model generated data  we restrain the model usage for generating myopic rollouts  thus reducing the compounding error of model generation. To pertain the independence of policy update  we introduce extended value function and theoretically prove that the resulting policy gradient is a close approximation to true policy gradients. We evaluate our algorithm on several benchmarks for intelligent transportation systems  which are connected autonomous vehicle control tasks  Flow and CACC  and adaptive traffic signal control  ATSC . Empirically results show that our method achieves superior data efficiency and matches the performance of model free methods using true models.,1
Multi Study Boosting  Theoretical Considerations for Merging vs. Ensembling Cross study replicability is a powerful model evaluation criterion that emphasizes generalizability of predictions. When training cross study replicable prediction models  it is critical to decide between merging and treating the studies separately. We study boosting algorithms in the presence of potential heterogeneity in predictor outcome relationships across studies and compare two multi study learning strategies     merging all the studies and training a single model  and    multi study ensembling  which involves training a separate model on each study and ensembling the resulting predictions. In the regression setting  we provide theoretical guidelines based on an analytical transition point to determine whether it is more beneficial to merge or to ensemble for boosting with linear learners. In addition  we characterize a bias variance decomposition of estimation error for boosting with component wise linear learners. We verify the theoretical transition point result in simulation and illustrate how it can guide the decision on merging vs. ensembling in an application to breast cancer gene expression data.,1
Contextual Bandits with Large Action Spaces  Made Practical A central problem in sequential decision making is to develop algorithms that are practical and computationally efficient  yet support the use of flexible  general purpose models. Focusing on the contextual bandit problem  recent progress provides provably efficient algorithms with strong empirical performance when the number of possible alternatives   actions   is small  but guarantees for decision making in large  continuous action spaces have remained elusive  leading to a significant gap between theory and practice. We present the first efficient  general purpose algorithm for contextual bandits with continuous  linearly structured action spaces. Our algorithm makes use of computational oracles for  i  supervised learning  and  ii  optimization over the action space  and achieves sample complexity  runtime  and memory independent of the size of the action space. In addition  it is simple and practical. We perform a large scale empirical evaluation  and show that our approach typically enjoys superior performance and efficiency compared to standard baselines.,1
The Union of Manifolds Hypothesis and its Implications for Deep Generative Modelling Deep learning has had tremendous success at learning low dimensional representations of high dimensional data. This success would be impossible if there was no hidden low dimensional structure in data of interest  this existence is posited by the manifold hypothesis  which states that the data lies on an unknown manifold of low intrinsic dimension. In this paper  we argue that this hypothesis does not properly capture the low dimensional structure typically present in data. Assuming the data lies on a single manifold implies intrinsic dimension is identical across the entire data space  and does not allow for subregions of this space to have a different number of factors of variation. To address this deficiency  we put forth the union of manifolds hypothesis  which accommodates the existence of non constant intrinsic dimensions. We empirically verify this hypothesis on commonly used image datasets  finding that indeed  intrinsic dimension should be allowed to vary. We also show that classes with higher intrinsic dimensions are harder to classify  and how this insight can be used to improve classification accuracy. We then turn our attention to the impact of this hypothesis in the context of deep generative models  DGMs . Most current DGMs struggle to model datasets with several connected components and or varying intrinsic dimensions. To tackle these shortcomings  we propose clustered DGMs  where we first cluster the data and then train a DGM on each cluster. We show that clustered DGMs can model multiple connected components with different intrinsic dimensions  and empirically outperform their non clustered counterparts without increasing computational requirements.,1
A New Index for Clustering Evaluation Based on Density Estimation A new index for internal evaluation of clustering is introduced. The index is defined as a mixture of two sub indices. The first sub index   I a   is called the Ambiguous Index  the second sub index   I s   is called the Similarity Index. Calculation of the two sub indices is based on density estimation to each cluster of a partition of the data. An experiment is conducted to test the performance of the new index  and compared with three popular internal clustering evaluation indices    Calinski Harabasz index  Silhouette coefficient  and Davies Bouldin index  on a set of     datasets. The result shows the new index improves the three popular indices by           and      correspondingly.,1
When does SGD favor flat minima  A quantitative characterization via linear stability The observation that stochastic gradient descent  SGD  favors flat minima has played a fundamental role in understanding implicit regularization of SGD and guiding the tuning of hyperparameters. In this paper  we provide a quantitative explanation of this striking phenomenon by relating the particular noise structure of SGD to its  emph linear stability   Wu et al.       . Specifically  we consider training over parameterized models with square loss. We prove that if a global minimum   theta    is linearly stable for SGD  then it must satisfy    H  theta      F leq O  sqrt B   eta    where    H  theta      F  B  eta  denote the Frobenius norm of Hessian at   theta     batch size  and learning rate  respectively. Otherwise  SGD will escape from that minimum  emph exponentially  fast. Hence  for minima accessible to SGD  the flatness    as measured by the Frobenius norm of the Hessian    is bounded independently of the model size and sample size. The key to obtaining these results is exploiting the particular geometry awareness of SGD noise     the noise magnitude is proportional to loss value     the noise directions concentrate in the sharp directions of local landscape. This property of SGD noise provably holds for linear networks and random feature models  RFMs  and is empirically verified for nonlinear networks. Moreover  the validity and practical relevance of our theoretical findings are justified by extensive numerical experiments.,1
Feed Forward Source Free Latent Domain Adaptation via Cross Attention We study the highly practical but comparatively under studied problem of latent domain adaptation  where a source model should be adapted to a target dataset that contains a mixture of unlabelled domain relevant and domain irrelevant examples. Furthermore  motivated by the requirements for data privacy and the need for embedded and resource constrained devices of all kinds to adapt to local data distributions  we focus on the setting of feed forward source free domain adaptation  where adaptation should not require access to the source dataset  and also be back propagation free. Our solution is to meta learn a network capable of embedding the mixed relevance target dataset and dynamically adapting inference for target examples using cross attention. The resulting framework leads to consistent improvement on strong ERM baselines. We also show that our framework sometimes even improves on the upper bound of domain supervised adaptation  where only domain relevant instances are provided for adaptation. This suggests that human annotated domain labels may not always be optimal  and raises the possibility of doing better through automated instance selection.,1
Uncertainty quantification for predictions of atomistic neural networks The value of uncertainty quantification on predictions for trained neural networks  NNs  on quantum chemical reference data is quantitatively explored. For this  the architecture of the PhysNet NN was suitably modified and the resulting model was evaluated with different metrics to quantify calibration  quality of predictions  and whether prediction error and the predicted uncertainty can be correlated. The results from training on the QM  database and evaluating data from the test set within and outside the distribution indicate that error and uncertainty are not linearly related. The results clarify that noise and redundancy complicate property prediction for molecules even in cases for which changes   e.g. double bond migration in two otherwise identical molecules   are small. The model was then applied to a real database of tautomerization reactions. Analysis of the distance between members in feature space combined with other parameters shows that redundant information in the training dataset can lead to large variances and small errors whereas the presence of similar but unspecific information returns large errors but small variances. This was  e.g.  observed for nitro containing aliphatic chains for which predictions were difficult although the training set contained several examples for nitro groups bound to aromatic molecules. This underlines the importance of the composition of the training data and provides chemical insight into how this affects the prediction capabilities of a ML model. Finally  the approach put forward can be used for information based improvement of chemical databases for target applications through active learning optimization.,1
Learning structures of the French clinical language development and validation of word embedding models using    million clinical reports from electronic health records Background,1
Forget me not  Contrastive Critics for Mitigating Posterior Collapse Variational autoencoders  VAEs  suffer from posterior collapse  where the powerful neural networks used for modeling and inference optimize the objective without meaningfully using the latent representation. We introduce inference critics that detect and incentivize against posterior collapse by requiring correspondence between latent variables and the observations. By connecting the critic s objective to the literature in self supervised contrastive representation learning  we show both theoretically and empirically that optimizing inference critics increases the mutual information between observations and latents  mitigating posterior collapse. This approach is straightforward to implement and requires significantly less training time than prior methods  yet obtains competitive results on three established datasets. Overall  the approach lays the foundation to bridge the previously disconnected frameworks of contrastive learning and probabilistic modeling with variational autoencoders  underscoring the benefits both communities may find at their intersection.,1
Conservative SPDEs as fluctuating mean field limits of stochastic gradient descent The convergence of stochastic interacting particle systems in the mean field limit to solutions to conservative stochastic partial differential equations is shown  with optimal rate of convergence. As a second main result  a quantitative central limit theorem for such SPDEs is derived  again with optimal rate of convergence.,1
Uncertainty Calibration in Bayesian Neural Networks via Distance Aware Priors As we move away from the data  the predictive uncertainty should increase  since a great variety of explanations are consistent with the little available information. We introduce Distance Aware Prior  DAP  calibration  a method to correct overconfidence of Bayesian deep learning models outside of the training domain. We define DAPs as prior distributions over the model parameters that depend on the inputs through a measure of their distance from the training set. DAP calibration is agnostic to the posterior inference method  and it can be performed as a post processing step. We demonstrate its effectiveness against several baselines in a variety of classification and regression problems  including benchmarks designed to test the quality of predictive distributions away from the data.,1
Predicting Out of Domain Generalization with Local Manifold Smoothness Understanding how machine learning models generalize to new environments is a critical part of their safe deployment. Recent work has proposed a variety of complexity measures that directly predict or theoretically bound the generalization capacity of a model. However  these methods rely on a strong set of assumptions that in practice are not always satisfied. Motivated by the limited settings in which existing measures can be applied  we propose a novel complexity measure based on the local manifold smoothness of a classifier. We define local manifold smoothness as a classifier s output sensitivity to perturbations in the manifold neighborhood around a given test point. Intuitively  a classifier that is less sensitive to these perturbations should generalize better. To estimate smoothness we sample points using data augmentation and measure the fraction of these points classified into the majority class. Our method only requires selecting a data augmentation method and makes no other assumptions about the model or data distributions  meaning it can be applied even in out of domain  OOD  settings where existing methods cannot. In experiments on robustness benchmarks in image classification  sentiment analysis  and natural language inference  we demonstrate a strong and robust correlation between our manifold smoothness measure and actual OOD generalization on over       models evaluated on over     train test domain pairs.,1
Discrimination in machine learning algorithms Machine learning algorithms are routinely used for business decisions that may directly affect individuals  for example  because a credit scoring algorithm refuses them a loan. It is then relevant from an ethical  and legal  point of view to ensure that these algorithms do not discriminate based on sensitive attributes  like sex or race   which may occur unwittingly and unknowingly by the operator and the management. Statistical tools and methods are then required to detect and eliminate such potential biases.,1
Learning Optimal Transport Between two Empirical Distributions with Normalizing Flows Optimal transport  OT  provides effective tools for comparing and mapping probability measures. We propose to leverage the flexibility of neural networks to learn an approximate optimal transport map. More precisely  we present a new and original method to address the problem of transporting a finite set of samples associated with a first underlying unknown distribution towards another finite set of samples drawn from another unknown distribution. We show that a particular instance of invertible neural networks  namely the normalizing flows  can be used to approximate the solution of this OT problem between a pair of empirical distributions. To this aim  we propose to relax the Monge formulation of OT by replacing the equality constraint on the push forward measure by the minimization of the corresponding Wasserstein distance. The push forward operator to be retrieved is then restricted to be a normalizing flow which is trained by optimizing the resulting cost function. This approach allows the transport map to be discretized as a composition of functions. Each of these functions is associated to one sub flow of the network  whose output provides intermediate steps of the transport between the original and target measures. This discretization yields also a set of intermediate barycenters between the two measures of interest. Experiments conducted on toy examples as well as a challenging task of unsupervised translation demonstrate the interest of the proposed method. Finally  some experiments show that the proposed approach leads to a good approximation of the true OT.,1
A State Transition Model for Mobile Notifications via Survival Analysis Mobile notifications have become a major communication channel for social networking services to keep users informed and engaged. As more mobile applications push notifications to users  they constantly face decisions on what to send  when and how. A lack of research and methodology commonly leads to heuristic decision making. Many notifications arrive at an inappropriate moment or introduce too many interruptions  failing to provide value to users and spurring users  complaints. In this paper we explore unique features of interactions between mobile notifications and user engagement. We propose a state transition framework to quantitatively evaluate the effectiveness of notifications. Within this framework  we develop a survival model for badging notifications assuming a log linear structure and a Weibull distribution. Our results show that this model achieves more flexibility for applications and superior prediction accuracy than a logistic regression model. In particular  we provide an online use case on notification delivery time optimization to show how we make better decisions  drive more user engagement  and provide more value to users.,1
Model Selection in Reinforcement Learning with General Function Approximations We consider model selection for classic Reinforcement Learning  RL  environments    Multi Armed Bandits  MABs  and Markov Decision Processes  MDPs     under general function approximations. In the model selection framework  we do not know the function classes  denoted by   mathcal F   and   mathcal M    where the true models    reward generating function for MABs and and transition kernel for MDPs    lie  respectively. Instead  we are given  M  nested function  hypothesis  classes such that true models are contained in at least one such class. In this paper  we propose and analyze efficient model selection algorithms for MABs and MDPs  that  emph adapt  to the smallest function class  among the nested  M  classes  containing the true underlying model. Under a separability assumption on the nested hypothesis classes  we show that the cumulative regret of our adaptive algorithms match to that of an oracle which knows the correct function classes  i.e.    cF  and   cM   a priori. Furthermore  for both the settings  we show that the cost of model selection is an additive term in the regret having weak  logarithmic  dependence on the learning horizon  T .,1
Classifying Crop Types using Gaussian Bayesian Models and Neural Networks on GHISACONUS USGS data from NASA Hyperspectral Satellite Imagery Hyperspectral Imagining is a type of digital imaging in which each pixel contains typically hundreds of wavelengths of light providing spectroscopic information about the materials present in the pixel. In this paper we provide classification methods for determining crop type in the USGS GHISACONUS data  which contains around       pixel spectra from the five major U.S. agricultural crops  winter wheat  rice  corn  soybeans  and cotton  collected by the NASA Hyperion satellite  and includes the spectrum  geolocation  crop type  and stage of growth for each pixel. We apply standard LDA and QDA as well as Bayesian custom versions that compute the joint probability of crop type and stage  and then the marginal probability for crop type  outperforming the non Bayesian methods. We also test a single layer neural network with dropout on the data  which performs comparable to LDA and QDA but not as well as the Bayesian methods.,1
Ultra low latency recurrent neural network inference on FPGAs for physics applications with hls ml Recurrent neural networks have been shown to be effective architectures for many tasks in high energy physics  and thus have been widely adopted. Their use in low latency environments has  however  been limited as a result of the difficulties of implementing recurrent architectures on field programmable gate arrays  FPGAs . In this paper we present an implementation of two types of recurrent neural network layers    long short term memory and gated recurrent unit    within the hls ml framework. We demonstrate that our implementation is capable of producing effective designs for both small and large models  and can be customized to meet specific design requirements for inference latencies and FPGA resources. We show the performance and synthesized designs for multiple neural networks  many of which are trained specifically for jet identification tasks at the CERN Large Hadron Collider.,1
Do Not Sleep on Linear Models  Simple and Interpretable Techniques Outperform Deep Learning for Sleep Scoring Over the last few years  research in automatic sleep scoring has mainly focused on developing increasingly complex deep learning architectures. However  recently these approaches achieved only marginal improvements  often at the expense of requiring more data and more expensive training procedures. Despite all these efforts and their satisfactory performance  automatic sleep staging solutions are not widely adopted in a clinical context yet. We argue that most deep learning solutions for sleep scoring are limited in their real world applicability as they are hard to train  deploy  and reproduce. Moreover  these solutions lack interpretability and transparency  which are often key to increase adoption rates. In this work  we revisit the problem of sleep stage classification using classical machine learning. Results show that state of the art performance can be achieved with a conventional machine learning pipeline consisting of preprocessing  feature extraction  and a simple machine learning model. In particular  we analyze the performance of a linear model and a non linear  gradient boosting  model. Our approach surpasses state of the art  that uses the same data  on two public datasets  Sleep EDF SC     MF   .     and Sleep EDF ST  MF   .      while achieving competitive results on Sleep EDF SC     MF   .     and MASS SS   MF   .    . We show that  for the sleep stage scoring task  the expressiveness of an engineered feature vector is on par with the internally learned representations of deep learning models. This observation opens the door to clinical adoption  as a representative feature vector allows to leverage both the interpretability and successful track record of traditional machine learning models.,1
Alternating minimization for generalized rank one matrix sensing  Sharp predictions from a random initialization We consider the problem of estimating the factors of a rank     matrix with i.i.d. Gaussian  rank     measurements that are nonlinearly transformed and corrupted by noise. Considering two prototypical choices for the nonlinearity  we study the convergence properties of a natural alternating update rule for this nonconvex optimization problem starting from a random initialization. We show sharp convergence guarantees for a sample split version of the algorithm by deriving a deterministic recursion that is accurate even in high dimensional problems. Notably  while the infinite sample population update is uninformative and suggests exact recovery in a single step  the algorithm    and our deterministic prediction    converges geometrically fast from a random initialization. Our sharp  non asymptotic analysis also exposes several other fine grained properties of this problem  including how the nonlinearity and noise level affect convergence behavior.,1
Variational Inference of overparameterized Bayesian Neural Networks  a theoretical and empirical study This paper studies the Variational Inference  VI  used for training Bayesian Neural Networks  BNN  in the overparameterized regime  i.e.  when the number of neurons tends to infinity. More specifically  we consider overparameterized two layer BNN and point out a critical issue in the mean field VI training. This problem arises from the decomposition of the lower bound on the evidence  ELBO  into two terms  one corresponding to the likelihood function of the model and the second to the Kullback Leibler  KL  divergence between the prior distribution and the variational posterior. In particular  we show both theoretically and empirically that there is a trade off between these two terms in the overparameterized regime only when the KL is appropriately re scaled with respect to the ratio between the the number of observations and neurons. We also illustrate our theoretical results with numerical experiments that highlight the critical choice of this ratio.,1
Sliced Wasserstein normalizing flows  beyond maximum likelihood training Despite their advantages  normalizing flows generally suffer from several shortcomings including their tendency to generate unrealistic data  e.g.  images  and their failing to detect out of distribution data. One reason for these deficiencies lies in the training strategy which traditionally exploits a maximum likelihood principle only. This paper proposes a new training paradigm based on a hybrid objective function combining the maximum likelihood principle  MLE  and a sliced Wasserstein distance. Results obtained on synthetic toy examples and real image data sets show better generative abilities in terms of both likelihood and visual aspects of the generated samples. Reciprocally  the proposed approach leads to a lower likelihood of out of distribution data  demonstrating a greater data fidelity of the resulting flows.,1
Estimating Classification Confidence Using Kernel Densities This paper investigates the post hoc calibration of confidence for  exploratory  machine learning classification problems. The difficulty in these problems stems from the continuing desire to push the boundaries of which categories have enough examples to generalize from when curating datasets  and confusion regarding the validity of those categories. We argue that for such problems the  one versus all  approach  top label calibration  must be used rather than the  calibrate the full response matrix  approach advocated elsewhere in the literature. We introduce and test four new algorithms designed to handle the idiosyncrasies of category specific confidence estimation. Chief among these methods is the use of kernel density ratios for confidence calibration including a novel  bulletproof algorithm for choosing the bandwidth. We test our claims and explore the limits of calibration on a bioinformatics application  PhANNs  as well as the classic MNIST benchmark. Finally  our analysis argues that post hoc calibration should always be performed  should be based only on the test dataset  and should be sanity checked visually.,1
PASHA  Efficient HPO with Progressive Resource Allocation Hyperparameter optimization  HPO  and neural architecture search  NAS  are methods of choice to obtain the best in class machine learning models  but in practice they can be costly to run. When models are trained on large datasets  tuning them with HPO or NAS rapidly becomes prohibitively expensive for practitioners  even when efficient multi fidelity methods are employed. We propose an approach to tackle the challenge of tuning machine learning models trained on large datasets with limited computational resources. Our approach  named PASHA  is able to dynamically allocate maximum resources for the tuning procedure depending on the need. The experimental comparison shows that PASHA identifies well performing hyperparameter configurations and architectures while consuming significantly fewer computational resources than solutions like ASHA.,1
Bayesian Recurrent Units and the Forward Backward Algorithm Using Bayes s theorem  we derive a unit wise recurrence as well as a backward recursion similar to the forward backward algorithm. The resulting Bayesian recurrent units can be integrated as recurrent neural networks within deep learning frameworks  while retaining a probabilistic interpretation from the direct correspondence with hidden Markov models. Whilst the contribution is mainly theoretical  experiments on speech recognition indicate that adding the derived units at the end of state of the art recurrent architectures can improve the performance at a very low cost in terms of trainable parameters.,1
Characterizing the Effect of Class Imbalance on the Learning Dynamics Data imbalance is a common problem in the machine learning literature that can have a critical effect on the performance of a model. Various solutions exist   such as the ones that focus on resampling or data generation   but their impact on the convergence of gradient based optimizers used in deep learning is not understood. We here elucidate the significant negative impact of data imbalance on learning  showing that the learning curves for minority and majority classes follow sub optimal trajectories when training with a gradient based optimizer. The reason is not only that the gradient signal neglects the minority classes  but also that the minority classes are subject to a larger directional noise  which slows their learning by an amount related to the imbalance ratio. To address this problem  we propose a new algorithmic solution  for which we provide a detailed analysis of its convergence behavior. We show both theoretically and empirically that this new algorithm exhibits a better behavior with more stable learning curves for each class  as well as a better generalization performance.,1
Stochastic Functional Analysis and Multilevel Vector Field Anomaly Detection Massive vector field datasets are common in multi spectral optical and radar sensors and modern multimodal MRI data  among many other areas of application. In this paper we develop a novel stochastic functional analysis approach for detecting anomalies based on the covariance structure of nominal stochastic behavior across a domain with multi band vector field data. An optimal vector field Karhunen Loeve  KL  expansion is applied to such random field data. A series of multilevel orthogonal functional subspaces is constructed from the geometry of the domain  adapted from the KL expansion. Detection is achieved by examining the projection of the random field on the multilevel basis. The anomalies can be quantified in suitable normed spaces based on local and global information. In addition  reliable hypothesis tests are formed with controllable distributions that do not require prior assumptions on probability distributions of the data. Only the covariance function is needed  which makes for significantly simpler estimates. Furthermore this approach allows stochastic vector based fusion of anomalies without any loss of information. The method is applied to the important problem of deforestation and degradation in the Amazon forest. This is a complex non monotonic process  as forests can degrade and recover. This particular problem is further compounded by the presence of clouds that are hard to remove with current masking algorithms. Using multi spectral satellite data from Sentinel    the multilevel filter is constructed and anomalies are treated as deviations from the initial state of the forest. Forest anomalies are quantified with robust hypothesis tests and distinguished from false variations such as cloud cover. Our approach shows the advantage of using multiple bands of data in a vectorized complex  leading to better anomaly detection beyond the capabilities of scalar based methods.,1
Guaranteed Discovery of Controllable Latent States with Multi Step Inverse Models A person walking along a city street who tries to model all aspects of the world would quickly be overwhelmed by a multitude of shops  cars  and people moving in and out of view  following their own complex and inscrutable dynamics. Exploration and navigation in such an environment is an everyday task  requiring no vast exertion of mental resources. Is it possible to turn this fire hose of sensory information into a minimal latent state which is necessary and sufficient for an agent to successfully act in the world  We formulate this question concretely  and propose the Agent Controllable State Discovery algorithm  AC State   which has theoretical guarantees and is practically demonstrated to discover the  textit minimal controllable latent state  which contains all of the information necessary for controlling the agent  while fully discarding all irrelevant information. This algorithm consists of a multi step inverse model  predicting actions from distant observations  with an information bottleneck. AC State enables localization  exploration  and navigation without reward or demonstrations. We demonstrate the discovery of controllable latent state in three domains  localizing a robot arm with distractions  e.g.  changing lighting conditions and background   exploring in a maze alongside other agents  and navigating in the Matterport house simulator.,1
Gradients should stay on Path  Better Estimators of the Reverse  and Forward KL Divergence for Normalizing Flows We propose an algorithm to estimate the path gradient of both the reverse and forward Kullback Leibler divergence for an arbitrary manifestly invertible normalizing flow. The resulting path gradient estimators are straightforward to implement  have lower variance  and lead not only to faster convergence of training but also to better overall approximation results compared to standard total gradient estimators. We also demonstrate that path gradient training is less susceptible to mode collapse. In light of our results  we expect that path gradient estimators will become the new standard method to train normalizing flows for variational inference.,1
Probabilistic forecasting for geosteering in fluvial successions using a generative adversarial network Quantitative workflows utilizing real time data to constrain ahead of bit uncertainty have the potential to improve geosteering significantly. Fast updates based on real time data are essential when drilling in complex reservoirs with high uncertainties in pre drill models. However  practical assimilation of real time data requires effective geological modeling and mathematically robust parameterization. We propose a generative adversarial deep neural network  GAN   trained to reproduce geologically consistent  D sections of fluvial successions. Offline training produces a fast GAN based approximation of complex geology parameterized as a    dimensional model vector with standard Gaussian distribution of each component. Probabilistic forecasts are generated using an ensemble of equiprobable model vector realizations. A forward modeling sequence  including a GAN  converts the initial  prior  ensemble of realizations into EM log predictions. An ensemble smoother minimizes statistical misfits between predictions and real time data  yielding an update of model vectors and reduced uncertainty around the well. Updates can be then translated to probabilistic predictions of facies and resistivities. The present paper demonstrates a workflow for geosteering in an outcrop based  synthetic fluvial succession. In our example  the method reduces uncertainty and correctly predicts most major geological features up to     meters ahead of drill bit.,1
D CBRS  Accounting For Intra Class Diversity in Continual Learning Continual learning    accumulating knowledge from a sequence of learning experiences    is an important yet challenging problem. In this paradigm  the model s performance for previously encountered instances may substantially drop as additional data are seen. When dealing with class imbalanced data  forgetting is further exacerbated. Prior work has proposed replay based approaches which aim at reducing forgetting by intelligently storing instances for future replay. Although Class Balancing Reservoir Sampling  CBRS  has been successful in dealing with imbalanced data  the intra class diversity has not been accounted for  implicitly assuming that each instance of a class is equally informative. We present Diverse CBRS  D CBRS   an algorithm that allows us to consider within class diversity when storing instances in the memory. Our results show that D CBRS outperforms state of the art memory management continual learning algorithms on data sets with considerable intra class diversity.,1
On uniform in time diffusion approximation for stochastic gradient descent The diffusion approximation of stochastic gradient descent  SGD  in current literature is only valid on a finite time interval. In this paper  we establish the uniform in time diffusion approximation of SGD  by only assuming that the expected loss is strongly convex and some other mild conditions  without assuming the convexity of each random loss function. The main technique is to establish the exponential decay rates of the derivatives of the solution to the backward Kolmogorov equation. The uniform in time approximation allows us to study asymptotic behaviors of SGD via the continuous stochastic differential equation  SDE  even when the random objective function  f  cdot  xi   is not strongly convex.,1
Mathematical Foundations of Graph Based Bayesian Semi Supervised Learning In recent decades  science and engineering have been revolutionized by a momentous growth in the amount of available data. However  despite the unprecedented ease with which data are now collected and stored  labeling data by supplementing each feature with an informative tag remains to be challenging. Illustrative tasks where the labeling process requires expert knowledge or is tedious and time consuming include labeling X rays with a diagnosis  protein sequences with a protein type  texts by their topic  tweets by their sentiment  or videos by their genre. In these and numerous other examples  only a few features may be manually labeled due to cost and time constraints. How can we best propagate label information from a small number of expensive labeled features to a vast number of unlabeled ones  This is the question addressed by semi supervised learning  SSL .,1
Improved conformalized quantile regression Conformalized quantile regression is a procedure that inherits the advantages of conformal prediction and quantile regression. That is  we use quantile regression to estimate the true conditional quantile and then apply a conformal step on a calibration set to ensure marginal coverage. In this way  we get adaptive prediction intervals that account for heteroscedasticity. However  the aforementioned conformal step lacks adaptiveness as described in  Romano et al.       . To overcome this limitation  instead of applying a single conformal step after estimating conditional quantiles with quantile regression  we propose to cluster the explanatory variables weighted by their permutation importance with an optimized k means and apply k conformal steps. To show that this improved version outperforms the classic version of conformalized quantile regression and is more adaptive to heteroscedasticity  we extensively compare the prediction intervals of both in open datasets.,1
Adaptive Step Size Methods for Compressed SGD Compressed Stochastic Gradient Descent  SGD  algorithms have been recently proposed to address the communication bottleneck in distributed and decentralized optimization problems  such as those that arise in federated machine learning. Existing compressed SGD algorithms assume the use of non adaptive step sizes constant or diminishing  to provide theoretical convergence guarantees. Typically  the step sizes are fine tuned in practice to the dataset and the learning algorithm to provide good empirical performance. Such fine tuning might be impractical in many learning scenarios  and it is therefore of interest to study compressed SGD using adaptive step sizes. Motivated by prior work on adaptive step size methods for SGD to train neural networks efficiently in the uncompressed setting  we develop an adaptive step size method for compressed SGD. In particular  we introduce a scaling technique for the descent step in compressed SGD  which we use to establish order optimal convergence rates for convex smooth and strong convex smooth objectives under an interpolation condition and for non convex objectives under a strong growth condition. We also show through simulation examples that without this scaling  the algorithm can fail to converge. We present experimental results on deep neural networks for real world datasets  and compare the performance of our proposed algorithm with previously proposed compressed SGD methods in literature  and demonstrate improved performance on ResNet     ResNet    and DenseNet architectures for CIFAR     and CIFAR    datasets at various levels of compression.,1
A clinically motivated self supervised approach for content based image retrieval of CT liver images Deep learning based approaches for content based image retrieval  CBIR  of CT liver images is an active field of research  but suffers from some critical limitations. First  they are heavily reliant on labeled data  which can be challenging and costly to acquire. Second  they lack transparency and explainability  which limits the trustworthiness of deep CBIR systems. We address these limitations by     proposing a self supervised learning framework that incorporates domain knowledge into the training procedure and     providing the first representation learning explainability analysis in the context of CBIR of CT liver images. Results demonstrate improved performance compared to the standard self supervised approach across several metrics  as well as improved generalisation across datasets. Further  we conduct the first representation learning explainability analysis in the context of CBIR  which reveals new insights into the feature extraction process. Lastly  we perform a case study with cross examination CBIR that demonstrates the usability of our proposed framework. We believe that our proposed framework could play a vital role in creating trustworthy deep CBIR systems that can successfully take advantage of unlabeled data.,1
Multiscale Causal Structure Learning The inference of causal structures from observed data plays a key role in unveiling the underlying dynamics of the system. This paper exposes a novel method  named Multiscale Causal Structure Learning  MS CASTLE   to estimate the structure of linear causal relationships occurring at different time scales. Differently from existing approaches  MS CASTLE takes explicitly into account instantaneous and lagged inter relations between multiple time series  represented at different scales  hinging on stationary wavelet transform and non convex optimization. MS CASTLE incorporates  as a special case  a single scale version named SS CASTLE  which compares favorably in terms of computational efficiency  performance and robustness with respect to the state of the art onto synthetic data. We used MS CASTLE to study the multiscale causal structure of the risk of    global equity markets  during covid    pandemic  illustrating how MS CASTLE can extract meaningful information thanks to its multiscale analysis  outperforming SS CASTLE. We found that the most persistent and strongest interactions occur at mid term time resolutions. Moreover  we identified the stock markets that drive the risk during the considered period  Brazil  Canada and Italy. The proposed approach can be exploited by financial investors who  depending to their investment horizon  can manage the risk within equity portfolios from a causal perspective.,1
Discover Life Skills for Planning with Bandits via Observing and Learning How the World Works We propose a novel approach for planning agents to compose abstract skills via observing and learning from historical interactions with the world. Our framework operates in a Markov state space model via a set of actions under unknown pre conditions. We formulate skills as high level abstract policies that propose action plans based on the current state. Each policy learns new plans by observing the states  transitions while the agent interacts with the world. Such an approach automatically learns new plans to achieve specific intended effects  but the success of such plans is often dependent on the states in which they are applicable. Therefore  we formulate the evaluation of such plans as infinitely many multi armed bandit problems  where we balance the allocation of resources on evaluating the success probability of existing arms and exploring new options. The result is a planner capable of automatically learning robust high level skills under a noisy environment  such skills implicitly learn the action pre condition without explicit knowledge. We show that this planning approach is experimentally very competitive in high dimensional state space domains.,1
Nonparametric regression with modified ReLU networks We consider regression estimation with modified ReLU neural networks in which network weight matrices are first modified by a function   alpha  before being multiplied by input vectors. We give an example of continuous  piecewise linear function   alpha  for which the empirical risk minimizers over the classes of modified ReLU networks with  l    and squared  l    penalties attain  up to a logarithmic factor  the minimax rate of prediction of unknown   beta  smooth function.,1
The Poisson binomial mechanism for secure and private federated learning We introduce the Poisson Binomial mechanism  PBM   a discrete differential privacy mechanism for distributed mean estimation  DME  with applications to federated learning and analytics. We provide a tight analysis of its privacy guarantees  showing that it achieves the same privacy accuracy trade offs as the continuous Gaussian mechanism. Our analysis is based on a novel bound on the R nyi divergence of two Poisson binomial distributions that may be of independent interest.,1
Representing Random Utility Choice Models with Neural Networks Motivated by the successes of deep learning  we propose a class of neural network based discrete choice models  called RUMnets  which is inspired by the random utility maximization  RUM  framework. This model formulates the agents  random utility function using the sample average approximation  SAA  method. We show that RUMnets sharply approximate the class of RUM discrete choice models  any model derived from random utility maximization has choice probabilities that can be approximated arbitrarily closely by a RUMnet. Reciprocally  any RUMnet is consistent with the RUM principle. We derive an upper bound on the generalization error of RUMnets fitted on choice data  and gain theoretical insights on their ability to predict choices on new  unseen data depending on critical parameters of the dataset and architecture. By leveraging open source libraries for neural networks  we find that RUMnets outperform other state of the art choice modeling and machine learning methods by a significant margin on two real world datasets.,1
Multiple Robust Learning for Recommendation In recommender systems  a common problem is the presence of various biases in the collected data  which deteriorates the generalization ability of the recommendation models and leads to inaccurate predictions. Doubly robust  DR  learning has been studied in many tasks in RS  with the advantage that unbiased learning can be achieved when either a single imputation or a single propensity model is accurate. In this paper  we propose a multiple robust  MR  estimator that can take the advantage of multiple candidate imputation and propensity models to achieve unbiasedness. Specifically  the MR estimator is unbiased when any of the imputation or propensity models  or a linear combination of these models is accurate. Theoretical analysis shows that the proposed MR is an enhanced version of DR when only having a single imputation and propensity model  and has a smaller bias. Inspired by the generalization error bound of MR  we further propose a novel multiple robust learning approach with stabilization. We conduct extensive experiments on real world and semi synthetic datasets  which demonstrates the superiority of the proposed approach over state of the art methods.,1
Improving the Accuracy of Marginal Approximations in Likelihood Free Inference via Localisation Likelihood free methods are an essential tool for performing inference for implicit models which can be simulated from  but for which the corresponding likelihood is intractable. However  common likelihood free methods do not scale well to a large number of model parameters. A promising approach to high dimensional likelihood free inference involves estimating low dimensional marginal posteriors by conditioning only on summary statistics believed to be informative for the low dimensional component  and then combining the low dimensional approximations in some way. In this paper  we demonstrate that such low dimensional approximations can be surprisingly poor in practice for seemingly intuitive summary statistic choices. We describe an idealized low dimensional summary statistic that is  in principle  suitable for marginal estimation. However  a direct approximation of the idealized choice is difficult in practice. We thus suggest an alternative approach to marginal estimation which is easier to implement and automate. Given an initial choice of low dimensional summary statistic that might only be informative about a marginal posterior location  the new method improves performance by first crudely localising the posterior approximation using all the summary statistics to ensure global identifiability  followed by a second step that hones in on an accurate low dimensional approximation using the low dimensional summary statistic. We show that the posterior this approach targets can be represented as a logarithmic pool of posterior distributions based on the low dimensional and full summary statistics  respectively. The good performance of our method is illustrated in several examples.,1
Subgraph Frequency Distribution Estimation using Graph Neural Networks Small subgraphs  graphlets  are important features to describe fundamental units of a large network. The calculation of the subgraph frequency distributions has a wide application in multiple domains including biology and engineering. Unfortunately due to the inherent complexity of this task  most of the existing methods are computationally intensive and inefficient. In this work  we propose GNNS  a novel representational learning framework that utilizes graph neural networks to sample subgraphs efficiently for estimating their frequency distribution. Our framework includes an inference model and a generative model that learns hierarchical embeddings of nodes  subgraphs  and graph types. With the learned model and embeddings  subgraphs are sampled in a highly scalable and parallel way and the frequency distribution estimation is then performed based on these sampled subgraphs. Eventually  our methods achieve comparable accuracy and a significant speedup by three orders of magnitude compared to existing methods.,1
Neural Stein critics with staged  L    regularization Learning to differentiate model distributions from observed data is a fundamental problem in statistics and machine learning  and high dimensional data remains a challenging setting for such problems. Metrics that quantify the disparity in probability distributions  such as the Stein discrepancy  play an important role in statistical testing in high dimensions. In this paper  we consider the setting where one wishes to distinguish between data sampled from an unknown probability distribution and a nominal model distribution. While recent studies revealed that the optimal  L    regularized Stein critic equals the difference of the score functions of two probability distributions up to a multiplicative constant  we investigate the role of  L    regularization when training a neural network Stein discrepancy critic function. Motivated by the Neural Tangent Kernel theory of training neural networks  we develop a novel staging procedure for the weight of regularization over training time. This leverages the advantages of highly regularized training at early times while also empirically delaying overfitting. Theoretically  we relate the training dynamic with large regularization weight to the kernel regression optimization of  lazy training  regime in early training times. The benefit of the staged  L    regularization is demonstrated on simulated high dimensional distribution drift data and an application to evaluating generative models of image data.,1
Selection of the Most Probable Best We consider an expected value ranking and selection problem where all k solutions  simulation outputs depend on a common uncertain input model. Given that the uncertainty of the input model is captured by a probability simplex on a finite support  we define the most probable best  MPB  to be the solution whose probability of being optimal is the largest. To devise an efficient sampling algorithm to find the MPB  we first derive a lower bound to the large deviation rate of the probability of falsely selecting the MPB  then formulate an optimal computing budget allocation  OCBA  problem to find the optimal static sampling ratios for all solution input model pairs that maximize the lower bound. We devise a series of sequential algorithms that apply interpretable and computationally efficient sampling rules and prove their sampling ratios achieve the optimality conditions for the OCBA problem as the simulation budget increases. The algorithms are benchmarked against a state of the art sequential sampling algorithm designed for contextual ranking and selection problems and demonstrated to have superior empirical performances at finding the MPB.,1
High dimensional stochastic linear contextual bandit with missing covariates Recent works in bandit problems adopted lasso convergence theory in the sequential decision making setting. Even with fully observed contexts  there are technical challenges that hinder the application of existing lasso convergence theory     proving the restricted eigenvalue condition under conditionally sub Gaussian noise and    accounting for the dependence between the context variables and the chosen actions. This paper studies the effect of missing covariates on regret for stochastic linear bandit algorithms. Our work provides a high probability upper bound on the regret incurred by the proposed algorithm in terms of covariate sampling probabilities  showing that the regret degrades due to missingness by at most   zeta  min      where   zeta  min   is the minimum probability of observing covariates in the context vector. We illustrate our algorithm for the practical application of experimental design for collecting gene expression data by a sequential selection of class discriminating DNA probes.,1
Partial Disentanglement via Mechanism Sparsity Disentanglement via mechanism sparsity was introduced recently as a principled approach to extract latent factors without supervision when the causal graph relating them in time is sparse  and or when actions are observed and affect them sparsely. However  this theory applies only to ground truth graphs satisfying a specific criterion. In this work  we introduce a generalization of this theory which applies to any ground truth graph and specifies qualitatively how disentangled the learned representation is expected to be  via a new equivalence relation over models we call consistency. This equivalence captures which factors are expected to remain entangled and which are not based on the specific form of the ground truth graph. We call this weaker form of identifiability partial disentanglement. The graphical criterion that allows complete disentanglement  proposed in an earlier work  can be derived as a special case of our theory. Finally  we enforce graph sparsity with constrained optimization and illustrate our theory and algorithm in simulations.,1
A Generative Framework for Personalized Learning and Estimation  Theory  Algorithms  and Privacy A distinguishing characteristic of federated learning is that the  local  client data could have statistical heterogeneity. This heterogeneity has motivated the design of personalized learning  where individual  personalized  models are trained  through collaboration. There have been various personalization methods proposed in literature  with seemingly very different forms and methods ranging from use of a single global model for local regularization and model interpolation  to use of multiple global models for personalized clustering  etc. In this work  we begin with a generative framework that could potentially unify several different algorithms as well as suggest new algorithms. We apply our generative framework to personalized estimation  and connect it to the classical empirical Bayes  methodology. We develop private personalized estimation under this framework. We then use our generative framework for learning  which unifies several known personalized FL algorithms and also suggests new ones  we propose and study a new algorithm AdaPeD based on a Knowledge Distillation  which numerically outperforms several known algorithms. We also develop privacy for personalized learning methods with guarantees for user level privacy and composition. We numerically evaluate the performance as well as the privacy for both the estimation and learning problems  demonstrating the advantages of our proposed methods.,1
Recommendation Systems with Distribution Free Reliability Guarantees When building recommendation systems  we seek to output a helpful set of items to the user. Under the hood  a ranking model predicts which of two candidate items is better  and we must distill these pairwise comparisons into the user facing output. However  a learned ranking model is never perfect  so taking its predictions at face value gives no guarantee that the user facing output is reliable. Building from a pre trained ranking model  we show how to return a set of items that is rigorously guaranteed to contain mostly good items. Our procedure endows any ranking model with rigorous finite sample control of the false discovery rate  FDR   regardless of the  unknown  data distribution. Moreover  our calibration algorithm enables the easy and principled integration of multiple objectives in recommender systems. As an example  we show how to optimize for recommendation diversity subject to a user specified level of FDR control  circumventing the need to specify ad hoc weights of a diversity loss against an accuracy loss. Throughout  we focus on the problem of learning to rank a set of possible recommendations  evaluating our methods on the Yahoo  Learning to Rank and MSMarco datasets.,1
Uniform Stability for First Order Empirical Risk Minimization We consider the problem of designing uniformly stable first order optimization algorithms for empirical risk minimization. Uniform stability is often used to obtain generalization error bounds for optimization algorithms  and we are interested in a general approach to achieve it. For Euclidean geometry  we suggest a black box conversion which given a smooth optimization algorithm  produces a uniformly stable version of the algorithm while maintaining its convergence rate up to logarithmic factors. Using this reduction we obtain a  nearly  optimal algorithm for smooth optimization with convergence rate   widetilde O    T     and uniform stability  O T   n    resolving an open problem of Chen et al.         Attia and Koren       . For more general geometries  we develop a variant of Mirror Descent for smooth optimization with convergence rate   widetilde O    T   and uniform stability  O T n    leaving open the question of devising a general conversion method as in the Euclidean case.,1
A Near Optimal Primal Dual Method for Off Policy Learning in CMDP As an important framework for safe Reinforcement Learning  the Constrained Markov Decision Process  CMDP  has been extensively studied in the recent literature. However  despite the rich results under various on policy learning settings  there still lacks some essential understanding of the offline CMDP problems  in terms of both the algorithm design and the information theoretic sample complexity lower bound. In this paper  we focus on solving the CMDP problems where only offline data are available. By adopting the concept of the single policy concentrability coefficient  C     we establish an   Omega left  frac  min left    mathcal S    mathcal A     mathcal S   I right   C        gamma    epsilon    right   sample complexity lower bound for the offline CMDP problem  where  I  stands for the number of constraints. By introducing a simple but novel deviation control mechanism  we propose a near optimal primal dual learning algorithm called DPDL. This algorithm provably guarantees zero constraint violation and its sample complexity matches the above lower bound except for an   tilde  mathcal O       gamma         factor. Comprehensive discussion on how to deal with the unknown constant  C    and the potential asynchronous structure on the offline dataset are also included.,1
Employing Feature Selection Algorithms to Determine the Immune State of Mice with Rheumatoid Arthritis The immune response is a dynamic process by which the body determines whether an antigen is self or nonself. The state of this dynamic process is defined by the relative balance and population of inflammatory and regulatory actors which comprise this decision making process. The goal of immunotherapy as applied to  e.g. Rheumatoid Arthritis  RA   then  is to bias the immune state in favor of the regulatory actors   thereby shutting down autoimmune pathways in the response. While there are several known approaches to immunotherapy  the effectiveness of the therapy will depend on how this intervention alters the evolution of this state. Unfortunately  this process is determined not only by the dynamics of the process  but the state of the system at the time of intervention   a state which is difficult if not impossible to determine prior to application of the therapy.,1
Differentially Private Graph Learning via Sensitivity Bounded Personalized PageRank Personalized PageRank  PPR  is a fundamental tool in unsupervised learning of graph representations such as node ranking  labeling  and graph embedding. However  while data privacy is one of the most important recent concerns  existing PPR algorithms are not designed to protect user privacy. PPR is highly sensitive to the input graph edges  the difference of only one edge may cause a big change in the PPR vector  potentially leaking private user data.,1
Learning Mutual Fund Categorization using Natural Language Processing Categorization of mutual funds or Exchange Traded funds  ETFs  have long served the financial analysts to perform peer analysis for various purposes starting from competitor analysis  to quantifying portfolio diversification. The categorization methodology usually relies on fund composition data in the structured format extracted from the Form N  A. Here  we initiate a study to learn the categorization system directly from the unstructured data as depicted in the forms using natural language processing  NLP . Positing as a multi class classification problem with the input data being only the investment strategy description as reported in the form and the target variable being the Lipper Global categories  and using various NLP models  we show that the categorization system can indeed be learned with high accuracy. We discuss implications and applications of our findings as well as limitations of existing pre trained architectures in applying them to learn fund categorization.,1
PRoA  A Probabilistic Robustness Assessment against Functional Perturbations In safety critical deep learning applications robustness measurement is a vital pre deployment phase. However  existing robustness verification methods are not sufficiently practical for deploying machine learning systems in the real world. On the one hand  these methods attempt to claim that no perturbations can   fool   deep neural networks  DNNs   which may be too stringent in practice. On the other hand  existing works rigorously consider  L p  bounded additive perturbations on the pixel space  although perturbations  such as colour shifting and geometric transformations  are more practically and frequently occurring in the real world. Thus  from the practical standpoint  we present a novel and general   it probabilistic robustness assessment method   PRoA  based on the adaptive concentration  and it can measure the robustness of deep learning models against functional perturbations. PRoA can provide statistical guarantees on the probabilistic robustness of a model   textit i.e.   the probability of failure encountered by the trained model after deployment. Our experiments demonstrate the effectiveness and flexibility of PRoA in terms of evaluating the probabilistic robustness against a broad range of functional perturbations  and PRoA can scale well to various large scale deep neural networks compared to existing state of the art baselines. For the purpose of reproducibility  we release our tool on GitHub   url ,1
Latent Variable Models for Bayesian Causal Discovery Learning predictors that do not rely on spurious correlations involves building causal representations. However  learning such a representation is very challenging. We  therefore  formulate the problem of learning a causal representation from high dimensional data and study causal recovery with synthetic data. This work introduces a latent variable decoder model  Decoder BCD  for Bayesian causal discovery and performs experiments in mildly supervised and unsupervised settings. We present a series of synthetic experiments to characterize important factors for causal discovery and show that using known intervention targets as labels helps in unsupervised Bayesian inference over structure and parameters of linear Gaussian additive noise latent structural causal models.,1
Efficient One Sided Kolmogorov Approximation We present an efficient algorithm that  given a discrete random variable  X  and a number  m   computes a random variable whose support is of size at most  m  and whose Kolmogorov distance from  X  is minimal  also for the one sided Kolmogorov approximation. We present some variants of the algorithm  analyse their correctness and computational complexity  and present a detailed empirical evaluation that shows how they performs in practice. The main application that we examine  which is our motivation for this work  is estimation of the probability missing deadlines in series parallel schedules. Since exact computation of these probabilities is NP hard  we propose to use the algorithms described in this paper to obtain an approximation.,1
Functional Generalized Empirical Likelihood Estimation for Conditional Moment Restrictions Important problems in causal inference  economics  and  more generally  robust machine learning can be expressed as conditional moment restrictions  but estimation becomes challenging as it requires solving a continuum of unconditional moment restrictions. Previous works addressed this problem by extending the generalized method of moments  GMM  to continuum moment restrictions. In contrast  generalized empirical likelihood  GEL  provides a more general framework and has been shown to enjoy favorable small sample properties compared to GMM based estimators. To benefit from recent developments in machine learning  we provide a functional reformulation of GEL in which arbitrary models can be leveraged. Motivated by a dual formulation of the resulting infinite dimensional optimization problem  we devise a practical method and explore its asymptotic properties. Finally  we provide kernel  and neural network based implementations of the estimator  which achieve state of the art empirical performance on two conditional moment restriction problems.,1
Efficient Search of Multiple Neural Architectures with Different Complexities via Importance Sampling Neural architecture search  NAS  aims to automate architecture design processes and improve the performance of deep neural networks. Platform aware NAS methods consider both performance and complexity and can find well performing architectures with low computational resources. Although ordinary NAS methods result in tremendous computational costs owing to the repetition of model training  one shot NAS  which trains the weights of a supernetwork containing all candidate architectures only once during the search process  has been reported to result in a lower search cost. This study focuses on the architecture complexity aware one shot NAS that optimizes the objective function composed of the weighted sum of two metrics  such as the predictive performance and number of parameters. In existing methods  the architecture search process must be run multiple times with different coefficients of the weighted sum to obtain multiple architectures with different complexities. This study aims at reducing the search cost associated with finding multiple architectures. The proposed method uses multiple distributions to generate architectures with different complexities and updates each distribution using the samples obtained from multiple distributions based on importance sampling. The proposed method allows us to obtain multiple architectures with different complexities in a single architecture search  resulting in reducing the search cost. The proposed method is applied to the architecture search of convolutional neural networks on the CIAFR    and ImageNet datasets. Consequently  compared with baseline methods  the proposed method finds multiple architectures with varying complexities while requiring less computational effort.,1
Inference of Regulatory Networks Through Temporally Sparse Data A major goal in genomics is to properly capture the complex dynamical behaviors of gene regulatory networks  GRNs . This includes inferring the complex interactions between genes  which can be used for a wide range of genomics analyses  including diagnosis or prognosis of diseases and finding effective treatments for chronic diseases such as cancer. Boolean networks have emerged as a successful class of models for capturing the behavior of GRNs. In most practical settings  inference of GRNs should be achieved through limited and temporally sparse genomics data. A large number of genes in GRNs leads to a large possible topology candidate space  which often cannot be exhaustively searched due to the limitation in computational resources. This paper develops a scalable and efficient topology inference for GRNs using Bayesian optimization and kernel based methods. Rather than an exhaustive search over possible topologies  the proposed method constructs a Gaussian Process  GP  with a topology inspired kernel function to account for correlation in the likelihood function. Then  using the posterior distribution of the GP model  the Bayesian optimization efficiently searches for the topology with the highest likelihood value by optimally balancing between exploration and exploitation. The performance of the proposed method is demonstrated through comprehensive numerical experiments using a well known mammalian cell cycle network.,1
Shrinkage Estimation of Higher Order Bochner Integrals We consider shrinkage estimation of higher order Hilbert space valued Bochner integrals in a non parametric setting. We propose estimators that shrink the  U  statistic estimator of the Bochner integral towards a pre specified target element in the Hilbert space. Depending on the degeneracy of the kernel of the  U  statistic  we construct consistent shrinkage estimators with fast rates of convergence  and develop oracle inequalities comparing the risks of the the  U  statistic estimator and its shrinkage version. Surprisingly  we show that the shrinkage estimator designed by assuming complete degeneracy of the kernel of the  U  statistic is a consistent estimator even when the kernel is not complete degenerate. This work subsumes and improves upon Krikamol et al.        JMLR and Zhou et al.        JMVA  which only handle mean element and covariance operator estimation in a reproducing kernel Hilbert space. We also specialize our results to normal mean estimation and show that for  d ge     the proposed estimator strictly improves upon the sample mean in terms of the mean squared error.,1
Meta Learning a Real Time Tabular AutoML Method For Small Data We present TabPFN  an AutoML method that is competitive with the state of the art on small tabular datasets while being over        times  faster. Our method is very simple  it is fully entailed in the weights of a single neural network  and a single forward pass directly yields predictions for a new dataset. Our AutoML method is meta learned using the Transformer based Prior Data Fitted Network  PFN  architecture and approximates Bayesian inference with a prior that is based on assumptions of simplicity and causal structures. The prior contains a large space of structural causal models and Bayesian neural networks with a bias for small architectures and thus low complexity. Furthermore  we extend the PFN approach to differentiably calibrate the prior s hyperparameters on real data. By doing so  we separate our abstract prior assumptions from their heuristic calibration on real data. Afterwards  the calibrated hyperparameters are fixed and TabPFN can be applied to any new tabular dataset at the push of a button. Finally  on    datasets from the OpenML CC   suite we show that our method outperforms boosted trees and performs on par with complex state of the art AutoML systems with predictions produced in less than a second. We provide all our code and our final trained TabPFN in the supplementary materials.,1
The Mean Dimension of Neural Networks    What causes the interaction effects  Owen and Hoyt recently showed that the effective dimension offers key structural information about the input output mapping underlying an artificial neural network. Along this line of research  this work proposes an estimation procedure that allows the calculation of the mean dimension from a given dataset  without resampling from external distributions. The design yields total indices when features are independent and a variant of total indices when features are correlated. We show that this variant possesses the zero independence property. With synthetic datasets  we analyse how the mean dimension evolves layer by layer and how the activation function impacts the magnitude of interactions. We then use the mean dimension to study some of the most widely employed convolutional architectures for image recognition  LeNet  ResNet  DenseNet . To account for pixel correlations  we propose calculating the mean dimension after the addition of an inverse PCA layer that allows one to work on uncorrelated PCA transformed features  without the need to retrain the neural network. We use the generalized total indices to produce heatmaps for post hoc explanations  and we employ the mean dimension on the PCA transformed features for cross comparisons of the artificial neural networks structures. Results provide several insights on the difference in magnitude of interactions across the architectures  as well as indications on how the mean dimension evolves during training.,1
FACT  High Dimensional Random Forests Inference Random forests is one of the most widely used machine learning methods over the past decade thanks to its outstanding empirical performance. Yet  because of its black box nature  the results by random forests can be hard to interpret in many big data applications. Quantifying the usefulness of individual features in random forests learning can greatly enhance its interpretability. Existing studies have shown that some popularly used feature importance measures for random forests suffer from the bias issue. In addition  there lack comprehensive size and power analyses for most of these existing methods. In this paper  we approach the problem via hypothesis testing  and suggest a framework of the self normalized feature residual correlation test  FACT  for evaluating the significance of a given feature in the random forests model with bias resistance property  where our null hypothesis concerns whether the feature is conditionally independent of the response given all other features. Such an endeavor on random forests inference is empowered by some recent developments on high dimensional random forests consistency. The vanilla version of our FACT test can suffer from the bias issue in the presence of feature dependency. We exploit the techniques of imbalancing and conditioning for bias correction. We further incorporate the ensemble idea into the FACT statistic through feature transformations for the enhanced power. Under a fairly general high dimensional nonparametric model setting with dependent features  we formally establish that FACT can provide theoretically justified random forests feature p values and enjoy appealing power through nonasymptotic analyses. The theoretical results and finite sample advantages of the newly suggested method are illustrated with several simulation examples and an economic forecasting application in relation to COVID   .,1
Twitmo  A Twitter Data Topic Modeling and Visualization Package for R We present Twitmo  a package that provides a broad range of methods to collect  pre process  analyze and visualize geo tagged Twitter data. Twitmo enables the user to collect geo tagged Tweets from Twitter and and provides a comprehensive and user friendly toolbox to generate topic distributions from Latent Dirichlet Allocations  LDA   correlated topic models  CTM  and structural topic models  STM . Functions are included for pre processing of text  model building and prediction. In addition  one of the innovations of the package is the automatic pooling of Tweets into longer pseudo documents using hashtags and cosine similarities for better topic coherence. The package additionally comes with functionality to visualize collected data sets and fitted models in static as well as interactive ways and offers built in support for model visualizations via LDAvis providing great convenience for researchers in this area. The Twitmo package is an innovative toolbox that can be used to analyze public discourse of various topics  political parties or persons of interest in space and time.,1
Recommendation Systems with Distribution Free Reliability Guarantees When building recommendation systems  we seek to output a helpful set of items to the user. Under the hood  a ranking model predicts which of two candidate items is better  and we must distill these pairwise comparisons into the user facing output. However  a learned ranking model is never perfect  so taking its predictions at face value gives no guarantee that the user facing output is reliable. Building from a pre trained ranking model  we show how to return a set of items that is rigorously guaranteed to contain mostly good items. Our procedure endows any ranking model with rigorous finite sample control of the false discovery rate  FDR   regardless of the  unknown  data distribution. Moreover  our calibration algorithm enables the easy and principled integration of multiple objectives in recommender systems. As an example  we show how to optimize for recommendation diversity subject to a user specified level of FDR control  circumventing the need to specify ad hoc weights of a diversity loss against an accuracy loss. Throughout  we focus on the problem of learning to rank a set of possible recommendations  evaluating our methods on the Yahoo  Learning to Rank and MSMarco datasets.,1
General Policy Evaluation and Improvement by Learning to Identify Few But Crucial States Learning to evaluate and improve policies is a core problem of Reinforcement Learning  RL . Traditional RL algorithms learn a value function defined for a single policy. A recently explored competitive alternative is to learn a single value function for many policies. Here we combine the actor critic architecture of Parameter Based Value Functions and the policy embedding of Policy Evaluation Networks to learn a single value function for evaluating  and thus helping to improve  any policy represented by a deep neural network  NN . The method yields competitive experimental results. In continuous control problems with infinitely many states  our value function minimizes its prediction error by simultaneously learning a small set of  probing states  and a mapping from actions produced in probing states to the policy s return. The method extracts crucial abstract knowledge about the environment in form of very few states sufficient to fully specify the behavior of many policies. A policy improves solely by changing actions in probing states  following the gradient of the value function s predictions. Surprisingly  it is possible to clone the behavior of a near optimal policy in Swimmer v  and Hopper v  environments only by knowing how to act in   and   such learned states  respectively. Remarkably  our value function trained to evaluate NN policies is also invariant to changes of the policy architecture  we show that it allows for zero shot learning of linear policies competitive with the best policy seen during training. Our code is public.,1
Finite time High probability Bounds for Polyak Ruppert Averaged Iterates of Linear Stochastic Approximation This paper provides a finite time analysis of linear stochastic approximation  LSA  algorithms with fixed step size  a core method in statistics and machine learning. LSA is used to compute approximate solutions of a  d  dimensional linear system   bar  mathbf A    theta    bar  mathbf b     for which    bar  mathbf A     bar  mathbf b     can only be estimated through  asymptotically  unbiased observations      mathbf A  Z n   mathbf b  Z n      n  in  mathbb N   . We consider here the case where    Z n    n  in  mathbb N    is an i.i.d. sequence or a uniformly geometrically ergodic Markov chain  and derive  p  moments inequality and high probability bounds for the iterates defined by LSA and its Polyak Ruppert averaged version. More precisely  we establish bounds of order   p  alpha t   operatorname mix         d    p   on the  p  th moment of the last iterate of LSA. In this formula   alpha  is the step size of the procedure and  t   operatorname mix    is the mixing time of the underlying chain   t   operatorname mix      in the i.i.d. setting . We then prove finite time instance dependent bounds on the Polyak Ruppert averaged sequence of iterates. These results are sharp in the sense that the leading term we obtain matches the local asymptotic minimax limit  including tight dependence on the parameters   d t   operatorname mix     in the higher order terms.,1
Measuring and signing fairness as performance under multiple stakeholder distributions As learning machines increase their influence on decisions concerning human lives  analyzing their fairness properties becomes a subject of central importance. Yet  our best tools for measuring the fairness of learning systems are rigid fairness metrics encapsulated as mathematical one liners  offer limited power to the stakeholders involved in the prediction task  and are easy to manipulate when we exhort excessive pressure to optimize them. To advance these issues  we propose to shift focus from shaping fairness metrics to curating the distributions of examples under which these are computed. In particular  we posit that every claim about fairness should be immediately followed by the tagline  Fair under what examples  and collected by whom  . By highlighting connections to the literature in domain generalization  we propose to measure fairness as the ability of the system to generalize under multiple stress tests    distributions of examples with social relevance. We encourage each stakeholder to curate one or multiple stress tests containing examples reflecting their  possibly conflicting  interests. The machine passes or fails each stress test by falling short of or exceeding a pre defined metric value. The test results involve all stakeholders in a discussion about how to improve the learning system  and provide flexible assessments of fairness dependent on context and based on interpretable data. We provide full implementation guidelines for stress testing  illustrate both the benefits and shortcomings of this framework  and introduce a cryptographic scheme to enable a degree of prediction accountability from system providers.,1
Is a Caption Worth a Thousand Images  A Controlled Study for Representation Learning The development of CLIP  Radford et al.        has sparked a debate on whether language supervision can result in vision models with more transferable representations than traditional image only methods. Our work studies this question through a carefully controlled comparison of two approaches in terms of their ability to learn representations that generalize to downstream classification tasks. We find that when the pre training dataset meets certain criteria    it is sufficiently large and contains descriptive captions with low variability    image only methods do not match CLIP s transfer performance  even when they are trained with more image data. However  contrary to what one might expect  there are practical settings in which these criteria are not met  wherein added supervision through captions is actually detrimental. Motivated by our findings  we devise simple prescriptions to enable CLIP to better leverage the language information present in existing pre training datasets.,1
Private Convex Optimization in General Norms We propose a new framework for differentially private optimization of convex functions which are Lipschitz in an arbitrary norm   normx  cdot  . Our algorithms are based on a regularized exponential mechanism which samples from the density   propto  exp  k F  mu r    where  F  is the empirical loss and  r  is a regularizer which is strongly convex with respect to   normx  cdot    generalizing a recent work of  cite GLL    to non Euclidean settings. We show that this mechanism satisfies Gaussian differential privacy and solves both DP ERM  empirical risk minimization  and DP SCO  stochastic convex optimization   by using localization tools from convex geometry. Our framework is the first to apply to private convex optimization in general normed spaces  and directly recovers non private SCO rates achieved by mirror descent  as the privacy parameter   eps  to  infty . As applications  for Lipschitz optimization in   ell p  norms for all  p  in          we obtain the first optimal privacy utility tradeoffs  for  p       we improve tradeoffs obtained by the recent works  cite AsiFKT    BassilyGN    by at least a logarithmic factor. Our   ell p  norm and Schatten  p  norm optimization frameworks are complemented with polynomial time samplers whose query complexity we explicitly bound.,1
Homomorphism Autoencoder    Learning Group Structured Representations from Observed Transitions How can we acquire world models that veridically represent the outside world both in terms of what is there and in terms of how our actions affect it  Can we acquire such models by interacting with the world  and can we state mathematical desiderata for their relationship with a hypothetical reality existing outside our heads  As machine learning is moving towards representations containing not just observational but also interventional knowledge  we study these problems using tools from representation learning and group theory. Under the assumption that our actuators act upon the world  we propose methods to learn internal representations of not just sensory information but also of actions that modify our sensory representations in a way that is consistent with the actions and transitions in the world. We use an autoencoder equipped with a group representation linearly acting on its latent space  trained on   step reconstruction such as to enforce a suitable homomorphism property on the group representation. Compared to existing work  our approach makes fewer assumptions on the group representation and on which transformations the agent can sample from the group. We motivate our method theoretically  and demonstrate empirically that it can learn the correct representation of the groups and the topology of the environment. We also compare its performance in trajectory prediction with previous methods.,1
The d separation criterion in Categorical Probability The d separation criterion detects the compatibility of a joint probability distribution with a directed acyclic graph through certain conditional independences. In this work  we study this problem in the context of categorical probability theory by introducing a categorical definition of causal models  a categorical notion of d separation  and proving an abstract version of the d separation criterion. This approach has two main benefits. First  categorical d separation is a very intuitive criterion based on topological connectedness. Second  our results apply in measure theoretic probability  with standard Borel spaces   and therefore provide a clean proof of the equivalence of local and global Markov properties with causal compatibility for continuous and mixed variables.,1
Collaborative Uncertainty Benefits Multi Agent Multi Modal Trajectory Forecasting In multi modal multi agent trajectory forecasting  two major challenges have not been fully tackled     how to measure the uncertainty brought by the interaction module that causes correlations among the predicted trajectories of multiple agents     how to rank the multiple predictions and select the optimal predicted trajectory. In order to handle these challenges  this work first proposes a novel concept  collaborative uncertainty  CU   which models the uncertainty resulting from interaction modules. Then we build a general CU aware regression framework with an original permutation equivariant uncertainty estimator to do both tasks of regression and uncertainty estimation. Further  we apply the proposed framework to current SOTA multi agent multi modal forecasting systems as a plugin module  which enables the SOTA systems to    estimate the uncertainty in the multi agent multi modal trajectory forecasting task     rank the multiple predictions and select the optimal one based on the estimated uncertainty. We conduct extensive experiments on a synthetic dataset and two public large scale multi agent trajectory forecasting benchmarks. Experiments show that     on the synthetic dataset  the CU aware regression framework allows the model to appropriately approximate the ground truth Laplace distribution     on the multi agent trajectory forecasting benchmarks  the CU aware regression framework steadily helps SOTA systems improve their performances. Specially  the proposed framework helps VectorNet improve by     cm regarding the Final Displacement Error of the chosen optimal prediction on the nuScenes dataset     for multi agent multi modal trajectory forecasting systems  prediction uncertainty is positively correlated with future stochasticity  and    the estimated CU values are highly related to the interactive information among agents.,1
Scalable Bayesian Inference for Detection and Deblending in Astronomical Images We present a new probabilistic method for detecting  deblending  and cataloging astronomical sources called the Bayesian Light Source Separator  BLISS . BLISS is based on deep generative models  which embed neural networks within a Bayesian model. For posterior inference  BLISS uses a new form of variational inference known as Forward Amortized Variational Inference. The BLISS inference routine is fast  requiring a single forward pass of the encoder networks on a GPU once the encoder networks are trained. BLISS can perform fully Bayesian inference on megapixel images in seconds  and produces highly accurate catalogs. BLISS is highly extensible  and has the potential to directly answer downstream scientific questions in addition to producing probabilistic catalogs.,1
Orthogonalization of data via Gromov Wasserstein type feedback for clustering and visualization In this paper we propose an adaptive approach for clustering and visualization of data by an orthogonalization process. Starting with the data points being represented by a Markov process using the diffusion map framework  the method adaptively increase the orthogonality of the clusters by applying a feedback mechanism inspired by the Gromov Wasserstein distance. This mechanism iteratively increases the spectral gap and refines the orthogonality of the data to achieve a clustering with high specificity. By using the diffusion map framework and representing the relation between data points using transition probabilities  the method is robust with respect to both the underlying distance  noise in the data and random initialization. We prove that the method converges globally to a unique fixpoint for certain parameter values. We also propose a related approach where the transition probabilities in the Markov process are required to be doubly stochastic  in which case the method generates a minimizer to a nonconvex optimization problem. We apply the method on cryo electron microscopy image data from biopharmaceutical manufacturing where we can confirm biologically relevant insights related to therapeutic efficacy. We consider an example with morphological variations of gene packaging and confirm that the method produces biologically meaningful clustering results consistent with human expert classification.,1
Continuous time Analysis for Variational Inequalities  An Overview and Desiderata Algorithms that solve zero sum games  multi objective agent objectives  or  more generally  variational inequality  VI  problems are notoriously unstable on general problems. Owing to the increasing need for solving such problems in machine learning  this instability has been highlighted in recent years as a significant research challenge. In this paper  we provide an overview of recent progress in the use of continuous time perspectives in the analysis and design of methods targeting the broad VI problem class. Our presentation draws parallels between single objective problems and multi objective problems  highlighting the challenges of the latter. We also formulate various desiderata for algorithms that apply to general VIs and we argue that achieving these desiderata may profit from an understanding of the associated continuous time dynamics.,1
Wasserstein multivariate auto regressive models for modeling distributional time series and its application in graph learning We propose a new auto regressive model for the statistical analysis of multivariate distributional time series. The data of interest consist of a collection of multiple series of probability measures supported over a bounded interval of the real line  and that are indexed by distinct time instants. The probability measures are modelled as random objects in the Wasserstein space. We establish the auto regressive model in the tangent space at the Lebesgue measure by first centering all the raw measures so that their Fr chet means turn to be the Lebesgue measure. Using the theory of iterated random function systems  results on the existence  uniqueness and stationarity of the solution of such a model are provided. We also propose a consistent estimator for the model coefficient. In addition to the analysis of simulated data  the proposed model is illustrated with two real data sets made of observations from age distribution in different countries and bike sharing network in Paris. Finally  due to the positive and boundedness constraints that we impose on the model coefficients  the proposed estimator that is learned under these constraints  naturally has a sparse structure. The sparsity allows furthermore the application of the proposed model in learning a graph of temporal dependency from the multivariate distributional time series.,1
VTrackIt  A Synthetic Self Driving Dataset with Infrastructure and Pooled Vehicle Information Artificial intelligence solutions for Autonomous Vehicles  AVs  have been developed using publicly available datasets such as Argoverse  ApolloScape  Level   and NuScenes. One major limitation of these datasets is the absence of infrastructure and or pooled vehicle information like lane line type  vehicle speed  traffic signs  and intersections. Such information is necessary and not complementary to eliminating high risk edge cases. The rapid advancements in Vehicle to Infrastructure and Vehicle to Vehicle technologies show promise that infrastructure and pooled vehicle information will soon be accessible in near real time. Taking a leap in the future  we introduce the first comprehensive synthetic dataset with intelligent infrastructure and pooled vehicle information for advancing the next generation of AVs  named VTrackIt. We also introduce the first deep learning model  InfraGAN  for trajectory predictions that considers such information. Our experiments with InfraGAN show that the comprehensive information offered by VTrackIt reduces the number of high risk edge cases. The VTrackIt dataset is available upon request under the Creative Commons CC BY NC SA  .  license at http   vtrackit.irda.club.,1
