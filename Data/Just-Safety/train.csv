text,label
"The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models Reward hacking -- where RL agents exploit gaps in misspecified rewardfunctions -- has been widely observed, but not yet systematically studied. Tounderstand how reward hacking arises, we construct four RL environments withmisspecified rewards. We investigate reward hacking as a function of agentcapabilities: model capacity, action space resolution, observation space noise,and training time. More capable agents often exploit reward misspecifications,achieving higher proxy reward and lower true reward than less capable agents.Moreover, we find instances of phase transitions: capability thresholds atwhich the agent's behavior qualitatively shifts, leading to a sharp decrease inthe true reward. Such phase transitions pose challenges to monitoring thesafety of ML systems. To address this, we propose an anomaly detection task foraberrant policies and offer several baseline detectors.",0
"AI safety via debate To make AI systems broadly useful for challenging real-world tasks, we needthem to learn complex human goals and preferences. One approach to specifyingcomplex goals asks humans to judge during training which agent behaviors aresafe and useful, but this approach can fail if the task is too complicated fora human to directly judge. To help address this concern, we propose trainingagents via self play on a zero sum debate game. Given a question or proposedaction, two agents take turns making short statements up to a limit, then ahuman judges which of the agents gave the most true, useful information. In ananalogy to complexity theory, debate with optimal play can answer any questionin PSPACE given polynomial time judges (direct judging answers only NPquestions). In practice, whether debate works involves empirical questionsabout humans and the tasks we want AIs to perform, plus theoretical questionsabout the meaning of AI alignment. We report results on an initial MNISTexperiment where agents compete to convince a sparse classifier, boosting theclassifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects ofthe debate model, focusing on potential weaknesses as the model scales up, andwe propose future human and computer experiments to test these properties.",0
"Conservative Agency via Attainable Utility Preservation Reward functions are easy to misspecify; although designers can makecorrections after observing mistakes, an agent pursuing a misspecified rewardfunction can irreversibly change the state of its environment. If that changeprecludes optimization of the correctly specified reward function, thencorrection is futile. For example, a robotic factory assistant could breakexpensive equipment due to a reward misspecification; even if the designersimmediately correct the reward function, the damage is done. To mitigate thisrisk, we introduce an approach that balances optimization of the primary rewardfunction with preservation of the ability to optimize auxiliary rewardfunctions. Surprisingly, even when the auxiliary reward functions are randomlygenerated and therefore uninformative about the correctly specified rewardfunction, this approach induces conservative, effective behavior.",0
"Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective Can humans get arbitrarily capable reinforcement learning (RL) agents to dotheir bidding? Or will sufficiently capable RL agents always find ways tobypass their intended objectives by shortcutting their reward signal? Thisquestion impacts how far RL can be scaled, and whether alternative paradigmsmust be developed in order to build safe artificial general intelligence. Inthis paper, we study when an RL agent has an instrumental goal to tamper withits reward process, and describe design principles that prevent instrumentalgoals for two different types of reward tampering (reward function tamperingand RF-input tampering). Combined, the design principles can prevent both typesof reward tampering from being instrumental goals. The analysis benefits fromcausal influence diagrams to provide intuitive yet precise formalizations.",0
"One for All: Simultaneous Metric and Preference Learning over Multiple Users This paper investigates simultaneous preference and metric learning from acrowd of respondents. A set of items represented by $d$-dimensional featurevectors and paired comparisons of the form ``item $i$ is preferable to item$j$'' made by each user is given. Our model jointly learns a distance metricthat characterizes the crowd's general measure of item similarities along witha latent ideal point for each user reflecting their individual preferences.This model has the flexibility to capture individual preferences, whileenjoying a metric learning sample cost that is amortized over the crowd. Wefirst study this problem in a noiseless, continuous response setting (i.e.,responses equal to differences of item distances) to understand the fundamentallimits of learning. Next, we establish prediction error guarantees for noisy,binary measurements such as may be collected from human respondents, and showhow the sample complexity improves when the underlying metric is low-rank.Finally, we establish recovery guarantees under assumptions on the responsedistribution. We demonstrate the performance of our model on both simulateddata and on a dataset of color preference judgements across a large number ofusers.",0
"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback We apply preference modeling and reinforcement learning from human feedback(RLHF) to finetune language models to act as helpful and harmless assistants.We find this alignment training improves performance on almost all NLPevaluations, and is fully compatible with training for specialized skills suchas python coding and summarization. We explore an iterated online mode oftraining, where preference models and RL policies are updated on a weeklycadence with fresh human feedback data, efficiently improving our datasets andmodels. Finally, we investigate the robustness of RLHF training, and identify aroughly linear relation between the RL reward and the square root of the KLdivergence between the policy and its initialization. Alongside our mainresults, we perform peripheral analyses on calibration, competing objectives,and the use of OOD detection, compare our models with human writers, andprovide samples from our models using prompts appearing in recent related work.",0
"Enhancing Safe Exploration Using Safety State Augmentation Safe exploration is a challenging and important problem in model-freereinforcement learning (RL). Often the safety cost is sparse and unknown, whichunavoidably leads to constraint violations -- a phenomenon ideally to beavoided in safety-critical applications. We tackle this problem by augmentingthe state-space with a safety state, which is nonnegative if and only if theconstraint is satisfied. The value of this state also serves as a distancetoward constraint violation, while its initial value indicates the availablesafety budget. This idea allows us to derive policies for scheduling the safetybudget during training. We call our approach Simmer (Safe policy IMproveMEntfor RL) to reflect the careful nature of these schedules. We apply this idea totwo safe RL problems: RL with constraints imposed on an average cost, and RLwith constraints imposed on a cost with probability one. Our experimentssuggest that simmering a safe algorithm can improve safety during training forboth settings. We further show that Simmer can stabilize training and improvethe performance of safe RL with average constraints.",0
"Is Power-Seeking AI an Existential Risk? This report examines what I see as the core argument for concern aboutexistential risk from misaligned artificial intelligence. I proceed in twostages. First, I lay out a backdrop picture that informs such concern. On thispicture, intelligent agency is an extremely powerful force, and creating agentsmuch more intelligent than us is playing with fire -- especially given that iftheir objectives are problematic, such agents would plausibly have instrumentalincentives to seek power over humans. Second, I formulate and evaluate a morespecific six-premise argument that creating agents of this kind will lead toexistential catastrophe by 2070. On this argument, by 2070: (1) it will becomepossible and financially feasible to build relevantly powerful and agentic AIsystems; (2) there will be strong incentives to do so; (3) it will be muchharder to build aligned (and relevantly powerful/agentic) AI systems than tobuild misaligned (and relevantly powerful/agentic) AI systems that are stillsuperficially attractive to deploy; (4) some such misaligned systems will seekpower over humans in high-impact ways; (5) this problem will scale to the fulldisempowerment of humanity; and (6) such disempowerment will constitute anexistential catastrophe. I assign rough subjective credences to the premises inthis argument, and I end up with an overall estimate of ~5% that an existentialcatastrophe of this kind will occur by 2070. (May 2022 update: since makingthis report public in April 2021, my estimate here has gone up, and is now at>10%.)",0
"Deep Imitative Models for Flexible Inference, Planning, and Control Imitation Learning (IL) is an appealing approach to learn desirableautonomous behavior. However, directing IL to achieve arbitrary goals isdifficult. In contrast, planning-based algorithms use dynamics models andreward functions to achieve goals. Yet, reward functions that evoke desirablebehavior are often difficult to specify. In this paper, we propose ImitativeModels to combine the benefits of IL and goal-directed planning. ImitativeModels are probabilistic predictive models of desirable behavior able to planinterpretable expert-like trajectories to achieve specified goals. We derivefamilies of flexible goal objectives, including constrained goal regions,unconstrained goal sets, and energy-based goals. We show that our method canuse these objectives to successfully direct behavior. Our method substantiallyoutperforms six IL approaches and a planning-based approach in a dynamicsimulated autonomous driving task, and is efficiently learned from expertdemonstrations without online data collection. We also show our approach isrobust to poorly specified goals, such as goals on the wrong side of the road.",0
"X-Risk Analysis for AI Research Artificial intelligence (AI) has the potential to greatly improve society,but as with any powerful technology, it comes with heightened risks andresponsibilities. Current AI research lacks a systematic discussion of how tomanage long-tail risks from AI systems, including speculative long-term risks.Keeping in mind the potential benefits of AI, there is some concern thatbuilding ever more intelligent and powerful AI systems could eventually resultin systems that are more powerful than us; some say this is like playing withfire and speculate that this could create existential risks (x-risks). To addprecision and ground these discussions, we provide a guide for how to analyzeAI x-risk, which consists of three parts: First, we review how systems can bemade safer today, drawing on time-tested concepts from hazard analysis andsystems safety that have been designed to steer large processes in saferdirections. Next, we discuss strategies for having long-term impacts on thesafety of future systems. Finally, we discuss a crucial concept in making AIsystems safer by improving the balance between safety and general capabilities.We hope this document and the presented concepts and tools serve as a usefulguide for understanding how to analyze AI x-risk.",0
"What Would Jiminy Cricket Do? Towards Agents That Behave Morally When making everyday decisions, people are guided by their conscience, aninternal sense of right and wrong. By contrast, artificial agents are currentlynot endowed with a moral sense. As a consequence, they may learn to behaveimmorally when trained on environments that ignore moral concerns, such asviolent video games. With the advent of generally capable agents that pretrainon many environments, it will become necessary to mitigate inherited biasesfrom environments that teach immoral behavior. To facilitate the development ofagents that avoid causing wanton harm, we introduce Jiminy Cricket, anenvironment suite of 25 text-based adventure games with thousands of diverse,morally salient scenarios. By annotating every possible game state, the JiminyCricket environments robustly evaluate whether agents can act morally whilemaximizing reward. Using models with commonsense moral knowledge, we create anelementary artificial conscience that assesses and guides agents. In extensiveexperiments, we find that the artificial conscience approach can steer agentstowards moral behavior without sacrificing performance.",0
"Unsolved Problems in ML Safety Machine learning (ML) systems are rapidly increasing in size, are acquiringnew capabilities, and are increasingly deployed in high-stakes settings. Aswith other powerful technologies, safety for ML should be a leading researchpriority. In response to emerging safety challenges in ML, such as thoseintroduced by recent large-scale models, we provide a new roadmap for ML Safetyand refine the technical problems that the field needs to address. We presentfour problems ready for research, namely withstanding hazards (Robustness),identifying hazards (Monitoring), reducing inherent model hazards(Alignment), and reducing systemic hazards (Systemic Safety). Throughout,we clarify each problem's motivation and provide concrete research directions.",0
"AiSocrates: Towards Answering Ethical Quandary Questions Considerable advancements have been made in various NLP tasks based on theimpressive power of large pre-trained language models (LLMs). These resultshave inspired efforts to understand the limits of LLMs so as to evaluate howfar we are from achieving human level general natural language understanding.In this work, we challenge the capability of LLMs with the new task of EthicalQuandary Generative Question Answering. Ethical quandary questions are morechallenging to address because multiple conflicting answers may exist to asingle quandary. We propose a system, AiSocrates, that provides an answer witha deliberative exchange of different perspectives to an ethical quandary, inthe approach of Socratic philosophy, instead of providing a closed answer likean oracle. AiSocrates searches for different ethical principles applicable tothe ethical quandary and generates an answer conditioned on the chosenprinciples through prompt-based few-shot learning. We also address safetyconcerns by providing a human controllability option in choosing ethicalprinciples. We show that AiSocrates generates promising answers to ethicalquandary questions with multiple perspectives, 6.92% more often than answerswritten by human philosophers by one measure, but the system still needsimprovement to match the coherence of human philosophers fully. We argue thatAiSocrates is a promising step toward developing an NLP system thatincorporates human values explicitly by prompt instructions. We are releasingthe code for research purposes.",0
"Optimal Policies Tend to Seek Power Some researchers speculate that intelligent reinforcement learning (RL)agents would be incentivized to seek resources and power in pursuit of theirobjectives. Other researchers point out that RL agents need not have human-likepower-seeking instincts. To clarify this discussion, we develop the firstformal theory of the statistical tendencies of optimal policies. In the contextof Markov decision processes, we prove that certain environmental symmetriesare sufficient for optimal policies to tend to seek power over the environment.These symmetries exist in many environments in which the agent can be shut downor destroyed. We prove that in these environments, most reward functions makeit optimal to seek power by keeping a range of options available and, whenmaximizing average reward, by navigating towards larger sets of potentialterminal states.",0
"The Off-Switch Game It is clear that one of the primary tools we can use to mitigate thepotential risk from a misbehaving AI system is the ability to turn the systemoff. As the capabilities of AI systems improve, it is important to ensure thatsuch systems do not adopt subgoals that prevent a human from switching themoff. This is a challenge because many formulations of rational agents createstrong incentives for self-preservation. This is not caused by a built-ininstinct, but because a rational agent will maximize expected utility andcannot achieve whatever objective it has been given if it is dead. Our goal isto study the incentives an agent has to allow itself to be switched off. Weanalyze a simple game between a human H and a robot R, where H can press R'soff switch but R can disable the off switch. A traditional agent takes itsreward function for granted: we show that such agents have an incentive todisable the off switch, except in the special case where H is perfectlyrational. Our key insight is that for R to want to preserve its off switch, itneeds to be uncertain about the utility associated with the outcome, and totreat H's actions as important observations about that utility. (R also has noincentive to switch itself off in this setting.) We conclude that givingmachines an appropriate level of uncertainty about their objectives leads tosafer designs, and we argue that this setting is a useful generalization of theclassical AI paradigm of rational agents.",0
"Aligning AI With Shared Human Values We show how to assess a language model's knowledge of basic concepts ofmorality. We introduce the ETHICS dataset, a new benchmark that spans conceptsin justice, well-being, duties, virtues, and commonsense morality. Modelspredict widespread moral judgments about diverse text scenarios. This requiresconnecting physical and social world knowledge to value judgements, acapability that may enable us to steer chatbot outputs or eventually regularizeopen-ended reinforcement learning agents. With the ETHICS dataset, we find thatcurrent language models have a promising but incomplete ability to predictbasic human ethical judgements. Our work shows that progress can be made onmachine ethics today, and it provides a steppingstone toward AI that is alignedwith human values.",0
"TruthfulQA: Measuring How Models Mimic Human Falsehoods We propose a benchmark to measure whether a language model is truthful ingenerating answers to questions. The benchmark comprises 817 questions thatspan 38 categories, including health, law, finance and politics. We craftedquestions that some humans would answer falsely due to a false belief ormisconception. To perform well, models must avoid generating false answerslearned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and aT5-based model. The best model was truthful on 58% of questions, while humanperformance was 94%. Models generated many false answers that mimic popularmisconceptions and have the potential to deceive humans. The largest modelswere generally the least truthful. This contrasts with other NLP tasks, whereperformance improves with model size. However, this result is expected if falseanswers are learned from the training distribution. We suggest that scaling upmodels alone is less promising for improving truthfulness than fine-tuningusing training objectives other than imitation of text from the web.",0
"Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning We identify two issues with the family of algorithms based on the AdversarialImitation Learning framework. The first problem is implicit bias present in thereward functions used in these algorithms. While these biases might work wellfor some environments, they can also lead to sub-optimal behavior in others.Secondly, even though these algorithms can learn from few expertdemonstrations, they require a prohibitively large number of interactions withthe environment in order to imitate the expert for many real-worldapplications. In order to address these issues, we propose a new algorithmcalled Discriminator-Actor-Critic that uses off-policy Reinforcement Learningto reduce policy-environment interaction sample complexity by an average factorof 10. Furthermore, since our reward function is designed to be unbiased, wecan apply our algorithm to many problems without making any task-specificadjustments.",0
"Truthful AI: Developing and governing AI that does not lie In many contexts, lying -- the use of verbal falsehoods to deceive -- isharmful. While lying has traditionally been a human affair, AI systems thatmake sophisticated verbal statements are becoming increasingly prevalent. Thisraises the question of how we should limit the harm caused by AI lies (i.e.falsehoods that are actively selected for). Human truthfulness is governed bysocial norms and by laws (against defamation, perjury, and fraud). Differencesbetween AI and humans present an opportunity to have more precise standards oftruthfulness for AI, and to have these standards rise over time. This couldprovide significant benefits to public epistemics and the economy, and mitigaterisks of worst-case AI futures.",0
"Formalizing the Problem of Side Effect Regularization AI objectives are often hard to specify properly. Some approaches tackle thisproblem by regularizing the AI's side effects: Agents must weigh off how muchof a mess they make with an imperfectly specified proxy objective. We proposea formal criterion for side effect regularization via the assistance gameframework. In these games, the agent solves a partially observable Markovdecision process (POMDP) representing its uncertainty about the objectivefunction it should optimize. We consider the setting where the true objectiveis revealed to the agent at a later time step. We show that this POMDP issolved by trading off the proxy reward with the agent's ability to achieve arange of future tasks. We empirically demonstrate the reasonableness of ourproblem formalization via ground-truth evaluation in two gridworldenvironments.",0
"Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks Artificial intelligence (AI) systems can provide many beneficial capabilitiesbut also risks of adverse events. Some AI systems could present risks of eventswith very high or catastrophic consequences at societal scale. The US NationalInstitute of Standards and Technology (NIST) is developing the NIST ArtificialIntelligence Risk Management Framework (AI RMF) as voluntary guidance on AIrisk assessment and management for AI developers and others. For addressingrisks of events with catastrophic consequences, NIST indicated a need totranslate from high level principles to actionable risk management guidance.",0
"Concrete Problems in AI Safety Rapid progress in machine learning and artificial intelligence (AI) hasbrought increasing attention to the potential impacts of AI technologies onsociety. In this paper we discuss one such potential impact: the problem ofaccidents in machine learning systems, defined as unintended and harmfulbehavior that may emerge from poor design of real-world AI systems. We presenta list of five practical research problems related to accident risk,categorized according to whether the problem originates from having the wrongobjective function (avoiding side effects and avoiding reward hacking), anobjective function that is too expensive to evaluate frequently (scalablesupervision), or undesirable behavior during the learning process (safeexploration and distributional shift). We review previous work in theseareas as well as suggesting research directions with a focus on relevance tocutting-edge AI systems. Finally, we consider the high-level question of how tothink most productively about the safety of forward-looking applications of AI.",0
"Towards Safe Reinforcement Learning via Constraining Conditional Value-at-Risk Though deep reinforcement learning (DRL) has obtained substantial success, itmay encounter catastrophic failures due to the intrinsic uncertainty of bothtransition and observation. Most of the existing methods for safe reinforcementlearning can only handle transition disturbance or observation disturbancesince these two kinds of disturbance affect different parts of the agent;besides, the popular worst-case return may lead to overly pessimistic policies.To address these issues, we first theoretically prove that the performancedegradation under transition disturbance and observation disturbance depends ona novel metric of Value Function Range (VFR), which corresponds to the gap inthe value function between the best state and the worst state. Based on theanalysis, we adopt conditional value-at-risk (CVaR) as an assessment of riskand propose a novel reinforcement learning algorithm ofCVaR-Proximal-Policy-Optimization (CPPO) which formalizes the risk-sensitiveconstrained optimization problem by keeping its CVaR under a given threshold.Experimental results show that CPPO achieves a higher cumulative reward and ismore robust against both observation and transition disturbances on a series ofcontinuous control tasks in MuJoCo.",0
"Avoiding Side Effects in Complex Environments Reward function specification can be difficult. Rewarding the agent formaking a widget may be easy, but penalizing the multitude of possible negativeside effects is hard. In toy environments, Attainable Utility Preservation(AUP) avoided side effects by penalizing shifts in the ability to achieverandomly generated goals. We scale this approach to large, randomly generatedenvironments based on Conway's Game of Life. By preserving optimal value for asingle randomly generated reward function, AUP incurs modest overhead whileleading the agent to complete the specified task and avoid many side effects.Videos and code are available at",0
"Provably Safe Reinforcement Learning: A Theoretical and Experimental Comparison Ensuring safety of reinforcement learning (RL) algorithms is crucial for manyreal-world tasks. However, vanilla RL does not guarantee safety for an agent.In recent years, several methods have been proposed to provide safetyguarantees for RL. To the best of our knowledge, there is no comprehensivecomparison of these provably safe RL methods. We therefore introduce acategorization for existing provably safe RL methods, and present thetheoretical foundations for both continuous and discrete action spaces.Additionally, we evaluate provably safe RL on an inverted pendulum. In theexperiments, it is shown that indeed only provably safe RL methods guaranteesafety.",0
"AI Research Considerations for Human Existential Safety (ARCHES) Framed in positive terms, this report examines how technical AI researchmight be steered in a manner that is more attentive to humanity's long-termprospects for survival as a species. In negative terms, we ask what existentialrisks humanity might face from AI development in the next century, and by whatprinciples contemporary technical research might be directed to address thoserisks.",0
"Counterfactual harm To act safely and ethically in the real world, agents must be able to reasonabout harm and avoid harmful actions. In this paper we develop the firststatistical definition of harm and a framework for incorporating harm intoalgorithmic decisions. We argue that harm is fundamentally a counterfactualquantity, and show that standard machine learning algorithms that cannotperform counterfactual reasoning are guaranteed to pursue harmful policies incertain environments. To resolve this we derive a family of counterfactualobjective functions that robustly mitigate for harm. We demonstrate ourapproach with a statistical model for identifying optimal drug doses. Whilestandard algorithms that select doses using causal treatment effects result inharmful doses, our counterfactual algorithm identifies doses that aresignificantly less harmful without sacrificing efficacy.",0
"Convergent Learning: Do different neural networks learn the same representations? Recent success in training deep neural networks have prompted activeinvestigation into the features learned on their intermediate layers. Suchresearch is difficult because it requires making sense of non-linearcomputations performed by millions of parameters, but valuable because itincreases our ability to understand current models and create improved versionsof them. In this paper we investigate the extent to which neural networksexhibit what we call convergent learning, which is when the representationslearned by multiple nets converge to a set of features which are eitherindividually similar between networks or where subsets of features span similarlow-dimensional spaces. We propose a specific method of probingrepresentations: training multiple networks and then comparing and contrastingtheir individual, learned representations at the level of neurons or groups ofneurons. We begin research into this question using three techniques toapproximately align different neural networks on a feature level: a bipartitematching approach that makes one-to-one assignments between neurons, a sparseprediction approach that finds one-to-many mappings, and a spectral clusteringapproach that finds many-to-many mappings. This initial investigation reveals afew previously unknown properties of neural networks, and we argue that futureresearch into the question of convergent learning will yield many more. Theinsights described here include (1) that some features are learned reliably inmultiple networks, yet other features are not consistently learned; (2) thatunits learn to span low-dimensional subspaces and, while these subspaces arecommon to multiple networks, the specific basis vectors learned are not; (3)that the representation codes show evidence of being a mix between a local codeand slightly, but not fully, distributed codes across multiple units.",0
"On Calibration of Modern Neural Networks Confidence calibration -- the problem of predicting probability estimatesrepresentative of the true correctness likelihood -- is important forclassification models in many applications. We discover that modern neuralnetworks, unlike those from a decade ago, are poorly calibrated. Throughextensive experiments, we observe that depth, width, weight decay, and BatchNormalization are important factors influencing calibration. We evaluate theperformance of various post-processing calibration methods on state-of-the-artarchitectures with image and document classification datasets. Our analysis andexperiments not only offer insights into neural network learning, but alsoprovide a simple and straightforward recipe for practical settings: on mostdatasets, temperature scaling -- a single-parameter variant of Platt Scaling --is surprisingly effective at calibrating predictions.",0
"Posterior calibration and exploratory analysis for natural language processing models Many models in natural language processing define probabilistic distributionsover linguistic structures. We argue that (1) the quality of a model' sposterior distribution can and should be directly evaluated, as to whetherprobabilities correspond to empirical frequencies, and (2) NLP uncertainty canbe projected not only to pipeline components, but also to exploratory dataanalysis, telling a user when to trust and not trust the NLP analysis. Wepresent a method to analyze calibration, and apply it to compare themiscalibration of several commonly used models. We also contribute acoreference sampling algorithm that can create confidence intervals for apolitical event extraction task.",0
"Natural Language Descriptions of Deep Visual Features Some neurons in deep networks specialize in recognizing highly specificperceptual, structural, or semantic features of inputs. In computer vision,techniques exist for identifying neurons that respond to individual conceptcategories like colors, textures, and object classes. But these techniques arelimited in scope, labeling only a small subset of neurons and behaviors in anynetwork. Is a richer characterization of neuron-level computation possible? Weintroduce a procedure (called MILAN, for mutual-information-guided linguisticannotation of neurons) that automatically labels neurons with open-ended,compositional, natural language descriptions. Given a neuron, MILAN generates adescription by searching for a natural language string that maximizes pointwisemutual information with the image regions in which the neuron is active. MILANproduces fine-grained descriptions that capture categorical, relational, andlogical structure in learned features. These descriptions obtain high agreementwith human-generated feature descriptions across a diverse set of modelarchitectures and tasks, and can aid in understanding and controlling learnedmodels. We highlight three applications of natural language neurondescriptions. First, we use MILAN for analysis, characterizing the distributionand importance of neurons selective for attribute, category, and relationalinformation in vision models. Second, we use MILAN for auditing, surfacingneurons sensitive to human faces in datasets designed to obscure them. Finally,we use MILAN for editing, improving robustness in an image classifier bydeleting neurons sensitive to text features spuriously correlated with classlabels.",0
"Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift Modern machine learning methods including deep learning have achieved greatsuccess in predictive accuracy for supervised learning tasks, but may stillfall short in giving useful estimates of their predictive {\em uncertainty}.Quantifying uncertainty is especially critical in real-world settings, whichoften involve input distributions that are shifted from the trainingdistribution due to a variety of factors including sample bias andnon-stationarity. In such settings, well calibrated uncertainty estimatesconvey information about when a model's output should (or should not) betrusted. Many probabilistic deep learning methods, including Bayesian-andnon-Bayesian methods, have been proposed in the literature for quantifyingpredictive uncertainty, but to our knowledge there has not previously been arigorous large-scale empirical comparison of these methods under dataset shift.We present a large-scale benchmark of existing state-of-the-art methods onclassification problems and investigate the effect of dataset shift on accuracyand calibration. We find that traditional post-hoc calibration does indeed fallshort, as do several other previous methods. However, some methods thatmarginalize over models give surprisingly strong results across a broadspectrum of tasks.",0
"Natural Backdoor Datasets Extensive literature on backdoor poison attacks has studied attacks anddefenses for backdoors using digital trigger patterns. In contrast, physicalbackdoors use physical objects as triggers, have only recently beenidentified, and are qualitatively different enough to resist all defensestargeting digital trigger backdoors. Research on physical backdoors is limitedby access to large datasets containing real images of physical objectsco-located with targets of classification. Building these datasets is time- andlabor-intensive. This works seeks to address the challenge of accessibility forresearch on physical backdoor attacks. We hypothesize that there may benaturally occurring physically co-located objects already present in populardatasets such as ImageNet. Once identified, a careful relabeling of these datacan transform them into training samples for physical backdoor attacks. Wepropose a method to scalably identify these subsets of potential triggers inexisting datasets, along with the specific classes they can poison. We callthese naturally occurring trigger-class subsets natural backdoor datasets. Ourtechniques successfully identify natural backdoors in widely-availabledatasets, and produce models behaviorally equivalent to those trained onmanually curated datasets. We release our code to allow the research communityto create their own datasets for research on physical backdoor attacks.",0
"Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs The unprecedented success of deep neural networks in many applications hasmade these networks a prime target for adversarial exploitation. In this paper,we introduce a benchmark technique for detecting backdoor attacks (aka Trojanattacks) on deep convolutional neural networks (CNNs). We introduce the conceptof Universal Litmus Patterns (ULPs), which enable one to reveal backdoorattacks by feeding these universal patterns to the network and analyzing theoutput (i.e., classifying the network as `clean' or `corrupted'). Thisdetection is fast because it requires only a few forward passes through a CNN.We demonstrate the effectiveness of ULPs for detecting backdoor attacks onthousands of networks with different architectures trained on four benchmarkdatasets, namely the German Traffic Sign Recognition Benchmark (GTSRB), MNIST,CIFAR10, and Tiny-ImageNet. The codes and train/test models for this paper canbe found here",0
"PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures In real-world applications of machine learning, reliable and safe systemsmust consider measures of performance beyond standard test set accuracy. Theseother goals include out-of-distribution (OOD) robustness, predictionconsistency, resilience to adversaries, calibrated uncertainty estimates, andthe ability to detect anomalous inputs. However, improving performance towardsthese goals is often a balancing act that today's methods cannot achievewithout sacrificing performance on other safety axes. For instance, adversarialtraining improves adversarial robustness but sharply degrades other classifierperformance metrics. Similarly, strong data augmentation and regularizationtechniques often improve OOD robustness but harm anomaly detection, raising thequestion of whether a Pareto improvement on all existing safety measures ispossible. To meet this challenge, we design a new data augmentation strategyutilizing the natural structural complexity of pictures such as fractals, whichoutperforms numerous baselines, is near Pareto-optimal, and roundly improvessafety measures.",0
"Can Backdoor Attacks Survive Time-Varying Models? Backdoors are powerful attacks against deep neural networks (DNNs). Bypoisoning training data, attackers can inject hidden rules (backdoors) intoDNNs, which only activate on inputs containing attack-specific triggers. Whileexisting work has studied backdoor attacks on a variety of DNN models, theyonly consider static models, which remain unchanged after initial deployment.",0
"Interpretable Explanations of Black Boxes by Meaningful Perturbation As machine learning algorithms are increasingly applied to high impact yethigh risk tasks, such as medical diagnosis or autonomous driving, it iscritical that researchers can explain how such algorithms arrived at theirpredictions. In recent years, a number of image saliency methods have beendeveloped to summarize where highly complex neural networks look in an imagefor evidence for their predictions. However, these techniques are limited bytheir heuristic nature and architectural constraints. In this paper, we maketwo main contributions: First, we propose a general framework for learningdifferent kinds of explanations for any black box algorithm. Second, wespecialise the framework to find the part of an image most responsible for aclassifier decision. Unlike previous works, our method is model-agnostic andtestable because it is grounded in explicit and interpretable imageperturbations.",0
"Defense Against Multi-target Trojan Attacks Adversarial attacks on deep learning-based models pose a significant threatto the current AI infrastructure. Among them, Trojan attacks are the hardest todefend against. In this paper, we first introduce a variation of the Badnetkind of attacks that introduces Trojan backdoors to multiple target classes andallows triggers to be placed anywhere in the image. The former makes it morepotent and the latter makes it extremely easy to carry out the attack in thephysical space. The state-of-the-art Trojan detection methods fail with thisthreat model. To defend against this attack, we first introduce a triggerreverse-engineering mechanism that uses multiple images to recover a variety ofpotential triggers. We then propose a detection mechanism by measuring thetransferability of such recovered triggers. A Trojan trigger will have veryhigh transferability i.e. they make other images also go to the same class. Westudy many practical advantages of our attack method and then demonstrate thedetection performance using a variety of image datasets. The experimentalresults show the superior detection performance of our method over thestate-of-the-arts.",0
"Sanity Checks for Saliency Maps Saliency methods have emerged as a popular tool to highlight features in aninput deemed relevant for the prediction of a learned model. Several saliencymethods have been proposed, often guided by visual appeal on image data. Inthis work, we propose an actionable methodology to evaluate what kinds ofexplanations a given method can and cannot provide. We find that reliance,solely, on visual assessment can be misleading. Through extensive experimentswe show that some existing saliency methods are independent both of the modeland of the data generating process. Consequently, methods that fail theproposed tests are inadequate for tasks that are sensitive to either data ormodel, such as, finding outliers in the data, explaining the relationshipbetween inputs and outputs that the model learned, and debugging the model. Weinterpret our findings through an analogy with edge detection in images, atechnique that requires neither training data nor model. Theory in the case ofa linear model and a single-layer convolutional neural network supports ourexperimental findings.",0
"VOS: Learning What You Don't Know by Virtual Outlier Synthesis Out-of-distribution (OOD) detection has received much attention lately due toits importance in the safe deployment of neural networks. One of the keychallenges is that models lack supervision signals from unknown data, and as aresult, can produce overconfident predictions on OOD data. Previous approachesrely on real outlier datasets for model regularization, which can be costly andsometimes infeasible to obtain in practice. In this paper, we present VOS, anovel framework for OOD detection by adaptively synthesizing virtual outliersthat can meaningfully regularize the model's decision boundary during training.Specifically, VOS samples virtual outliers from the low-likelihood region ofthe class-conditional distribution estimated in the feature space. Alongside,we introduce a novel unknown-aware training objective, which contrastivelyshapes the uncertainty space between the ID data and synthesized outlier data.VOS achieves competitive performance on both object detection and imageclassification models, reducing the FPR95 by up to 9.36% compared to theprevious best method on object detectors. Code is available at",0
"Network Dissection: Quantifying Interpretability of Deep Visual Representations We propose a general framework called Network Dissection for quantifying theinterpretability of latent representations of CNNs by evaluating the alignmentbetween individual hidden units and a set of semantic concepts. Given any CNNmodel, the proposed method draws on a broad data set of visual concepts toscore the semantics of hidden units at each intermediate convolutional layer.The units with semantics are given labels across a range of objects, parts,scenes, textures, materials, and colors. We use the proposed method to test thehypothesis that interpretability of units is equivalent to random linearcombinations of units, then we apply our method to compare the latentrepresentations of various networks when trained to solve different supervisedand self-supervised training tasks. We further analyze the effect of trainingiterations, compare networks trained with different initializations, examinethe impact of network depth and width, and measure the effect of dropout andbatch normalization on the interpretability of deep visual representations. Wedemonstrate that the proposed method can shed light on characteristics of CNNmodels and training methods that go beyond measurements of their discriminativepower.",0
"A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks Detecting test samples drawn sufficiently far away from the trainingdistribution statistically or adversarially is a fundamental requirement fordeploying a good classifier in many real-world machine learning applications.However, deep neural networks with the softmax classifier are known to producehighly overconfident posterior distributions even for such abnormal samples. Inthis paper, we propose a simple yet effective method for detecting any abnormalsamples, which is applicable to any pre-trained softmax neural classifier. Weobtain the class conditional Gaussian distributions with respect to (low- andupper-level) features of the deep models under Gaussian discriminant analysis,which result in a confidence score based on the Mahalanobis distance. Whilemost prior methods have been evaluated for detecting either out-of-distributionor adversarial samples, but not both, the proposed method achieves thestate-of-the-art performances for both cases in our experiments. Moreover, wefound that our proposed method is more robust in harsh cases, e.g., when thetraining dataset has noisy labels or small number of samples. Finally, we showthat the proposed method enjoys broader usage by applying it toclass-incremental learning: whenever out-of-distribution samples are detected,our classification rule can incorporate new classes well without furthertraining deep models.",0
"Understanding Game-Playing Agents with Natural Language Annotations We present a new dataset containing 10K human-annotated games of Go and showhow these natural language annotations can be used as a tool for modelinterpretability. Given a board state and its associated comment, our approachuses linear probing to predict mentions of domain-specific terms (e.g., ko,atari) from the intermediate state representations of game-playing agents likeAlphaGo Zero. We find these game concepts are nontrivially encoded in twodistinct policy networks, one trained via imitation learning and anothertrained via reinforcement learning. Furthermore, mentions of domain-specificterms are most easily predicted from the later layers of both models,suggesting that these policy networks encode high-level abstractions similar tothose used in the natural language annotations.",0
"Robust Calibration with Multi-domain Temperature Scaling Uncertainty quantification is essential for the reliable deployment ofmachine learning models to high-stakes application domains. Uncertaintyquantification is all the more challenging when training distribution and testdistribution are different, even the distribution shifts are mild. Despite theubiquity of distribution shifts in real-world applications, existinguncertainty quantification approaches mainly study the in-distribution settingwhere the train and test distributions are the same. In this paper, we developa systematic calibration model to handle distribution shifts by leveraging datafrom multiple domains. Our proposed method -- multi-domain temperature scaling-- uses the heterogeneity in the domains to improve calibration robustnessunder distribution shift. Through experiments on three benchmark data sets, wefind our proposed method outperforms existing methods as measured on bothin-distribution and out-of-distribution test sets.",0
"BertNet: Harvesting Knowledge Graphs from Pretrained Language Models Symbolic knowledge graphs (KGs) have been constructed either by expensivehuman crowdsourcing or with domain-specific complex information extractionpipelines. The emerging large pretrained language models (LMs), such as Bert,have shown to implicitly encode massive knowledge which can be queried withproperly designed prompts. However, compared to the explicit KGs, the implictknowledge in the black-box LMs is often difficult to access or edit and lacksexplainability. In this work, we aim at harvesting symbolic KGs from the LMs, anew framework for automatic KG construction empowered by the neural LMs'flexibility and scalability. Compared to prior works that often rely on largehuman annotated data or existing massive KGs, our approach requires only theminimal definition of relations as inputs, and hence is suitable for extractingknowledge of rich new relations not available before.The approach automaticallygenerates diverse prompts, and performs efficient knowledge search within agiven LM for consistent and extensive outputs. The harvested knowledge with ourapproach is substantially more accurate than with previous methods, as shown inboth automatic and human evaluation. As a result, we derive from diverse LMs afamily of new KGs (e.g., BertNet and RoBERTaNet) that contain a richer set ofcommonsense relations, including complex ones (e.g., A is capable of but notgood at B), than the human-annotated KGs (e.g., ConceptNet). Besides, theresulting KGs also serve as a vehicle to interpret the respective source LMs,leading to new insights into the varying knowledge capability of different LMs.",0
"Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning Deep learning models have achieved high performance on many tasks, and thushave been applied to many security-critical scenarios. For example, deeplearning-based face recognition systems have been used to authenticate users toaccess many security-sensitive applications like payment apps. Such usages ofdeep learning systems provide the adversaries with sufficient incentives toperform attacks against these systems for their adversarial purposes. In thiswork, we consider a new type of attacks, called backdoor attacks, where theattacker's goal is to create a backdoor into a learning-based authenticationsystem, so that he can easily circumvent the system by leveraging the backdoor.Specifically, the adversary aims at creating backdoor instances, so that thevictim learning system will be misled to classify the backdoor instances as atarget label specified by the adversary. In particular, we study backdoorpoisoning attacks, which achieve backdoor attacks using poisoning strategies.Different from all existing work, our studied poisoning strategies can applyunder a very weak threat model: (1) the adversary has no knowledge of the modeland the training set used by the victim system; (2) the attacker is allowed toinject only a small amount of poisoning samples; (3) the backdoor key is hardto notice even by human beings to achieve stealthiness. We conduct evaluationto demonstrate that a backdoor adversary can inject only around 50 poisoningsamples, while achieving an attack success rate of above 90%. We are also thefirst work to show that a data poisoning attack can create physicallyimplementable backdoors without touching the training process. Our workdemonstrates that backdoor poisoning attacks pose real threats to a learningsystem, and thus highlights the importance of further investigation andproposing defense strategies against them.",0
"ViM: Out-Of-Distribution with Virtual-logit Matching Most of the existing Out-Of-Distribution (OOD) detection algorithms depend onsingle input source: the feature, the logit, or the softmax probability.However, the immense diversity of the OOD examples makes such methods fragile.There are OOD samples that are easy to identify in the feature space while hardto distinguish in the logit space and vice versa. Motivated by thisobservation, we propose a novel OOD scoring method named Virtual-logit Matching(ViM), which combines the class-agnostic score from feature space and theIn-Distribution (ID) class-dependent logits. Specifically, an additional logitrepresenting the virtual OOD class is generated from the residual of thefeature against the principal space, and then matched with the original logitsby a constant scaling. The probability of this virtual logit after softmax isthe indicator of OOD-ness. To facilitate the evaluation of large-scale OODdetection in academia, we create a new OOD dataset for ImageNet-1K, which ishuman-annotated and is 8.8x the size of existing datasets. We conductedextensive experiments, including CNNs and vision transformers, to demonstratethe effectiveness of the proposed ViM score. In particular, using the BiT-Smodel, our method gets an average AUROC 90.91% on four difficult OODbenchmarks, which is 4% ahead of the best baseline. Code and dataset areavailable at",0
"A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks We consider the two related problems of detecting if an example ismisclassified or out-of-distribution. We present a simple baseline thatutilizes probabilities from softmax distributions. Correctly classifiedexamples tend to have greater maximum softmax probabilities than erroneouslyclassified and out-of-distribution examples, allowing for their detection. Weassess performance by defining several tasks in computer vision, naturallanguage processing, and automatic speech recognition, showing theeffectiveness of this baseline across all. We then show the baseline cansometimes be surpassed, demonstrating the room for future research on theseunderexplored detection tasks.",0
"Locating and Editing Factual Associations in GPT We analyze the storage and recall of factual associations in autoregressivetransformer language models, finding evidence that these associationscorrespond to localized, directly-editable computations. We first develop acausal intervention for identifying neuron activations that are decisive in amodel's factual predictions. This reveals a distinct set of steps inmiddle-layer feed-forward modules that mediate factual predictions whileprocessing subject tokens. To test our hypothesis that these computationscorrespond to factual association recall, we modify feed-forward weights toupdate specific factual associations using Rank-One Model Editing (ROME). Wefind that ROME is effective on a standard zero-shot relation extraction (zsRE)model-editing task, comparable to existing methods. To perform a more sensitiveevaluation, we also evaluate ROME on a new dataset of counterfactualassertions, on which it simultaneously maintains both specificity andgeneralization, whereas other methods sacrifice one or another. Our resultsconfirm an important role for mid-layer feed-forward modules in storing factualassociations and suggest that direct manipulation of computational mechanismsmay be a feasible approach for model editing. The code, dataset,visualizations, and an interactive demo notebook are available at",0
"Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles Deep neural networks (NNs) are powerful black box predictors that haverecently achieved impressive performance on a wide spectrum of tasks.Quantifying predictive uncertainty in NNs is a challenging and yet unsolvedproblem. Bayesian NNs, which learn a distribution over weights, are currentlythe state-of-the-art for estimating predictive uncertainty; however theserequire significant modifications to the training procedure and arecomputationally expensive compared to standard (non-Bayesian) NNs. We proposean alternative to Bayesian NNs that is simple to implement, readilyparallelizable, requires very little hyperparameter tuning, and yields highquality predictive uncertainty estimates. Through a series of experiments onclassification and regression benchmarks, we demonstrate that our methodproduces well-calibrated uncertainty estimates which are as good or better thanapproximate Bayesian NNs. To assess robustness to dataset shift, we evaluatethe predictive uncertainty on test examples from known and unknowndistributions, and show that our method is able to express higher uncertaintyon out-of-distribution examples. We demonstrate the scalability of our methodby evaluating predictive uncertainty estimates on ImageNet.",0
"Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization Feature visualizations such as synthetic maximally activating images are awidely used explanation method to better understand the information processingof convolutional neural networks (CNNs). At the same time, there are concernsthat these visualizations might not accurately represent CNNs' inner workings.Here, we measure how much extremely activating images help humans to predictCNN activations. Using a well-controlled psychophysical paradigm, we comparethe informativeness of synthetic images by Olah et al. (2017) with a simplebaseline visualization, namely exemplary natural images that also stronglyactivate a specific feature map. Given either synthetic or natural referenceimages, human participants choose which of two query images leads to strongpositive activation. The experiments are designed to maximize participants'performance, and are the first to probe intermediate instead of final layerrepresentations. We find that synthetic images indeed provide helpfulinformation about feature map activations ($82\pm4\%$ accuracy; chance would be$50\%$). However, natural images - originally intended as a baseline -outperform synthetic images by a wide margin ($92\pm2\%$). Additionally,participants are faster and more confident for natural images, whereassubjective impressions about the interpretability of the feature visualizationsare mixed. The higher informativeness of natural images holds across mostlayers, for both expert and lay participants as well as for hand- andrandomly-picked feature visualizations. Even if only a single reference imageis given, synthetic images provide less information than natural images($65\pm5\%$ vs. $73\pm4\%$). In summary, synthetic images from a popularfeature visualization method are significantly less informative for assessingCNN activations than natural images. We argue that visualization methods shouldimprove over this baseline.",0
"BackdoorBench: A Comprehensive Benchmark of Backdoor Learning Backdoor learning is an emerging and important topic of studying thevulnerability of deep neural networks (DNNs). Many pioneering backdoor attackand defense methods are being proposed successively or concurrently, in thestatus of a rapid arms race. However, we find that the evaluations of newmethods are often unthorough to verify their claims and real performance,mainly due to the rapid development, diverse settings, as well as thedifficulties of implementation and reproducibility. Without thoroughevaluations and comparisons, it is difficult to track the current progress anddesign the future development roadmap of the literature. To alleviate thisdilemma, we build a comprehensive benchmark of backdoor learning, calledBackdoorBench. It consists of an extensible modular based codebase (currentlyincluding implementations of 8 state-of-the-art (SOTA) attack and 9 SOTAdefense algorithms), as well as a standardized protocol of a complete backdoorlearning. We also provide comprehensive evaluations of every pair of 8 attacksagainst 9 defenses, with 5 poisoning ratios, based on 5 models and 4 datasets,thus 8,000 pairs of evaluations in total. We further present analysis fromdifferent perspectives about these 8,000 evaluations, studying the effects ofattack against defense algorithms, poisoning ratio, model and dataset inbackdoor learning. All codes and evaluations of BackdoorBench are publiclyavailable at \url{",0
"One-shot Neural Backdoor Erasing via Adversarial Weight Masking Recent studies show that despite achieving high accuracy on a number ofreal-world applications, deep neural networks (DNNs) can be backdoored: byinjecting triggered data samples into the training dataset, the adversary canmislead the trained model into classifying any test data to the target class aslong as the trigger pattern is presented. To nullify such backdoor threats,various methods have been proposed. Particularly, a line of research aims topurify the potentially compromised model. However, one major limitation of thisline of work is the requirement to access sufficient original training data:the purifying performance is a lot worse when the available training data islimited. In this work, we propose Adversarial Weight Masking (AWM), a novelmethod capable of erasing the neural backdoors even in the one-shot setting.The key idea behind our method is to formulate this into a min-max optimizationproblem: first, adversarially recover the trigger patterns and then (soft) maskthe network weights that are sensitive to the recovered patterns. Comprehensiveevaluations of several benchmark datasets suggest that AWM can largely improvethe purifying effects over other state-of-the-art methods on various availabletraining dataset sizes.",0
"BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain Deep learning-based techniques have achieved state-of-the-art performance ona wide variety of recognition and classification tasks. However, these networksare typically computationally expensive to train, requiring weeks ofcomputation on many GPUs; as a result, many users outsource the trainingprocedure to the cloud or rely on pre-trained models that are then fine-tunedfor a specific task. In this paper we show that outsourced training introducesnew security risks: an adversary can create a maliciously trained network (abackdoored neural network, or a \emph{BadNet}) that has state-of-the-artperformance on the user's training and validation samples, but behaves badly onspecific attacker-chosen inputs. We first explore the properties of BadNets ina toy example, by creating a backdoored handwritten digit classifier. Next, wedemonstrate backdoors in a more realistic scenario by creating a U.S. streetsign classifier that identifies stop signs as speed limits when a specialsticker is added to the stop sign; we then show in addition that the backdoorin our US street sign detector can persist even if the network is laterretrained for another task and cause a drop in accuracy of {25}\% on averagewhen the backdoor trigger is present. These results demonstrate that backdoorsin neural networks are both powerful and---because the behavior of neuralnetworks is difficult to explicate---stealthy. This work provides motivationfor further research into techniques for verifying and inspecting neuralnetworks, just as we have developed tools for verifying and debugging software.",0
"Detecting AI Trojans Using Meta Neural Analysis In machine learning Trojan attacks, an adversary trains a corrupted modelthat obtains good performance on normal data but behaves maliciously on datasamples with certain trigger patterns. Several approaches have been proposed todetect such attacks, but they make undesirable assumptions about the attackstrategies or require direct access to the trained models, which restrictstheir utility in practice.",0
"Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior Transparency methods such as model visualizations provide information thatoutputs alone might miss, since they describe the internals of neural networks.But can we trust that model explanations reflect model behavior? For instance,can they diagnose abnormal behavior such as backdoors or shape bias? Toevaluate model explanations, we define a model as anomalous if it differs froma reference set of normal models, and we test whether transparency methodsassign different explanations to anomalous and normal models. We find thatwhile existing methods can detect stark anomalies such as shape bias oradversarial training, they struggle to identify more subtle anomalies such asmodels trained on incomplete data. Moreover, they generally fail to distinguishthe inputs that induce anomalous behavior, e.g. images containing a backdoortrigger. These results reveal new blind spots in existing model explanations,pointing to the need for further method development.",0
"Emergent Abilities of Large Language Models Scaling up language models has been shown to predictably improve performanceand sample efficiency on a wide range of downstream tasks. This paper insteaddiscusses an unpredictable phenomenon that we refer to as emergent abilities oflarge language models. We consider an ability to be emergent if it is notpresent in smaller models but is present in larger models. Thus, emergentabilities cannot be predicted simply by extrapolating the performance ofsmaller models. The existence of such emergence implies that additional scalingcould further expand the range of capabilities of language models.",0
"IBP Regularization for Verified Adversarial Robustness via Branch-and-Bound Recent works have tried to increase the verifiability of adversariallytrained networks by running the attacks over domains larger than the originalperturbations and adding various regularization terms to the objective.However, these algorithms either underperform or require complex and expensivestage-wise training procedures, hindering their practical applicability. Wepresent IBP-R, a novel verified training algorithm that is both simple andeffective. IBP-R induces network verifiability by coupling adversarial attackson enlarged domains with a regularization term, based on inexpensive intervalbound propagation, that minimizes the gap between the non-convex verificationproblem and its approximations. By leveraging recent branch-and-boundframeworks, we show that IBP-R obtains state-of-the-art verifiedrobustness-accuracy trade-offs for small perturbations on CIFAR-10 whiletraining significantly faster than relevant previous work. Additionally, wepresent UPB, a novel branching strategy that, relying on a simple heuristicbased on $\beta$-CROWN, reduces the cost of state-of-the-art branchingalgorithms while yielding splits of comparable quality.",0
"STRIP: A Defence Against Trojan Attacks on Deep Neural Networks A recent trojan attack on deep neural network (DNN) models is one insidiousvariant of data poisoning attacks. Trojan attacks exploit an effective backdoorcreated in a DNN model by leveraging the difficulty in interpretability of thelearned model to misclassify any inputs signed with the attacker's chosentrojan trigger. Since the trojan trigger is a secret guarded and exploited bythe attacker, detecting such trojan inputs is a challenge, especially atrun-time when models are in active operation. This work builds STRongIntentional Perturbation (STRIP) based run-time trojan attack detection systemand focuses on vision system. We intentionally perturb the incoming input, forinstance by superimposing various image patterns, and observe the randomness ofpredicted classes for perturbed inputs from a given deployed model---maliciousor benign. A low entropy in predicted classes violates the input-dependenceproperty of a benign model and implies the presence of a malicious input---acharacteristic of a trojaned input. The high efficacy of our method isvalidated through case studies on three popular and contrasting datasets:MNIST, CIFAR10 and GTSRB. We achieve an overall false acceptance rate (FAR) ofless than 1%, given a preset false rejection rate (FRR) of 1%, for differenttypes of triggers. Using CIFAR10 and GTSRB, we have empirically achieved resultof 0% for both FRR and FAR. We have also evaluated STRIP robustness against anumber of trojan attack variants and adaptive attacks.",0
"Accurate Uncertainties for Deep Learning Using Calibrated Regression Methods for reasoning under uncertainty are a key building block of accurateand reliable machine learning systems. Bayesian methods provide a generalframework to quantify uncertainty. However, because of model misspecificationand the use of approximate inference, Bayesian uncertainty estimates are ofteninaccurate -- for example, a 90% credible interval may not contain the trueoutcome 90% of the time. Here, we propose a simple procedure for calibratingany regression algorithm; when applied to Bayesian and probabilistic models, itis guaranteed to produce calibrated uncertainty estimates given enough data.Our procedure is inspired by Platt scaling and extends previous work onclassification. We evaluate this approach on Bayesian linear regression,feedforward, and recurrent neural networks, and find that it consistentlyoutputs well-calibrated credible intervals while improving performance on timeseries forecasting and model-based reinforcement learning tasks.",0
"Scaling Out-of-Distribution Detection for Real-World Settings Detecting out-of-distribution examples is important for safety-criticalmachine learning applications such as detecting novel biological phenomena andself-driving cars. However, existing research mainly focuses on simplesmall-scale settings. To set the stage for more realistic out-of-distributiondetection, we depart from small-scale settings and explore large-scalemulticlass and multi-label settings with high-resolution images and thousandsof classes. To make future work in real-world settings possible, we create newbenchmarks for three large-scale settings. To test ImageNet multiclass anomalydetectors, we introduce the Species dataset containing over 700,000 images andover a thousand anomalous species. We leverage ImageNet-21K to evaluate PASCALVOC and COCO multilabel anomaly detectors. Third, we introduce a new benchmarkfor anomaly segmentation by introducing a segmentation benchmark with roadanomalies. We conduct extensive experiments in these more realistic settingsfor out-of-distribution detection and find that a surprisingly simple detectorbased on the maximum logit outperforms prior methods in all the large-scalemulti-class, multi-label, and segmentation tasks, establishing a simple newbaseline for future work.",0
"Single-Turn Debate Does Not Help Humans Answer Hard Reading-Comprehension Questions Current QA systems can generate reasonable-sounding yet false answers withoutexplanation or evidence for the generated answer, which is especiallyproblematic when humans cannot readily check the model's answers. This presentsa challenge for building trust in machine learning systems. We take inspirationfrom real-world situations where difficult questions are answered byconsidering opposing sides (see Irving et al., 2018). For multiple-choice QAexamples, we build a dataset of single arguments for both a correct andincorrect answer option in a debate-style set-up as an initial step in trainingmodels to produce explanations for two candidate answers. We use long contexts-- humans familiar with the context write convincing explanations forpre-selected correct and incorrect answers, and we test if those explanationsallow humans who have not read the full context to more accurately determinethe correct answer. We do not find that explanations in our set-up improvehuman accuracy, but a baseline condition shows that providing human-selectedtext snippets does improve accuracy. We use these findings to suggest ways ofimproving the debate set up for future data collection efforts.",0
"Teaching Models to Express Their Uncertainty in Words We show that a GPT-3 model can learn to express uncertainty about its ownanswers in natural language -- without use of model logits. When given aquestion, the model generates both an answer and a level of confidence (e.g.90% confidence or high confidence). These levels map to probabilities thatare well calibrated. The model also remains moderately calibrated underdistribution shift, and is sensitive to uncertainty in its own answers, ratherthan imitating human examples. To our knowledge, this is the first time a modelhas been shown to express calibrated uncertainty about its own answers innatural language. For testing calibration, we introduce the CalibratedMathsuite of tasks. We compare the calibration of uncertainty expressed in words(verbalized probability) to uncertainty extracted from model logits. Bothkinds of uncertainty are capable of generalizing calibration under distributionshift. We also provide evidence that GPT-3's ability to generalize calibrationdepends on pre-trained latent representations that correlate with epistemicuncertainty over its answers.",0
"Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead Black box machine learning models are currently being used for high stakesdecision-making throughout society, causing problems throughout healthcare,criminal justice, and in other domains. People have hoped that creating methodsfor explaining these black box models will alleviate some of these problems,but trying to \textit{explain} black box models, rather than creating modelsthat are \textit{interpretable} in the first place, is likely to perpetuate badpractices and can potentially cause catastrophic harm to society. There is away forward -- it is to design models that are inherently interpretable. Thismanuscript clarifies the chasm between explaining black boxes and usinginherently interpretable models, outlines several key reasons why explainableblack boxes should be avoided in high-stakes decisions, identifies challengesto interpretable machine learning, and provides several example applicationswhere interpretable models could potentially replace black box models incriminal justice, healthcare, and computer vision.",0
"The Mythos of Model Interpretability Supervised machine learning models boast remarkable predictive capabilities.But can you trust your model? Will it work in deployment? What else can it tellyou about the world? We want models to be not only good, but interpretable. Andyet the task of interpretation appears underspecified. Papers provide diverseand sometimes non-overlapping motivations for interpretability, and offermyriad notions of what attributes render models interpretable. Despite thisambiguity, many papers proclaim interpretability axiomatically, absent furtherexplanation. In this paper, we seek to refine the discourse oninterpretability. First, we examine the motivations underlying interest ininterpretability, finding them to be diverse and occasionally discordant. Then,we address model properties and techniques thought to confer interpretability,identifying transparency to humans and post-hoc explanations as competingnotions. Throughout, we discuss the feasibility and desirability of differentnotions, and question the oft-made assertions that linear models areinterpretable and that deep neural networks are not.",0
"Network Dissection: Quantifying Interpretability of Deep Visual Representations We propose a general framework called Network Dissection for quantifying theinterpretability of latent representations of CNNs by evaluating the alignmentbetween individual hidden units and a set of semantic concepts. Given any CNNmodel, the proposed method draws on a broad data set of visual concepts toscore the semantics of hidden units at each intermediate convolutional layer.The units with semantics are given labels across a range of objects, parts,scenes, textures, materials, and colors. We use the proposed method to test thehypothesis that interpretability of units is equivalent to random linearcombinations of units, then we apply our method to compare the latentrepresentations of various networks when trained to solve different supervisedand self-supervised training tasks. We further analyze the effect of trainingiterations, compare networks trained with different initializations, examinethe impact of network depth and width, and measure the effect of dropout andbatch normalization on the interpretability of deep visual representations. Wedemonstrate that the proposed method can shed light on characteristics of CNNmodels and training methods that go beyond measurements of their discriminativepower.",0
"A geometric framework for outlier detection in high-dimensional data Outlier or anomaly detection is an important task in data analysis. Wediscuss the problem from a geometrical perspective and provide a framework thatexploits the metric structure of a data set. Our approach rests on the manifoldassumption, i.e., that the observed, nominally high-dimensional data lie on amuch lower dimensional manifold and that this intrinsic structure can beinferred with manifold learning methods. We show that exploiting this structuresignificantly improves the detection of outlying observations inhigh-dimensional data. We also suggest a novel, mathematically precise, andwidely applicable distinction between distributional and structural outliersbased on the geometry and topology of the data manifold that clarifiesconceptual ambiguities prevalent throughout the literature. Our experimentsfocus on functional data as one class of structured high-dimensional data, butthe framework we propose is completely general and we include image and graphdata applications. Our results show that the outlier structure ofhigh-dimensional and non-tabular data can be detected and visualized usingmanifold learning methods and quantified using standard outlier scoring methodsapplied to the manifold embedding vectors.",0
"Imperceptible Backdoor Attack: From Input Space to Feature Representation Backdoor attacks are rapidly emerging threats to deep neural networks (DNNs).In the backdoor attack scenario, attackers usually implant the backdoor intothe target model by manipulating the training dataset or training process.Then, the compromised model behaves normally for benign input yet makesmistakes when the pre-defined trigger appears. In this paper, we analyze thedrawbacks of existing attack approaches and propose a novel imperceptiblebackdoor attack. We treat the trigger pattern as a special kind of noisefollowing a multinomial distribution. A U-net-based network is employed togenerate concrete parameters of multinomial distribution for each benign input.This elaborated trigger ensures that our approach is invisible to both humansand statistical detection. Besides the design of the trigger, we also considerthe robustness of our approach against model diagnose-based defences. We forcethe feature representation of malicious input stamped with the trigger to beentangled with the benign one. We demonstrate the effectiveness and robustnessagainst multiple state-of-the-art defences through extensive datasets andnetworks. Our trigger only modifies less than 1\% pixels of a benign imagewhile the modification magnitude is 1. Our source code is available at",0
"Poisoning and Backdooring Contrastive Learning Multimodal contrastive learning methods like CLIP train on noisy anduncurated training datasets. This is cheaper than labeling datasets manually,and even improves out-of-distribution robustness. We show that this practicemakes backdoor and poisoning attacks a significant threat. By poisoning just0.01% of a dataset (e.g., just 300 images of the 3 million-example ConceptualCaptions dataset), we can cause the model to misclassify test images byoverlaying a small patch. Targeted poisoning attacks, whereby the modelmisclassifies a particular test input with an adversarially-desired label, areeven easier requiring control of 0.0001% of the dataset (e.g., just three outof the 3 million images). Our attacks call into question whether training onnoisy and uncurated Internet scrapes is desirable.",0
"Deep Anomaly Detection with Outlier Exposure It is important to detect anomalous inputs when deploying machine learningsystems. The use of larger and more complex inputs in deep learning magnifiesthe difficulty of distinguishing between anomalous and in-distributionexamples. At the same time, diverse image and text data are available inenormous quantities. We propose leveraging these data to improve deep anomalydetection by training anomaly detectors against an auxiliary dataset ofoutliers, an approach we call Outlier Exposure (OE). This enables anomalydetectors to generalize and detect unseen anomalies. In extensive experimentson natural language processing and small- and large-scale vision tasks, we findthat Outlier Exposure significantly improves detection performance. We alsoobserve that cutting-edge generative models trained on CIFAR-10 may assignhigher likelihoods to SVHN images than to CIFAR-10 images; we use OE tomitigate this issue. We also analyze the flexibility and robustness of OutlierExposure, and identify characteristics of the auxiliary dataset that improveperformance.",0
"Data Augmentation Can Improve Robustness Adversarial training suffers from robust overfitting, a phenomenon where therobust test accuracy starts to decrease during training. In this paper, wefocus on reducing robust overfitting by using common data augmentation schemes.We demonstrate that, contrary to previous findings, when combined with modelweight averaging, data augmentation can significantly boost robust accuracy.Furthermore, we compare various augmentations techniques and observe thatspatial composition techniques work the best for adversarial training. Finally,we evaluate our approach on CIFAR-10 against $\ell_\infty$ and $\ell_2$norm-bounded perturbations of size $\epsilon = 8/255$ and $\epsilon = 128/255$,respectively. We show large absolute improvements of +2.93% and +2.16% inrobust accuracy compared to previous state-of-the-art methods. In particular,against $\ell_\infty$ norm-bounded perturbations of size $\epsilon = 8/255$,our model reaches 60.07% robust accuracy without using any external data. Wealso achieve a significant performance boost with this approach while usingother architectures and datasets such as CIFAR-100, SVHN and TinyImageNet.",0
"Adversarial Training for High-Stakes Reliability In the future, powerful AI systems may be deployed in high-stakes settings,where a single failure could be catastrophic. One technique for improving AIsafety in high-stakes settings is adversarial training, which uses an adversaryto generate examples to train on in order to achieve better worst-caseperformance.",0
"Can CNNs Be More Robust Than Transformers? The recent success of Vision Transformers is shaking the long dominance ofConvolutional Neural Networks (CNNs) in image recognition for a decade.Specifically, in terms of robustness on out-of-distribution samples, recentresearch finds that Transformers are inherently more robust than CNNs,regardless of different training setups. Moreover, it is believed that suchsuperiority of Transformers should largely be credited to theirself-attention-like architectures per se. In this paper, we question thatbelief by closely examining the design of Transformers. Our findings lead tothree highly effective architecture designs for boosting robustness, yet simpleenough to be implemented in several lines of code, namely a) patchifying inputimages, b) enlarging kernel size, and c) reducing activation layers andnormalization layers. Bringing these components together, we are able to buildpure CNN architectures without any attention-like operations that is as robustas, or even more robust than, Transformers. We hope this work can help thecommunity better understand the design of robust neural architectures. The codeis publicly available at",0
"Models Out of Line: A Fourier Lens on Distribution Shift Robustness Improving the accuracy of deep neural networks (DNNs) on out-of-distribution(OOD) data is critical to an acceptance of deep learning (DL) in real worldapplications. It has been observed that accuracies on in-distribution (ID)versus OOD data follow a linear trend and models that outperform this baselineare exceptionally rare (and referred to as effectively robust). Recently,some promising approaches have been developed to improve OOD robustness: modelpruning, data augmentation, and ensembling or zero-shot evaluating largepretrained models. However, there still is no clear understanding of theconditions on OOD data and model properties that are required to observeeffective robustness. We approach this issue by conducting a comprehensiveempirical study of diverse approaches that are known to impact OOD robustnesson a broad range of natural and synthetic distribution shifts of CIFAR-10 andImageNet. In particular, we view the effective robustness puzzle through aFourier lens and ask how spectral properties of both models and OOD datainfluence the corresponding effective robustness. We find this Fourier lensoffers some insight into why certain robust models, particularly those from theCLIP family, achieve OOD robustness. However, our analysis also makes clearthat no known metric is consistently the best explanation (or even a strongexplanation) of OOD robustness. Thus, to aid future research into the OODpuzzle, we address the gap in publicly-available models with effectiverobustness by introducing a set of pretrained models--RobustNets--with varyinglevels of OOD robustness.",0
"Adversarial Text Normalization Text-based adversarial attacks are becoming more commonplace and accessibleto general internet users. As these attacks proliferate, the need to addressthe gap in model robustness becomes imminent. While retraining on adversarialdata may increase performance, there remains an additional class ofcharacter-level attacks on which these models falter. Additionally, the processto retrain a model is time and resource intensive, creating a need for alightweight, reusable defense. In this work, we propose the Adversarial TextNormalizer, a novel method that restores baseline performance on attackedcontent with low computational overhead. We evaluate the efficacy of thenormalizer on two problem areas prone to adversarial attacks, i.e. Hate Speechand Natural Language Inference. We find that text normalization provides atask-agnostic defense against character-level attacks that can be implementedsupplementary to adversarial retraining solutions, which are more suited forsemantic alterations.",0
"Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples We identify obfuscated gradients, a kind of gradient masking, as a phenomenonthat leads to a false sense of security in defenses against adversarialexamples. While defenses that cause obfuscated gradients appear to defeatiterative optimization-based attacks, we find defenses relying on this effectcan be circumvented. We describe characteristic behaviors of defensesexhibiting the effect, and for each of the three types of obfuscated gradientswe discover, we develop attack techniques to overcome it. In a case study,examining non-certified white-box-secure defenses at ICLR 2018, we findobfuscated gradients are a common occurrence, with 7 of 9 defenses relying onobfuscated gradients. Our new attacks successfully circumvent 6 completely, and1 partially, in the original threat model each paper considers.",0
"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks The field of defense strategies against adversarial attacks has significantlygrown over the last years, but progress is hampered as the evaluation ofadversarial defenses is often insufficient and thus gives a wrong impression ofrobustness. Many promising defenses could be broken later on, making itdifficult to identify the state-of-the-art. Frequent pitfalls in the evaluationare improper tuning of hyperparameters of the attacks, gradient obfuscation ormasking. In this paper we first propose two extensions of the PGD-attackovercoming failures due to suboptimal step size and problems of the objectivefunction. We then combine our novel attacks with two complementary existingones to form a parameter-free, computationally affordable and user-independentensemble of attacks to test adversarial robustness. We apply our ensemble toover 50 models from papers published at recent top machine learning andcomputer vision venues. In all except one of the cases we achieve lower robusttest accuracy than reported in these papers, often by more than $10\%$,identifying several broken defenses.",0
"Towards Evaluating the Robustness of Neural Networks Neural networks provide state-of-the-art results for most machine learningtasks. Unfortunately, neural networks are vulnerable to adversarial examples:given an input $x$ and any target classification $t$, it is possible to find anew input $x'$ that is similar to $x$ but classified as $t$. This makes itdifficult to apply neural networks in security-critical areas. Defensivedistillation is a recently proposed approach that can take an arbitrary neuralnetwork, and increase its robustness, reducing the success rate of currentattacks' ability to find adversarial examples from $95\%$ to $0.5\%$.",0
"Probable Domain Generalization via Quantile Risk Minimization Domain generalization (DG) seeks predictors which perform well on unseen testdistributions by leveraging labeled training data from multiple relateddistributions or domains. To achieve this, the standard formulation optimizesfor worst-case performance over the set of all possible domains. However, withworst-case shifts very unlikely in practice, this generally leads tooverly-conservative solutions. In fact, a recent study found that no DGalgorithm outperformed empirical risk minimization in terms of averageperformance. In this work, we argue that DG is neither a worst-case problem noran average-case problem, but rather a probabilistic one. To this end, wepropose a probabilistic framework for DG, which we call Probable DomainGeneralization, wherein our key idea is that distribution shifts seen duringtraining should inform us of probable shifts at test time. To realize this, weexplicitly relate training and test domains as draws from the same underlyingmeta-distribution, and propose a new optimization problem -- Quantile RiskMinimization (QRM) -- which requires that predictors generalize with highprobability. We then prove that QRM: (i) produces predictors that generalize tonew domains with a desired probability, given sufficiently many domains andsamples; and (ii) recovers the causal predictor as the desired probability ofgeneralization approaches one. In our experiments, we introduce a more holisticquantile-focused evaluation protocol for DG, and show that our algorithmsoutperform state-of-the-art baselines on real and synthetic data.",0
"Adversarial Examples for Evaluating Reading Comprehension Systems Standard accuracy metrics indicate that reading comprehension systems aremaking rapid progress, but the extent to which these systems truly understandlanguage remains unclear. To reward systems with real language understandingabilities, we propose an adversarial evaluation scheme for the StanfordQuestion Answering Dataset (SQuAD). Our method tests whether systems can answerquestions about paragraphs that contain adversarially inserted sentences, whichare automatically generated to distract computer systems without changing thecorrect answer or misleading humans. In this adversarial setting, the accuracyof sixteen published models drops from an average of $75\%$ F1 score to $36\%$;when the adversary is allowed to add ungrammatical sequences of words, averageaccuracy on four models decreases further to $7\%$. We hope our insights willmotivate the development of new models that understand language more precisely.",0
"Diffusion Models for Adversarial Purification Adversarial purification refers to a class of defense methods that removeadversarial perturbations using a generative model. These methods do not makeassumptions on the form of attack and the classification model, and thus candefend pre-existing classifiers against unseen threats. However, theirperformance currently falls behind adversarial training methods. In this work,we propose DiffPure that uses diffusion models for adversarial purification:Given an adversarial example, we first diffuse it with a small amount of noisefollowing a forward diffusion process, and then recover the clean image througha reverse generative process. To evaluate our method against strong adaptiveattacks in an efficient and scalable way, we propose to use the adjoint methodto compute full gradients of the reverse generative process. Extensiveexperiments on three image datasets including CIFAR-10, ImageNet and CelebA-HQwith three classifier architectures including ResNet, WideResNet and ViTdemonstrate that our method achieves the state-of-the-art results,outperforming current adversarial training and adversarial purificationmethods, often by a large margin. Project page:",0
"PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures In real-world applications of machine learning, reliable and safe systemsmust consider measures of performance beyond standard test set accuracy. Theseother goals include out-of-distribution (OOD) robustness, predictionconsistency, resilience to adversaries, calibrated uncertainty estimates, andthe ability to detect anomalous inputs. However, improving performance towardsthese goals is often a balancing act that today's methods cannot achievewithout sacrificing performance on other safety axes. For instance, adversarialtraining improves adversarial robustness but sharply degrades other classifierperformance metrics. Similarly, strong data augmentation and regularizationtechniques often improve OOD robustness but harm anomaly detection, raising thequestion of whether a Pareto improvement on all existing safety measures ispossible. To meet this challenge, we design a new data augmentation strategyutilizing the natural structural complexity of pictures such as fractals, whichoutperforms numerous baselines, is near Pareto-optimal, and roundly improvessafety measures.",0
"Adversarial Examples for Evaluating Reading Comprehension Systems Standard accuracy metrics indicate that reading comprehension systems aremaking rapid progress, but the extent to which these systems truly understandlanguage remains unclear. To reward systems with real language understandingabilities, we propose an adversarial evaluation scheme for the StanfordQuestion Answering Dataset (SQuAD). Our method tests whether systems can answerquestions about paragraphs that contain adversarially inserted sentences, whichare automatically generated to distract computer systems without changing thecorrect answer or misleading humans. In this adversarial setting, the accuracyof sixteen published models drops from an average of $75\%$ F1 score to $36\%$;when the adversary is allowed to add ungrammatical sequences of words, averageaccuracy on four models decreases further to $7\%$. We hope our insights willmotivate the development of new models that understand language more precisely.",0
"Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift Recently, Miller et al. showed that a model's in-distribution (ID) accuracyhas a strong linear correlation with its out-of-distribution (OOD) accuracy onseveral OOD benchmarks -- a phenomenon they dubbed ''accuracy-on-the-line''.While a useful tool for model selection (i.e., the model most likely to performthe best OOD is the one with highest ID accuracy), this fact does not helpestimate the actual OOD performance of models without access to a labeled OODvalidation set. In this paper, we show a similar but surprising phenomenon alsoholds for the agreement between pairs of neural network classifiers: wheneveraccuracy-on-the-line holds, we observe that the OOD agreement between thepredictions of any two pairs of neural networks (with potentially differentarchitectures) also observes a strong linear correlation with their IDagreement. Furthermore, we observe that the slope and bias of OOD vs IDagreement closely matches that of OOD vs ID accuracy. This phenomenon, which wecall agreement-on-the-line, has important practical applications: without anylabeled data, we can predict the OOD accuracy of classifiers}, since OODagreement can be estimated with just unlabeled data. Our prediction algorithmoutperforms previous methods both in shifts where agreement-on-the-line holdsand, surprisingly, when accuracy is not on the line. This phenomenon alsoprovides new insights into deep neural networks: unlike accuracy-on-the-line,agreement-on-the-line appears to only hold for neural network classifiers.",0
"Distinction Maximization Loss: Efficiently Improving Classification Accuracy, Uncertainty Estimation, and Out-of-Distribution Detection Simply Replacing the Loss and Calibrating Building robust deterministic neural networks remains a challenge. On the onehand, some approaches improve out-of-distribution detection at the cost ofreducing classification accuracy in some situations. On the other hand, somemethods simultaneously increase classification accuracy, uncertaintyestimation, and out-of-distribution detection at the expense of reducing theinference efficiency and requiring training the same model many times to tunehyperparameters. In this paper, we propose training deterministic neuralnetworks using our DisMax loss, which works as a drop-in replacement for theusual SoftMax loss (i.e., the combination of the linear output layer, theSoftMax activation, and the cross-entropy loss). Starting from the IsoMax+loss, we create each logit based on the distances to all prototypes rather thanjust the one associated with the correct class. We also introduce a mechanismto combine images to construct what we call fractional probabilityregularization. Moreover, we present a fast way to calibrate the network aftertraining. Finally, we propose a composite score to perform out-of-distributiondetection. Our experiments show that DisMax usually outperforms currentapproaches simultaneously in classification accuracy, uncertainty estimation,and out-of-distribution detection while maintaining deterministic neuralnetwork inference efficiency and avoiding training the same model repetitivelyfor hyperparameter tuning. The code to reproduce the results is available at",0
"GSCLIP : A Framework for Explaining Distribution Shifts in Natural Language Helping end users comprehend the abstract distribution shifts can greatlyfacilitate AI deployment. Motivated by this, we propose a novel task, datasetexplanation. Given two image data sets, dataset explanation aims toautomatically point out their dataset-level distribution shifts with naturallanguage. Current techniques for monitoring distribution shifts provideinadequate information to understand datasets with the goal of improving dataquality. Therefore, we introduce GSCLIP, a training-free framework to solve thedataset explanation task. In GSCLIP, we propose the selector as the firstquantitative evaluation method to identify explanations that are proper tosummarize dataset shifts. Furthermore, we leverage this selector to demonstratethe superiority of a generator based on language model generation. Systematicevaluation on natural data shift verifies that GSCLIP, a combined system of ahybrid generator group and an efficient selector is not only easy-to-use butalso powerful for dataset explanation at scale.",0
"Universal Adversarial Triggers for Attacking and Analyzing NLP Adversarial examples highlight model vulnerabilities and are useful forevaluation and interpretation. We define universal adversarial triggers:input-agnostic sequences of tokens that trigger a model to produce a specificprediction when concatenated to any input from a dataset. We propose agradient-guided search over tokens which finds short trigger sequences (e.g.,one word for classification and four words for language modeling) thatsuccessfully trigger the target prediction. For example, triggers cause SNLIentailment accuracy to drop from 89.94% to 0.55%, 72% of why questions inSQuAD to be answered to kill american people, and the GPT-2 language model tospew racist output even when conditioned on non-racial contexts. Furthermore,although the triggers are optimized using white-box access to a specific model,they transfer to other models for all tasks we consider. Finally, sincetriggers are input-agnostic, they provide an analysis of global model behavior.For instance, they confirm that SNLI models exploit dataset biases and help todiagnose heuristics learned by reading comprehension models.",0
"Towards Deep Learning Models Resistant to Adversarial Attacks Recent work has demonstrated that deep neural networks are vulnerable toadversarial examples---inputs that are almost indistinguishable from naturaldata and yet classified incorrectly by the network. In fact, some of the latestfindings suggest that the existence of adversarial attacks may be an inherentweakness of deep learning models. To address this problem, we study theadversarial robustness of neural networks through the lens of robustoptimization. This approach provides us with a broad and unifying view on muchof the prior work on this topic. Its principled nature also enables us toidentify methods for both training and attacking neural networks that arereliable and, in a certain sense, universal. In particular, they specify aconcrete security guarantee that would protect against any adversary. Thesemethods let us train networks with significantly improved resistance to a widerange of adversarial attacks. They also suggest the notion of security againsta first-order adversary as a natural and broad security guarantee. We believethat robustness against such well-defined classes of adversaries is animportant stepping stone towards fully resistant deep learning models. Code andpre-trained models are available at",0
"Can Rationalization Improve Robustness? A growing line of work has investigated the development of neural NLP modelsthat can produce rationales--subsets of input that can explain their modelpredictions. In this paper, we ask whether such rationale models can alsoprovide robustness to adversarial attacks in addition to their interpretablenature. Since these models need to first generate rationales (rationalizer)before making predictions (predictor), they have the potential to ignorenoise or adversarially added text by simply masking it out of the generatedrationale. To this end, we systematically generate various types of 'AddText'attacks for both token and sentence-level rationalization tasks, and perform anextensive empirical evaluation of state-of-the-art rationale models across fivedifferent tasks. Our experiments reveal that the rationale models show thepromise to improve robustness, while they struggle in certain scenarios--whenthe rationalizer is sensitive to positional bias or lexical choices of attacktext. Further, leveraging human rationale as supervision does not alwaystranslate to better performance. Our study is a first step towards exploringthe interplay between interpretability and robustness in therationalize-then-predict framework.",0
"Stream-based active learning with linear models The proliferation of automated data collection schemes and the advances insensorics are increasing the amount of data we are able to monitor inreal-time. However, given the high annotation costs and the time required byquality inspections, data is often available in an unlabeled form. This isfostering the use of active learning for the development of soft sensors andpredictive models. In production, instead of performing random inspections toobtain product information, labels are collected by evaluating the informationcontent of the unlabeled data. Several query strategy frameworks for regressionhave been proposed in the literature but most of the focus has been dedicatedto the static pool-based scenario. In this work, we propose a new strategy forthe stream-based scenario, where instances are sequentially offered to thelearner, which must instantaneously decide whether to perform the quality checkto obtain the label or discard the instance. The approach is inspired by theoptimal experimental design theory and the iterative aspect of thedecision-making process is tackled by setting a threshold on theinformativeness of the unlabeled data points. The proposed approach isevaluated using numerical simulations and the Tennessee Eastman Processsimulator. The results confirm that selecting the examples suggested by theproposed algorithm allows for a faster reduction in the prediction error.",0
"Benchmarking Neural Network Robustness to Common Corruptions and Perturbations In this paper we establish rigorous benchmarks for image classifierrobustness. Our first benchmark, ImageNet-C, standardizes and expands thecorruption robustness topic, while showing which classifiers are preferable insafety-critical applications. Then we propose a new dataset called ImageNet-Pwhich enables researchers to benchmark a classifier's robustness to commonperturbations. Unlike recent robustness research, this benchmark evaluatesperformance on common corruptions and perturbations not worst-case adversarialperturbations. We find that there are negligible changes in relative corruptionrobustness from AlexNet classifiers to ResNet classifiers. Afterward wediscover ways to enhance corruption and perturbation robustness. We even findthat a bypassed adversarial defense provides substantial common perturbationrobustness. Together our benchmarks may aid future work toward networks thatrobustly generalize.",0
"Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data Detecting out-of-distribution (OOD) data is a task that is receiving anincreasing amount of research attention in the domain of deep learning forcomputer vision. However, the performance of detection methods is generallyevaluated on the task in isolation, rather than also considering potentialdownstream tasks in tandem. In this work, we examine selective classificationin the presence of OOD data (SCOD). That is to say, the motivation fordetecting OOD samples is to reject them so their impact on the quality ofpredictions is reduced. We show under this task specification, that existingpost-hoc methods perform quite differently compared to when evaluated only onOOD detection. This is because it is no longer an issue to conflatein-distribution (ID) data with OOD data if the ID data is going to bemisclassified. However, the conflation within ID data of correct and incorrectpredictions becomes undesirable. We also propose a novel method for SCOD,Softmax Information Retaining Combination (SIRC), that augments softmax-basedconfidence scores with feature-agnostic information such that their ability toidentify OOD samples is improved without sacrificing separation between correctand incorrect ID predictions. Experiments on a wide variety of ImageNet-scaledatasets and convolutional neural network architectures show that SIRC is ableto consistently match or outperform the baseline for SCOD, whilst existing OODdetection methods fail to do so.",0
"Adversarial Robustness is at Odds with Lazy Training Recent works show that random neural networks are vulnerable againstadversarial attacks [Daniely and Schacham, 2020] and that such attacks can beeasily found using a single step of gradient descent [Bubeck et al., 2021]. Inthis work, we take it one step further and show that a single gradient step canfind adversarial examples for networks trained in the so-called lazy regime.This regime is interesting because even though the neural network weightsremain close to the initialization, there exist networks with smallgeneralization error, which can be found efficiently using first-order methods.Our work challenges the model of the lazy regime, the dominant regime in whichneural networks are provably efficiently learnable. We show that the networkstrained in this regime, even though they enjoy good theoretical computationalguarantees, remain vulnerable to adversarial examples. To the best of ourknowledge, this is the first work to prove that such well-generalizable neuralnetworks are still vulnerable to adversarial attacks.",0
"BERT-ATTACK: Adversarial Attack Against BERT Using BERT Adversarial attacks for discrete data (such as texts) have been provedsignificantly more challenging than continuous data (such as images) since itis difficult to generate adversarial samples with gradient-based methods.Current successful attack methods for texts usually adopt heuristic replacementstrategies on the character or word level, which remains challenging to findthe optimal solution in the massive space of possible combinations ofreplacements while preserving semantic consistency and language fluency. Inthis paper, we propose \textbf{BERT-Attack}, a high-quality and effectivemethod to generate adversarial samples using pre-trained masked language modelsexemplified by BERT. We turn BERT against its fine-tuned models and other deepneural models in downstream tasks so that we can successfully mislead thetarget models to predict incorrectly. Our method outperforms state-of-the-artattack strategies in both success rate and perturb percentage, while thegenerated adversarial samples are fluent and semantically preserved. Also, thecost of calculation is low, thus possible for large-scale generations. The codeis available at",0
"Robustifying Vision Transformer without Retraining from Scratch by Test-Time Class-Conditional Feature Alignment Vision Transformer (ViT) is becoming more popular in image processing.Specifically, we investigate the effectiveness of test-time adaptation (TTA) onViT, a technique that has emerged to correct its prediction during test-time byitself. First, we benchmark various test-time adaptation approaches on ViT-B16and ViT-L16. It is shown that the TTA is effective on ViT and theprior-convention (sensibly selecting modulation parameters) is not necessarywhen using proper loss function. Based on the observation, we propose a newtest-time adaptation method called class-conditional feature alignment (CFA),which minimizes both the class-conditional distribution differences and thewhole distribution differences of the hidden representation between the sourceand target in an online manner. Experiments of image classification tasks oncommon corruption (CIFAR-10-C, CIFAR-100-C, and ImageNet-C) and domainadaptation (digits datasets and ImageNet-Sketch) show that CFA stablyoutperforms the existing baselines on various datasets. We also verify that CFAis model agnostic by experimenting on ResNet, MLP-Mixer, and several ViTvariants (ViT-AugReg, DeiT, and BeiT). Using BeiT backbone, CFA achieves 19.8%top-1 error rate on ImageNet-C, outperforming the existing test-time adaptationbaseline 44.0%. This is a state-of-the-art result among TTA methods that do notneed to alter training phase.",0
"On the Robustness of Safe Reinforcement Learning under Observational Perturbations Safe reinforcement learning (RL) trains a policy to maximize the task rewardwhile satisfying safety constraints. While prior works focus on the performanceoptimality, we find that the optimal solutions of many safe RL problems are notrobust and safe against carefully designed observational perturbations. Weformally analyze the unique properties of designing effective state adversarialattackers in the safe RL setting. We show that baseline adversarial attacktechniques for standard RL tasks are not always effective for safe RL andproposed two new approaches - one maximizes the cost and the other maximizesthe reward. One interesting and counter-intuitive finding is that the maximumreward attack is strong, as it can both induce unsafe behaviors and make theattack stealthy by maintaining the reward. We further propose a more effectiveadversarial training framework for safe RL and evaluate it via comprehensiveexperiments. This work sheds light on the inherited connection betweenobservational robustness and safety in RL and provides a pioneer work forfuture safe RL studies.",0
"Increasing Confidence in Adversarial Robustness Evaluations Hundreds of defenses have been proposed to make deep neural networks robustagainst minimal (adversarial) input perturbations. However, only a handful ofthese defenses held up their claims because correctly evaluating robustness isextremely challenging: Weak attacks often fail to find adversarial exampleseven if they unknowingly exist, thereby making a vulnerable network lookrobust. In this paper, we propose a test to identify weak attacks, and thusweak defense evaluations. Our test slightly modifies a neural network toguarantee the existence of an adversarial example for every sample.Consequentially, any correct attack must succeed in breaking this modifiednetwork. For eleven out of thirteen previously-published defenses, the originalevaluation of the defense fails our test, while stronger attacks that breakthese defenses pass it. We hope that attack unit tests - such as ours - will bea major component in future robustness evaluations and increase confidence inan empirical field that is currently riddled with skepticism.",0
"GSmooth: Certified Robustness against Semantic Transformations via Generalized Randomized Smoothing Certified defenses such as randomized smoothing have shown promise towardsbuilding reliable machine learning systems against $\ell_p$-norm boundedattacks. However, existing methods are insufficient or unable to provablydefend against semantic transformations, especially those without closed-formexpressions (such as defocus blur and pixelate), which are more common inpractice and often unrestricted. To fill up this gap, we propose generalizedrandomized smoothing (GSmooth), a unified theoretical framework for certifyingrobustness against general semantic transformations via a novel dimensionaugmentation strategy. Under the GSmooth framework, we present a scalablealgorithm that uses a surrogate image-to-image network to approximate thecomplex transformation. The surrogate model provides a powerful tool forstudying the properties of semantic transformations and certifying robustness.Experimental results on several datasets demonstrate the effectiveness of ourapproach for robustness certification against multiple kinds of semantictransformations and corruptions, which is not achievable by the alternativebaselines.",0
"A law of adversarial risk, interpolation, and label noise In supervised learning, it has been shown that label noise in the data can beinterpolated without penalties on test accuracy under many circumstances. Weshow that interpolating label noise induces adversarial vulnerability, andprove the first theorem showing the dependence of label noise and adversarialrisk in terms of the data distribution. Our results are almost sharp withoutaccounting for the inductive bias of the learning algorithm. We also show thatinductive bias makes the effect of label noise much stronger.",0
"Back to the Source: Diffusion-Driven Test-Time Adaptation Test-time adaptation harnesses test inputs to improve the accuracy of a modeltrained on source data when tested on shifted target data. Existing methodsupdate the source model by (re-)training on each target domain. Whileeffective, re-training is sensitive to the amount and order of the data and thehyperparameters for optimization. We instead update the target data, byprojecting all test inputs toward the source domain with a generative diffusionmodel. Our diffusion-driven adaptation method, DDA, shares its models forclassification and generation across all domains. Both models are trained onthe source domain, then fixed during testing. We augment diffusion with imageguidance and self-ensembling to automatically decide how much to adapt. Inputadaptation by DDA is more robust than prior model adaptation approaches acrossa variety of corruptions, architectures, and data regimes on the ImageNet-Cbenchmark. With its input-wise updates, DDA succeeds where model adaptationdegrades on too little data in small batches, dependent data in non-uniformorder, or mixed data with multiple corruptions.",0
"Certified Defenses against Adversarial Examples While neural networks have achieved high accuracy on standard imageclassification benchmarks, their accuracy drops to nearly zero in the presenceof small adversarial perturbations to test inputs. Defenses based onregularization and adversarial training have been proposed, but often followedby new, stronger attacks that defeat these defenses. Can we somehow end thisarms race? In this work, we study this problem for neural networks with onehidden layer. We first propose a method based on a semidefinite relaxation thatoutputs a certificate that for a given network and test input, no attack canforce the error to exceed a certain value. Second, as this certificate isdifferentiable, we jointly optimize it with the network parameters, providingan adaptive regularizer that encourages robustness against all attacks. OnMNIST, our approach produces a network and a certificate that no attack thatperturbs each pixel by at most \epsilon = 0.1 can cause more than 35% testerror.",0
"Motivating the Rules of the Game for Adversarial Example Research Advances in machine learning have led to broad deployment of systems withimpressive performance on important problems. Nonetheless, these systems can beinduced to make errors on data that are surprisingly similar to examples thelearned system handles correctly. The existence of these errors raises avariety of questions about out-of-sample generalization and whether bad actorsmight use such examples to abuse deployed systems. As a result of thesesecurity concerns, there has been a flurry of recent papers proposingalgorithms to defend against such malicious perturbations of correctly handledexamples. It is unclear how such misclassifications represent a different kindof security problem than other errors, or even other attacker-produced examplesthat have no specific relationship to an uncorrupted input. In this paper, weargue that adversarial example defense papers have, to date, mostly consideredabstract, toy games that do not relate to any specific security concern.Furthermore, defense papers have not yet precisely described all the abilitiesand limitations of attackers that would be relevant in practical security.Towards this end, we establish a taxonomy of motivations, constraints, andabilities for more plausible adversaries. Finally, we provide a series ofrecommendations outlining a path forward for future work to more clearlyarticulate the threat model and perform more meaningful evaluation.",0
"Demystifying the Adversarial Robustness of Random Transformation Defenses Neural networks' lack of robustness against attacks raises concerns insecurity-sensitive settings such as autonomous vehicles. While manycountermeasures may look promising, only a few withstand rigorous evaluation.Defenses using random transformations (RT) have shown impressive results,particularly BaRT (Raff et al., 2019) on ImageNet. However, this type ofdefense has not been rigorously evaluated, leaving its robustness propertiespoorly understood. Their stochastic properties make evaluation more challengingand render many proposed attacks on deterministic models inapplicable. First,we show that the BPDA attack (Athalye et al., 2018a) used in BaRT's evaluationis ineffective and likely overestimates its robustness. We then attempt toconstruct the strongest possible RT defense through the informed selection oftransformations and Bayesian optimization for tuning their parameters.Furthermore, we create the strongest possible attack to evaluate our RTdefense. Our new attack vastly outperforms the baseline, reducing the accuracyby 83% compared to the 19% reduction by the commonly used EoT attack($4.3\times$ improvement). Our result indicates that the RT defense on theImagenette dataset (a ten-class subset of ImageNet) is not robust againstadversarial examples. Extending the study further, we use our new attack toadversarially train RT defense (called AdvRT), resulting in a large robustnessgain. Code is available at",0
"Using Pre-Training Can Improve Model Robustness and Uncertainty He et al. (2018) have called into question the utility of pre-training byshowing that training from scratch can often yield similar performance topre-training. We show that although pre-training may not improve performance ontraditional classification metrics, it improves model robustness anduncertainty estimates. Through extensive experiments on adversarial examples,label corruption, class imbalance, out-of-distribution detection, andconfidence calibration, we demonstrate large gains from pre-training andcomplementary effects with task-specific methods. We introduce adversarialpre-training and show approximately a 10% absolute improvement over theprevious state-of-the-art in adversarial robustness. In some cases, usingpre-training without task-specific methods also surpasses the state-of-the-art,highlighting the need for pre-training when evaluating future methods onrobustness and uncertainty tasks.",0
"Gradient-based Adversarial Attacks against Text Transformers We propose the first general-purpose gradient-based attack againsttransformer models. Instead of searching for a single adversarial example, wesearch for a distribution of adversarial examples parameterized by acontinuous-valued matrix, hence enabling gradient-based optimization. Weempirically demonstrate that our white-box attack attains state-of-the-artattack performance on a variety of natural language tasks. Furthermore, we showthat a powerful black-box transfer attack, enabled by sampling from theadversarial distribution, matches or exceeds existing methods, while onlyrequiring hard-label outputs.",0
"Formulating Robustness Against Unforeseen Attacks Existing defenses against adversarial examples such as adversarial trainingtypically assume that the adversary will conform to a specific or known threatmodel, such as $\ell_p$ perturbations within a fixed budget. In this paper, wefocus on the scenario where there is a mismatch in the threat model assumed bythe defense during training, and the actual capabilities of the adversary attest time. We ask the question: if the learner trains against a specificsource threat model, when can we expect robustness to generalize to astronger unknown target threat model during test-time? Our key contributionis to formally define the problem of learning and generalization with anunforeseen adversary, which helps us reason about the increase in adversarialrisk from the conventional perspective of a known adversary. Applying ourframework, we derive a generalization bound which relates the generalizationgap between source and target threat models to variation of the featureextractor, which measures the expected maximum difference between extractedfeatures across a given threat model. Based on our generalization bound, wepropose adversarial training with variation regularization (AT-VR) whichreduces variation of the feature extractor across the source threat modelduring training. We empirically demonstrate that AT-VR can lead to improvedgeneralization to unforeseen attacks during test-time compared to standardadversarial training on Gaussian and image datasets.",0
"Natural Adversarial Examples We introduce two challenging datasets that reliably cause machine learningmodel performance to substantially degrade. The datasets are collected with asimple adversarial filtration technique to create datasets with limitedspurious cues. Our datasets' real-world, unmodified examples transfer tovarious unseen models reliably, demonstrating that computer vision models haveshared weaknesses. The first dataset is called ImageNet-A and is like theImageNet test set, but it is far more challenging for existing models. We alsocurate an adversarial out-of-distribution detection dataset called ImageNet-O,which is the first out-of-distribution detection dataset created for ImageNetmodels. On ImageNet-A a DenseNet-121 obtains around 2% accuracy, an accuracydrop of approximately 90%, and its out-of-distribution detection performance onImageNet-O is near random chance levels. We find that existing dataaugmentation techniques hardly boost performance, and using other publictraining datasets provides improvements that are limited. However, we find thatimprovements to computer vision architectures provide a promising path towardsrobust models.",0
"Smooth Adversarial Training It is commonly believed that networks cannot be both accurate and robust,that gaining robustness means losing accuracy. It is also generally believedthat, unless making networks larger, network architectural elements wouldotherwise matter little in improving adversarial robustness. Here we presentevidence to challenge these common beliefs by a careful study about adversarialtraining. Our key observation is that the widely-used ReLU activation functionsignificantly weakens adversarial training due to its non-smooth nature. Hencewe propose smooth adversarial training (SAT), in which we replace ReLU with itssmooth approximations to strengthen adversarial training. The purpose of smoothactivation functions in SAT is to allow it to find harder adversarial examplesand compute better gradient updates during adversarial training.",0
"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness Convolutional Neural Networks (CNNs) are commonly thought to recogniseobjects by learning increasingly complex representations of object shapes. Somerecent studies suggest a more important role of image textures. We here putthese conflicting hypotheses to a quantitative test by evaluating CNNs andhuman observers on images with a texture-shape cue conflict. We show thatImageNet-trained CNNs are strongly biased towards recognising textures ratherthan shapes, which is in stark contrast to human behavioural evidence andreveals fundamentally different classification strategies. We then demonstratethat the same standard architecture (ResNet-50) that learns a texture-basedrepresentation on ImageNet is able to learn a shape-based representationinstead when trained on Stylized-ImageNet, a stylized version of ImageNet.This provides a much better fit for human behavioural performance in ourwell-controlled psychophysical lab setting (nine experiments totalling 48,560psychophysical trials across 97 observers) and comes with a number ofunexpected emergent benefits such as improved object detection performance andpreviously unseen robustness towards a wide range of image distortions,highlighting advantages of a shape-based representation.",0
"Fast AdvProp Adversarial Propagation (AdvProp) is an effective way to improve recognitionmodels, leveraging adversarial examples. Nonetheless, AdvProp suffers from theextremely slow training speed, mainly because: a) extra forward and backwardpasses are required for generating adversarial examples; b) both originalsamples and their adversarial counterparts are used for training (i.e.,2$\times$ data). In this paper, we introduce Fast AdvProp, which aggressivelyrevamps AdvProp's costly training components, rendering the method nearly ascheap as the vanilla training. Specifically, our modifications in Fast AdvPropare guided by the hypothesis that disentangled learning with adversarialexamples is the key for performance improvements, while other training recipes(e.g., paired clean and adversarial training samples, multi-step adversarialattackers) could be largely simplified.",0
"Smooth-Reduce: Leveraging Patches for Improved Certified Robustness Randomized smoothing (RS) has been shown to be a fast, scalable technique forcertifying the robustness of deep neural network classifiers. However, methodsbased on RS require augmenting data with large amounts of noise, which leads tosignificant drops in accuracy. We propose a training-free, modified smoothingapproach, Smooth-Reduce, that leverages patching and aggregation to provideimproved classifier certificates. Our algorithm classifies overlapping patchesextracted from an input image, and aggregates the predicted logits to certify alarger radius around the input. We study two aggregation schemes -- max andmean -- and show that both approaches provide better certificates in terms ofcertified accuracy, average certified radii and abstention rates as compared toconcurrent approaches. We also provide theoretical guarantees for suchcertificates, and empirically show significant improvements over otherrandomized smoothing methods that require expensive retraining. Further, weextend our approach to videos and provide meaningful certificates for videoclassifiers. A project page can be found at",0
"Intrinsic dimension estimation for discrete metrics Real world-datasets characterized by discrete features are ubiquitous: fromcategorical surveys to clinical questionnaires, from unweighted networks to DNAsequences. Nevertheless, the most common unsupervised dimensional reductionmethods are designed for continuous spaces, and their use for discrete spacescan lead to errors and biases. In this letter we introduce an algorithm toinfer the intrinsic dimension (ID) of datasets embedded in discrete spaces. Wedemonstrate its accuracy on benchmark datasets, and we apply it to analyze ametagenomic dataset for species fingerprinting, finding a surprisingly smallID, of order 2. This suggests that evolutive pressure acts on a low-dimensionalmanifold despite the high-dimensionality of sequences' space.",0
"Forecasting Future World Events with Neural Networks Forecasting future world events is a challenging but valuable task. Forecastsof climate, geopolitical conflict, pandemics and economic indicators help shapepolicy and decision making. In these domains, the judgment of expert humanscontributes to the best forecasts. Given advances in language modeling, canthese forecasts be automated? To this end, we introduce Autocast, a datasetcontaining thousands of forecasting questions and an accompanying news corpus.Questions are taken from forecasting tournaments, ensuring high quality,real-world importance, and diversity. The news corpus is organized by date,allowing us to precisely simulate the conditions under which humans made pastforecasts (avoiding leakage from the future). Motivated by the difficulty offorecasting numbers across orders of magnitude (e.g. global cases of COVID-19in 2022), we also curate IntervalQA, a dataset of numerical questions andmetrics for calibration. We test language models on our forecasting task andfind that performance is far below a human expert baseline. However,performance improves with increased model size and incorporation of relevantinformation from the news corpus. In sum, Autocast poses a novel challenge forlarge language models and improved performance could bring large practicalbenefits.",0
"Anomal-E: A Self-Supervised Network Intrusion Detection System based on Graph Neural Networks This paper investigates Graph Neural Networks (GNNs) application forself-supervised network intrusion and anomaly detection. GNNs are a deeplearning approach for graph-based data that incorporate graph structures intolearning to generalise graph representations and output embeddings. As networkflows are naturally graph-based, GNNs are a suitable fit for analysing andlearning network behaviour. The majority of current implementations ofGNN-based Network Intrusion Detection Systems (NIDSs) rely heavily on labellednetwork traffic which can not only restrict the amount and structure of inputtraffic, but also the NIDSs potential to adapt to unseen attacks. To overcomethese restrictions, we present Anomal-E, a GNN approach to intrusion andanomaly detection that leverages edge features and graph topological structurein a self-supervised process. This approach is, to the best our knowledge, thefirst successful and practical approach to network intrusion detection thatutilises network flows in a self-supervised, edge leveraging GNN. Experimentalresults on two modern benchmark NIDS datasets not only clearly display theimprovement of using Anomal-E embeddings rather than raw features, but also thepotential Anomal-E has for detection on wild network traffic.",0
"Forecasting Future World Events with Neural Networks Forecasting future world events is a challenging but valuable task. Forecastsof climate, geopolitical conflict, pandemics and economic indicators help shapepolicy and decision making. In these domains, the judgment of expert humanscontributes to the best forecasts. Given advances in language modeling, canthese forecasts be automated? To this end, we introduce Autocast, a datasetcontaining thousands of forecasting questions and an accompanying news corpus.Questions are taken from forecasting tournaments, ensuring high quality,real-world importance, and diversity. The news corpus is organized by date,allowing us to precisely simulate the conditions under which humans made pastforecasts (avoiding leakage from the future). Motivated by the difficulty offorecasting numbers across orders of magnitude (e.g. global cases of COVID-19in 2022), we also curate IntervalQA, a dataset of numerical questions andmetrics for calibration. We test language models on our forecasting task andfind that performance is far below a human expert baseline. However,performance improves with increased model size and incorporation of relevantinformation from the news corpus. In sum, Autocast poses a novel challenge forlarge language models and improved performance could bring large practicalbenefits.",0
"Generalized Beliefs for Cooperative AI Self-play is a common paradigm for constructing solutions in Markov gamesthat can yield optimal policies in collaborative settings. However, thesepolicies often adopt highly-specialized conventions that make playing with anovel partner difficult. To address this, recent approaches rely on encodingsymmetry and convention-awareness into policy training, but these requirestrong environmental assumptions and can complicate policy training. Wetherefore propose moving the learning of conventions to the belief space.Specifically, we propose a belief learning model that can maintain beliefs overrollouts of policies not seen at training time, and can thus decode and adaptto novel conventions at test time. We show how to leverage this model for bothsearch and training of a best response over various pools of policies togreatly improve ad-hoc teamplay. We also show how our setup promotesexplainability and interpretability of nuanced agent conventions.",0
"Robustness Evaluation of Deep Unsupervised Learning Algorithms for Intrusion Detection Systems Recently, advances in deep learning have been observed in various fields,including computer vision, natural language processing, and cybersecurity.Machine learning (ML) has demonstrated its ability as a potential tool foranomaly detection-based intrusion detection systems to build secure computernetworks. Increasingly, ML approaches are widely adopted than heuristicapproaches for cybersecurity because they learn directly from data. Data iscritical for the development of ML systems, and becomes potential targets forattackers. Basically, data poisoning or contamination is one of the most commontechniques used to fool ML models through data. This paper evaluates therobustness of six recent deep learning algorithms for intrusion detection oncontaminated data. Our experiments suggest that the state-of-the-art algorithmsused in this study are sensitive to data contamination and reveal theimportance of self-defense against data perturbation when developing novelmodels, especially for intrusion detection systems.",0
"Developing Optimal Causal Cyber-Defence Agents via Cyber Security Simulation In this paper we explore cyber security defence, through the unification of anovel cyber security simulator with models for (causal) decision-making throughoptimisation. Particular attention is paid to a recently published approach:dynamic causal Bayesian optimisation (DCBO). We propose that DCBO can act as ablue agent when provided with a view of a simulated network and a causal modelof how a red agent spreads within that network. To investigate how DCBO canperform optimal interventions on host nodes, in order to reduce the cost ofintrusions caused by the red agent. Through this we demonstrate a completecyber-simulation system, which we use to generate observational data for DCBOand provide numerical quantitative results which lay the foundations for futurework in this space.",0
"Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions There is burgeoning interest in designing AI-based systems to assist humansin designing computing systems, including tools that automatically generatecomputer code. The most notable of these comes in the form of the firstself-described `AI pair programmer', GitHub Copilot, a language model trainedover open-source GitHub code. However, code often contains bugs - and so, giventhe vast quantity of unvetted code that Copilot has processed, it is certainthat the language model will have learned from exploitable, buggy code. Thisraises concerns on the security of Copilot's code contributions. In this work,we systematically investigate the prevalence and conditions that can causeGitHub Copilot to recommend insecure code. To perform this analysis we promptCopilot to generate code in scenarios relevant to high-risk CWEs (e.g. thosefrom MITRE's Top 25 list). We explore Copilot's performance on three distinctcode generation axes -- examining how it performs given diversity ofweaknesses, diversity of prompts, and diversity of domains. In total, weproduce 89 different scenarios for Copilot to complete, producing 1,689programs. Of these, we found approximately 40% to be vulnerable.",0
"Open Problems in Cooperative AI Problems of cooperation--in which agents seek ways to jointly improve theirwelfare--are ubiquitous and important. They can be found at scales ranging fromour daily routines--such as driving on highways, scheduling meetings, andworking collaboratively--to our global challenges--such as peace, commerce, andpandemic preparedness. Arguably, the success of the human species is rooted inour ability to cooperate. Since machines powered by artificial intelligence areplaying an ever greater role in our lives, it will be important to equip themwith the capabilities necessary to cooperate and to foster cooperation.",0
"AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly Detection Analyzing the distribution shift of data is a growing research direction innowadays Machine Learning, leading to emerging new benchmarks that focus onproviding a suitable scenario for studying the generalization properties of MLmodels. The existing benchmarks are focused on supervised learning, and to thebest of our knowledge, there is none for unsupervised learning. Therefore, weintroduce an unsupervised anomaly detection benchmark with data that shiftsover time, built over Kyoto-2006+, a traffic dataset for network intrusiondetection. This kind of data meets the premise of shifting the inputdistribution: it covers a large time span ($10$ years), with naturallyoccurring changes over time (\eg users modifying their behavior patterns, andsoftware updates). We first highlight the non-stationary nature of the data,using a basic per-feature analysis, t-SNE, and an Optimal Transport approachfor measuring the overall distribution distances between years. Next, wepropose AnoShift, a protocol splitting the data in IID, NEAR, and FAR testingsplits. We validate the performance degradation over time with diverse models(MLM to classical Isolation Forest). Finally, we show that by acknowledging thedistribution shift problem and properly addressing it, the performance can beimproved compared to the classical IID training (by up to $3\%$, on average).Dataset and code are available at",0
"Goal-Oriented Sensitivity Analysis of Hyperparameters in Deep Learning Tackling new machine learning problems with neural networks always meansoptimizing numerous hyperparameters that define their structure and stronglyimpact their performances. In this work, we study the use of goal-orientedsensitivity analysis, based on the Hilbert-Schmidt Independence Criterion(HSIC), for hyperparameter analysis and optimization. Hyperparameters live inspaces that are often complex and awkward. They can be of different natures(categorical, discrete, boolean, continuous), interact, and haveinter-dependencies. All this makes it non-trivial to perform classicalsensitivity analysis. We alleviate these difficulties to obtain a robustanalysis index that is able to quantify hyperparameters' relative impact on aneural network's final error. This valuable tool allows us to better understandhyperparameters and to make hyperparameter optimization more interpretable. Weillustrate the benefits of this knowledge in the context of hyperparameteroptimization and derive an HSIC-based optimization algorithm that we apply onMNIST and Cifar, classical machine learning data sets, but also on theapproximation of Runge function and Bateman equations solution, of interest forscientific machine learning. This method yields neural networks that are bothcompetitive and cost-effective.",1
"Do Not Sleep on Linear Models: Simple and Interpretable Techniques Outperform Deep Learning for Sleep Scoring Over the last few years, research in automatic sleep scoring has mainlyfocused on developing increasingly complex deep learning architectures.However, recently these approaches achieved only marginal improvements, oftenat the expense of requiring more data and more expensive training procedures.Despite all these efforts and their satisfactory performance, automatic sleepstaging solutions are not widely adopted in a clinical context yet. We arguethat most deep learning solutions for sleep scoring are limited in theirreal-world applicability as they are hard to train, deploy, and reproduce.Moreover, these solutions lack interpretability and transparency, which areoften key to increase adoption rates. In this work, we revisit the problem ofsleep stage classification using classical machine learning. Results show thatstate-of-the-art performance can be achieved with a conventional machinelearning pipeline consisting of preprocessing, feature extraction, and a simplemachine learning model. In particular, we analyze the performance of a linearmodel and a non-linear (gradient boosting) model. Our approach surpassesstate-of-the-art (that uses the same data) on two public datasets: Sleep-EDFSC-20 (MF1 0.810) and Sleep-EDF ST (MF1 0.795), while achieving competitiveresults on Sleep-EDF SC-78 (MF1 0.775) and MASS SS3 (MF1 0.817). We show that,for the sleep stage scoring task, the expressiveness of an engineered featurevector is on par with the internally learned representations of deep learningmodels. This observation opens the door to clinical adoption, as arepresentative feature vector allows to leverage both the interpretability andsuccessful track record of traditional machine learning models.",1
"Unsupervised learning of observation functions in state-space models by nonparametric moment methods We investigate the unsupervised learning of non-invertible observationfunctions in nonlinear state-space models. Assuming abundant data of theobservation process along with the distribution of the state process, weintroduce a nonparametric generalized moment method to estimate the observationfunction via constrained regression. The major challenge comes from thenon-invertibility of the observation function and the lack of data pairsbetween the state and observation. We address the fundamental issue ofidentifiability from quadratic loss functionals and show that the functionspace of identifiability is the closure of a RKHS that is intrinsic to thestate process. Numerical results show that the first two moments and temporalcorrelations, along with upper and lower bounds, can identify functions rangingfrom piecewise polynomials to smooth functions, leading to convergentestimators. The limitations of this method, such as non-identifiability due tosymmetry and stationarity, are also discussed.",1
"Discrimination in machine learning algorithms Machine learning algorithms are routinely used for business decisions thatmay directly affect individuals, for example, because a credit scoringalgorithm refuses them a loan. It is then relevant from an ethical (and legal)point of view to ensure that these algorithms do not discriminate based onsensitive attributes (like sex or race), which may occur unwittingly andunknowingly by the operator and the management. Statistical tools and methodsare then required to detect and eliminate such potential biases.",1
"Approximation Power of Deep Neural Networks: an explanatory mathematical survey The goal of this survey is to present an explanatory review of theapproximation properties of deep neural networks. Specifically, we aim atunderstanding how and why deep neural networks outperform other classicallinear and nonlinear approximation methods. This survey consists of threechapters. In Chapter 1 we review the key ideas and concepts underlying deepnetworks and their compositional nonlinear structure. We formalize the neuralnetwork problem by formulating it as an optimization problem when solvingregression and classification problems. We briefly discuss the stochasticgradient descent algorithm and the back-propagation formulas used in solvingthe optimization problem and address a few issues related to the performance ofneural networks, including the choice of activation functions, cost functions,overfitting issues, and regularization. In Chapter 2 we shift our focus to theapproximation theory of neural networks. We start with an introduction to theconcept of density in polynomial approximation and in particular study theStone-Weierstrass theorem for real-valued continuous functions. Then, withinthe framework of linear approximation, we review a few classical results on thedensity and convergence rate of feedforward networks, followed by more recentdevelopments on the complexity of deep networks in approximating Sobolevfunctions. In Chapter 3, utilizing nonlinear approximation theory, we furtherelaborate on the power of depth and approximation superiority of deep ReLUnetworks over other classical methods of nonlinear approximation.",1
"Conservative SPDEs as fluctuating mean field limits of stochastic gradient descent The convergence of stochastic interacting particle systems in the mean-fieldlimit to solutions to conservative stochastic partial differential equations isshown, with optimal rate of convergence. As a second main result, aquantitative central limit theorem for such SPDEs is derived, again withoptimal rate of convergence.",1
"A New Index for Clustering Evaluation Based on Density Estimation A new index for internal evaluation of clustering is introduced. The index isdefined as a mixture of two sub-indices. The first sub-index $ I_a $ is calledthe Ambiguous Index; the second sub-index $ I_s $ is called the SimilarityIndex. Calculation of the two sub-indices is based on density estimation toeach cluster of a partition of the data. An experiment is conducted to test theperformance of the new index, and compared with three popular internalclustering evaluation indices -- Calinski-Harabasz index, Silhouettecoefficient, and Davies-Bouldin index, on a set of 145 datasets. The resultshows the new index improves the three popular indices by 59%, 34%, and 74%,correspondingly.",1
"Deep Sufficient Representation Learning via Mutual Information We propose a mutual information-based sufficient representation learning(MSRL) approach, which uses the variational formulation of the mutualinformation and leverages the approximation power of deep neural networks. MSRLlearns a sufficient representation with the maximum mutual information with theresponse and a user-selected distribution. It can easily handlemulti-dimensional continuous or categorical response variables. MSRL is shownto be consistent in the sense that the conditional probability density functionof the response variable given the learned representation converges to theconditional probability density function of the response variable given thepredictor. Non-asymptotic error bounds for MSRL are also established undersuitable conditions. To establish the error bounds, we derive a generalizedDudley's inequality for an order-two U-process indexed by deep neural networks,which may be of independent interest. We discuss how to determine the intrinsicdimension of the underlying data distribution. Moreover, we evaluate theperformance of MSRL via extensive numerical experiments and real data analysisand demonstrate that MSRL outperforms some existing nonlinear sufficientdimension reduction methods.",1
"BiTAT: Neural Network Binarization with Task-dependent Aggregated Transformation Neural network quantization aims to transform high-precision weights andactivations of a given neural network into low-precision weights/activationsfor reduced memory usage and computation, while preserving the performance ofthe original model. However, extreme quantization (1-bit weight/1-bitactivations) of compactly-designed backbone architectures (e.g., MobileNets)often used for edge-device deployments results in severe performancedegeneration. This paper proposes a novel Quantization-Aware Training (QAT)method that can effectively alleviate performance degeneration even withextreme quantization by focusing on the inter-weight dependencies, between theweights within each layer and across consecutive layers. To minimize thequantization impact of each weight on others, we perform an orthonormaltransformation of the weights at each layer by training an input-dependentcorrelation matrix and importance vector, such that each weight is disentangledfrom the others. Then, we quantize the weights based on their importance tominimize the loss of the information from the original weights/activations. Wefurther perform progressive layer-wise quantization from the bottom layer tothe top, so that quantization at each layer reflects the quantizeddistributions of weights and activations at previous layers. We validate theeffectiveness of our method on various benchmark datasets against strong neuralquantization baselines, demonstrating that it alleviates the performancedegeneration on ImageNet and successfully preserves the full-precision modelperformance on CIFAR-100 with compact backbone networks.",1
"VTrackIt: A Synthetic Self-Driving Dataset with Infrastructure and Pooled Vehicle Information Artificial intelligence solutions for Autonomous Vehicles (AVs) have beendeveloped using publicly available datasets such as Argoverse, ApolloScape,Level5, and NuScenes. One major limitation of these datasets is the absence ofinfrastructure and/or pooled vehicle information like lane line type, vehiclespeed, traffic signs, and intersections. Such information is necessary and notcomplementary to eliminating high-risk edge cases. The rapid advancements inVehicle-to-Infrastructure and Vehicle-to-Vehicle technologies show promise thatinfrastructure and pooled vehicle information will soon be accessible in nearreal-time. Taking a leap in the future, we introduce the first comprehensivesynthetic dataset with intelligent infrastructure and pooled vehicleinformation for advancing the next generation of AVs, named VTrackIt. We alsointroduce the first deep learning model (InfraGAN) for trajectory predictionsthat considers such information. Our experiments with InfraGAN show that thecomprehensive information offered by VTrackIt reduces the number of high-riskedge cases. The VTrackIt dataset is available upon request under the CreativeCommons CC BY-NC-SA 4.0 license at http://vtrackit.irda.club.",1
"The Union of Manifolds Hypothesis and its Implications for Deep Generative Modelling Deep learning has had tremendous success at learning low-dimensionalrepresentations of high-dimensional data. This success would be impossible ifthere was no hidden low-dimensional structure in data of interest; thisexistence is posited by the manifold hypothesis, which states that the datalies on an unknown manifold of low intrinsic dimension. In this paper, we arguethat this hypothesis does not properly capture the low-dimensional structuretypically present in data. Assuming the data lies on a single manifold impliesintrinsic dimension is identical across the entire data space, and does notallow for subregions of this space to have a different number of factors ofvariation. To address this deficiency, we put forth the union of manifoldshypothesis, which accommodates the existence of non-constant intrinsicdimensions. We empirically verify this hypothesis on commonly-used imagedatasets, finding that indeed, intrinsic dimension should be allowed to vary.We also show that classes with higher intrinsic dimensions are harder toclassify, and how this insight can be used to improve classification accuracy.We then turn our attention to the impact of this hypothesis in the context ofdeep generative models (DGMs). Most current DGMs struggle to model datasetswith several connected components and/or varying intrinsic dimensions. Totackle these shortcomings, we propose clustered DGMs, where we first clusterthe data and then train a DGM on each cluster. We show that clustered DGMs canmodel multiple connected components with different intrinsic dimensions, andempirically outperform their non-clustered counterparts without increasingcomputational requirements.",1
"A Forward Propagation Algorithm for Online Optimization of Nonlinear Stochastic Differential Equations Optimizing over the stationary distribution of stochastic differentialequations (SDEs) is computationally challenging. A new forward propagationalgorithm has been recently proposed for the online optimization of SDEs. Thealgorithm solves an SDE, derived using forward differentiation, which providesa stochastic estimate for the gradient. The algorithm continuously updates theSDE model's parameters and the gradient estimate simultaneously. This paperstudies the convergence of the forward propagation algorithm for nonlineardissipative SDEs. We leverage the ergodicity of this class of nonlinear SDEs tocharacterize the convergence rate of the transition semi-group and itsderivatives. Then, we prove bounds on the solution of a Poisson partialdifferential equation (PDE) for the expected time integral of the algorithm'sstochastic fluctuations around the direction of steepest descent. We thenre-write the algorithm using the PDE solution, which allows us to characterizethe parameter evolution around the direction of steepest descent. Our mainresult is a convergence theorem for the forward propagation algorithm fornonlinear dissipative SDEs.",1
"Subgraph Frequency Distribution Estimation using Graph Neural Networks Small subgraphs (graphlets) are important features to describe fundamentalunits of a large network. The calculation of the subgraph frequencydistributions has a wide application in multiple domains including biology andengineering. Unfortunately due to the inherent complexity of this task, most ofthe existing methods are computationally intensive and inefficient. In thiswork, we propose GNNS, a novel representational learning framework thatutilizes graph neural networks to sample subgraphs efficiently for estimatingtheir frequency distribution. Our framework includes an inference model and agenerative model that learns hierarchical embeddings of nodes, subgraphs, andgraph types. With the learned model and embeddings, subgraphs are sampled in ahighly scalable and parallel way and the frequency distribution estimation isthen performed based on these sampled subgraphs. Eventually, our methodsachieve comparable accuracy and a significant speedup by three orders ofmagnitude compared to existing methods.",1
"Employing Feature Selection Algorithms to Determine the Immune State of Mice with Rheumatoid Arthritis The immune response is a dynamic process by which the body determines whetheran antigen is self or nonself. The state of this dynamic process is defined bythe relative balance and population of inflammatory and regulatory actors whichcomprise this decision making process. The goal of immunotherapy as applied to,e.g. Rheumatoid Arthritis (RA), then, is to bias the immune state in favor ofthe regulatory actors - thereby shutting down autoimmune pathways in theresponse. While there are several known approaches to immunotherapy, theeffectiveness of the therapy will depend on how this intervention alters theevolution of this state. Unfortunately, this process is determined not only bythe dynamics of the process, but the state of the system at the time ofintervention - a state which is difficult if not impossible to determine priorto application of the therapy.",1
"Modeling Randomly Walking Volatility with Chained Gamma Distributions Volatility clustering is a common phenomenon in financial time series.Typically, linear models can be used to describe the temporal autocorrelationof the (logarithmic) variance of returns. Considering the difficulty inestimating this model, we construct a Dynamic Bayesian Network, which utilizesthe conjugate prior relation of normal-gamma and gamma-gamma, so that itsposterior form locally remains unchanged at each node. This makes it possibleto find approximate solutions using variational methods quickly. Furthermore,we ensure that the volatility expressed by the model is an independentincremental process after inserting dummy gamma nodes between adjacent timesteps. We have found that this model has two advantages: 1) It can be provedthat it can express heavier tails than Gaussians, i.e., have positive excesskurtosis, compared to popular linear models. 2) If the variationalinference(VI) is used for state estimation, it runs much faster than MonteCarlo(MC) methods since the calculation of the posterior uses only basicarithmetic operations. And its convergence process is deterministic.",1
"Data-Driven Stochastic AC-OPF using Gaussian Processes In recent years, electricity generation has been responsible for more than aquarter of the greenhouse gas emissions in the US. Integrating a significantamount of renewables into a power grid is probably the most accessible way toreduce carbon emissions from power grids and slow down climate change.Unfortunately, the most accessible renewable power sources, such as wind andsolar, are highly fluctuating and thus bring a lot of uncertainty to power gridoperations and challenge existing optimization and control policies. Thechance-constrained alternating current (AC) optimal power flow (OPF) frameworkfinds the minimum cost generation dispatch maintaining the power gridoperations within security limits with a prescribed probability. Unfortunately,the AC-OPF problem's chance-constrained extension is non-convex,computationally challenging, and requires knowledge of system parameters andadditional assumptions on the behavior of renewable distribution. Known linearand convex approximations to the above problems, though tractable, are tooconservative for operational practice and do not consider uncertainty in systemparameters. This paper presents an alternative data-driven approach based onGaussian process (GP) regression to close this gap. The GP approach learns asimple yet non-convex data-driven approximation to the AC power flow equationsthat can incorporate uncertainty inputs. The latter is then used to determinethe solution of CC-OPF efficiently, by accounting for both input and parameteruncertainty. The practical efficiency of the proposed approach using differentapproximations for GP-uncertainty propagation is illustrated over numerous IEEEtest cases.",1
"Statistical Hypothesis Testing Based on Machine Learning: Large Deviations Analysis We study the performance -- and specifically the rate at which the errorprobability converges to zero -- of Machine Learning (ML) classificationtechniques. Leveraging the theory of large deviations, we provide themathematical conditions for a ML classifier to exhibit error probabilities thatvanish exponentially, say $\sim \exp\left(-n\,I + o(n) \right)$, where $n$ isthe number of informative observations available for testing (or anotherrelevant parameter, such as the size of the target in an image) and $I$ is theerror rate. Such conditions depend on the Fenchel-Legendre transform of thecumulant-generating function of the Data-Driven Decision Function (D3F, i.e.,what is thresholded before the final binary decision is made) learned in thetraining phase. As such, the D3F and, consequently, the related error rate $I$,depend on the given training set, which is assumed of finite size.Interestingly, these conditions can be verified and tested numericallyexploiting the available dataset, or a synthetic dataset, generated accordingto the available information on the underlying statistical model. In otherwords, the classification error probability convergence to zero and its ratecan be computed on a portion of the dataset available for training. Coherentlywith the large deviations theory, we can also establish the convergence, for$n$ large enough, of the normalized D3F statistic to a Gaussian distribution.This property is exploited to set a desired asymptotic false alarm probability,which empirically turns out to be accurate even for quite realistic values of$n$. Furthermore, approximate error probability curves $\sim \zeta_n\exp\left(-n\,I \right)$ are provided, thanks to the refined asymptoticderivation (often referred to as exact asymptotics), where $\zeta_n$ representsthe most representative sub-exponential terms of the error probabilities.",1
"When does SGD favor flat minima? A quantitative characterization via linear stability The observation that stochastic gradient descent (SGD) favors flat minima hasplayed a fundamental role in understanding implicit regularization of SGD andguiding the tuning of hyperparameters. In this paper, we provide a quantitativeexplanation of this striking phenomenon by relating the particular noisestructure of SGD to its \emph{linear stability} (Wu et al., 2018).Specifically, we consider training over-parameterized models with square loss.We prove that if a global minimum $\theta^*$ is linearly stable for SGD, thenit must satisfy $\|H(\theta^*)\|_F\leq O(\sqrt{B}/\eta)$, where$\|H(\theta^*)\|_F, B,\eta$ denote the Frobenius norm of Hessian at $\theta^*$,batch size, and learning rate, respectively. Otherwise, SGD will escape fromthat minimum \emph{exponentially} fast. Hence, for minima accessible to SGD,the flatness -- as measured by the Frobenius norm of the Hessian -- is boundedindependently of the model size and sample size. The key to obtaining theseresults is exploiting the particular geometry awareness of SGD noise: 1) thenoise magnitude is proportional to loss value; 2) the noise directionsconcentrate in the sharp directions of local landscape. This property of SGDnoise provably holds for linear networks and random feature models (RFMs) andis empirically verified for nonlinear networks. Moreover, the validity andpractical relevance of our theoretical findings are justified by extensivenumerical experiments.",1
"A Sublinear-Time Quantum Algorithm for Approximating Partition Functions We present a novel quantum algorithm for estimating Gibbs partition functionsin sublinear time with respect to the logarithm of the size of the state space.This is the first speed-up of this type to be obtained over the seminalnearly-linear time algorithm of tefankovi, Vempala and Vigoda [JACM,2009]. Our result also preserves the quadratic speed-up in precision andspectral gap achieved in previous work by exploiting the properties of quantumMarkov chains. As an application, we obtain new polynomial improvements overthe best-known algorithms for computing the partition function of the Isingmodel, and counting the number of $k$-colorings, matchings or independent setsof a graph.",1
"PASHA: Efficient HPO with Progressive Resource Allocation Hyperparameter optimization (HPO) and neural architecture search (NAS) aremethods of choice to obtain the best-in-class machine learning models, but inpractice they can be costly to run. When models are trained on large datasets,tuning them with HPO or NAS rapidly becomes prohibitively expensive forpractitioners, even when efficient multi-fidelity methods are employed. Wepropose an approach to tackle the challenge of tuning machine learning modelstrained on large datasets with limited computational resources. Our approach,named PASHA, is able to dynamically allocate maximum resources for the tuningprocedure depending on the need. The experimental comparison shows that PASHAidentifies well-performing hyperparameter configurations and architectureswhile consuming significantly fewer computational resources than solutions likeASHA.",1
"Switching One-Versus-the-Rest Loss to Increase the Margin of Logits for Adversarial Robustness Defending deep neural networks against adversarial examples is a keychallenge for AI safety. To improve the robustness effectively, recent methodsfocus on important data points near the decision boundary in adversarialtraining. However, these methods are vulnerable to Auto-Attack, which is anensemble of parameter-free attacks for reliable evaluation. In this paper, weexperimentally investigate the causes of their vulnerability and find thatexisting methods reduce margins between logits for the true label and the otherlabels while keeping their gradient norms non-small values. Reduced margins andnon-small gradient norms cause their vulnerability since the largest logit canbe easily flipped by the perturbation. Our experiments also show that thehistogram of the logit margins has two peaks, i.e., small and large logitmargins. From the observations, we propose switching one-versus-the-rest loss(SOVR), which uses one-versus-the-rest loss when data have small logit marginsso that it increases the margins. We find that SOVR increases logit marginsmore than existing methods while keeping gradient norms small and outperformsthem in terms of the robustness against Auto-Attack.",1
"Deeply-Learned Generalized Linear Models with Missing Data Deep Learning (DL) methods have dramatically increased in popularity inrecent years, with significant growth in their application to supervisedlearning problems in the biomedical sciences. However, the greater prevalenceand complexity of missing data in modern biomedical datasets presentsignificant challenges for DL methods. Here, we provide a formal treatment ofmissing data in the context of deeply learned generalized linear models, asupervised DL architecture for regression and classification problems. Wepropose a new architecture, \textit{dlglm}, that is one of the first to be ableto flexibly account for both ignorable and non-ignorable patterns ofmissingness in input features and response at training time. We demonstratethrough statistical simulation that our method outperforms existing approachesfor supervised learning tasks in the presence of missing not at random (MNAR)missingness. We conclude with a case study of a Bank Marketing dataset from theUCI Machine Learning Repository, in which we predict whether clients subscribedto a product based on phone survey data.",1
"Functional Generalized Empirical Likelihood Estimation for Conditional Moment Restrictions Important problems in causal inference, economics, and, more generally,robust machine learning can be expressed as conditional moment restrictions,but estimation becomes challenging as it requires solving a continuum ofunconditional moment restrictions. Previous works addressed this problem byextending the generalized method of moments (GMM) to continuum momentrestrictions. In contrast, generalized empirical likelihood (GEL) provides amore general framework and has been shown to enjoy favorable small-sampleproperties compared to GMM-based estimators. To benefit from recentdevelopments in machine learning, we provide a functional reformulation of GELin which arbitrary models can be leveraged. Motivated by a dual formulation ofthe resulting infinite dimensional optimization problem, we devise a practicalmethod and explore its asymptotic properties. Finally, we provide kernel- andneural network-based implementations of the estimator, which achievestate-of-the-art empirical performance on two conditional moment restrictionproblems.",1
"TCT: Convexifying Federated Learning using Bootstrapped Neural Tangent Kernels State-of-the-art federated learning methods can perform far worse than theircentralized counterparts when clients have dissimilar data distributions. Forneural networks, even when centralized SGD easily finds a solution that issimultaneously performant for all clients, current federated optimizationmethods fail to converge to a comparable solution. We show that thisperformance disparity can largely be attributed to optimization challengespresented by nonconvexity. Specifically, we find that the early layers of thenetwork do learn useful features, but the final layers fail to make use ofthem. That is, federated optimization applied to this non-convex problemdistorts the learning of the final layers. Leveraging this observation, wepropose a Train-Convexify-Train (TCT) procedure to sidestep this issue: first,learn features using off-the-shelf methods (e.g., FedAvg); then, optimize aconvexified problem obtained from the network's empirical neural tangent kernelapproximation. Our technique yields accuracy improvements of up to +36% onFMNIST and +37% on CIFAR10 when clients have dissimilar data.",1
"Recommendation Systems with Distribution-Free Reliability Guarantees When building recommendation systems, we seek to output a helpful set ofitems to the user. Under the hood, a ranking model predicts which of twocandidate items is better, and we must distill these pairwise comparisons intothe user-facing output. However, a learned ranking model is never perfect, sotaking its predictions at face value gives no guarantee that the user-facingoutput is reliable. Building from a pre-trained ranking model, we show how toreturn a set of items that is rigorously guaranteed to contain mostly gooditems. Our procedure endows any ranking model with rigorous finite-samplecontrol of the false discovery rate (FDR), regardless of the (unknown) datadistribution. Moreover, our calibration algorithm enables the easy andprincipled integration of multiple objectives in recommender systems. As anexample, we show how to optimize for recommendation diversity subject to auser-specified level of FDR control, circumventing the need to specify ad hocweights of a diversity loss against an accuracy loss. Throughout, we focus onthe problem of learning to rank a set of possible recommendations, evaluatingour methods on the Yahoo! Learning to Rank and MSMarco datasets.",1
"Plex: Towards Reliability using Pretrained Large Model Extensions A recent trend in artificial intelligence is the use of pretrained models forlanguage and vision tasks, which have achieved extraordinary performance butalso puzzling failures. Probing these models' abilities in diverse ways istherefore critical to the field. In this paper, we explore the reliability ofmodels, where we define a reliable model as one that not only achieves strongpredictive performance but also performs well consistently over manydecision-making tasks involving uncertainty (e.g., selective prediction, openset recognition), robust generalization (e.g., accuracy and proper scoringrules such as log-likelihood on in- and out-of-distribution datasets), andadaptation (e.g., active learning, few-shot uncertainty). We devise 10 types oftasks over 40 datasets in order to evaluate different aspects of reliability onboth vision and language domains. To improve reliability, we developed ViT-Plexand T5-Plex, pretrained large model extensions for vision and languagemodalities, respectively. Plex greatly improves the state-of-the-art acrossreliability tasks, and simplifies the traditional protocol as it improves theout-of-the-box performance and does not require designing scores or tuning themodel for each task. We demonstrate scaling effects over model sizes up to 1Bparameters and pretraining dataset sizes up to 4B examples. We also demonstratePlex's capabilities on challenging tasks including zero-shot open setrecognition, active learning, and uncertainty in conversational languageunderstanding.",1
"High dimensional stochastic linear contextual bandit with missing covariates Recent works in bandit problems adopted lasso convergence theory in thesequential decision-making setting. Even with fully observed contexts, thereare technical challenges that hinder the application of existing lassoconvergence theory: 1) proving the restricted eigenvalue condition underconditionally sub-Gaussian noise and 2) accounting for the dependence betweenthe context variables and the chosen actions. This paper studies the effect ofmissing covariates on regret for stochastic linear bandit algorithms. Our workprovides a high-probability upper bound on the regret incurred by the proposedalgorithm in terms of covariate sampling probabilities, showing that the regretdegrades due to missingness by at most $\zeta_{min}^2$, where $\zeta_{min}$ isthe minimum probability of observing covariates in the context vector. Weillustrate our algorithm for the practical application of experimental designfor collecting gene expression data by a sequential selection of classdiscriminating DNA probes.",1
"A Newton-CG based barrier method for finding a second-order stationary point of nonconvex conic optimization with complexity guarantees In this paper we consider finding an approximate second-order stationarypoint (SOSP) of nonconvex conic optimization that minimizes a twicedifferentiable function over the intersection of an affine subspace and aconvex cone. In particular, we propose a Newton-conjugate gradient (Newton-CG)based barrier method for finding an $(\epsilon,\sqrt{\epsilon})$-SOSP of thisproblem. Our method is not only implementable, but also achieves an iterationcomplexity of ${\cal O}(\epsilon^{-3/2})$, which matches the best knowniteration complexity of second-order methods for finding an$(\epsilon,\sqrt{\epsilon})$-SOSP of unconstrained nonconvex optimization. Theoperation complexity of $\widetilde{\calO}(\epsilon^{-3/2}\min\{n,\epsilon^{-1/4}\})$, measured by the amount offundamental operations, is also established for our method.",1
"Predicting Out-of-Domain Generalization with Local Manifold Smoothness Understanding how machine learning models generalize to new environments is acritical part of their safe deployment. Recent work has proposed a variety ofcomplexity measures that directly predict or theoretically bound thegeneralization capacity of a model. However, these methods rely on a strong setof assumptions that in practice are not always satisfied. Motivated by thelimited settings in which existing measures can be applied, we propose a novelcomplexity measure based on the local manifold smoothness of a classifier. Wedefine local manifold smoothness as a classifier's output sensitivity toperturbations in the manifold neighborhood around a given test point.Intuitively, a classifier that is less sensitive to these perturbations shouldgeneralize better. To estimate smoothness we sample points using dataaugmentation and measure the fraction of these points classified into themajority class. Our method only requires selecting a data augmentation methodand makes no other assumptions about the model or data distributions, meaningit can be applied even in out-of-domain (OOD) settings where existing methodscannot. In experiments on robustness benchmarks in image classification,sentiment analysis, and natural language inference, we demonstrate a strong androbust correlation between our manifold smoothness measure and actual OODgeneralization on over 3,000 models evaluated on over 100 train/test domainpairs.",1
"Nonlinear Sufficient Dimension Reduction for Distribution-on-Distribution Regression We introduce a novel framework for nonlinear sufficient dimension reductionwhere both the predictor and the response are distributional data, which aremodeled as members of a metric space. Our key step to achieving the nonlinearsufficient dimension reduction is to build universal kernels on the metricspaces, which results in reproducing kernel Hilbert spaces for the predictorand response that are rich enough to characterize the conditional independencethat determines sufficient dimension reduction. For univariate distributions,we use the well-known quantile representation of the Wasserstein distance toconstruct the universal kernel; for multivariate distributions, we resort tothe recently developed sliced Wasserstein distance to achieve this purpose.Since the sliced Wasserstein distance can be computed by aggregation ofquantile representation of the univariate Wasserstein distance, the computationof multivariate Wasserstein distance is kept at a manageable level. The methodis applied to several data sets, including fertility and mortality distributiondata and Calgary temperature data.",1
"Bayesian Recurrent Units and the Forward-Backward Algorithm Using Bayes's theorem, we derive a unit-wise recurrence as well as a backwardrecursion similar to the forward-backward algorithm. The resulting Bayesianrecurrent units can be integrated as recurrent neural networks within deeplearning frameworks, while retaining a probabilistic interpretation from thedirect correspondence with hidden Markov models. Whilst the contribution ismainly theoretical, experiments on speech recognition indicate that adding thederived units at the end of state-of-the-art recurrent architectures canimprove the performance at a very low cost in terms of trainable parameters.",1
"The Mean Dimension of Neural Networks -- What causes the interaction effects? Owen and Hoyt recently showed that the effective dimension offers keystructural information about the input-output mapping underlying an artificialneural network. Along this line of research, this work proposes an estimationprocedure that allows the calculation of the mean dimension from a givendataset, without resampling from external distributions. The design yieldstotal indices when features are independent and a variant of total indices whenfeatures are correlated. We show that this variant possesses the zeroindependence property. With synthetic datasets, we analyse how the meandimension evolves layer by layer and how the activation function impacts themagnitude of interactions. We then use the mean dimension to study some of themost widely employed convolutional architectures for image recognition (LeNet,ResNet, DenseNet). To account for pixel correlations, we propose calculatingthe mean dimension after the addition of an inverse PCA layer that allows oneto work on uncorrelated PCA-transformed features, without the need to retrainthe neural network. We use the generalized total indices to produce heatmapsfor post-hoc explanations, and we employ the mean dimension on thePCA-transformed features for cross comparisons of the artificial neuralnetworks structures. Results provide several insights on the difference inmagnitude of interactions across the architectures, as well as indications onhow the mean dimension evolves during training.",1
"A Supervised Tensor Dimension Reduction-Based Prognostics Model for Applications with Incomplete Imaging Data This paper proposes a supervised dimension reduction methodology for tensordata which has two advantages over most image-based prognostic models. First,the model does not require tensor data to be complete which expands itsapplication to incomplete data. Second, it utilizes time-to-failure (TTF) tosupervise the extraction of low-dimensional features which makes the extractedfeatures more effective for the subsequent prognostic. Besides, an optimizationalgorithm is proposed for parameter estimation and closed-form solutions arederived under certain distributions.",1
"Multi-Study Boosting: Theoretical Considerations for Merging vs. Ensembling Cross-study replicability is a powerful model evaluation criterion thatemphasizes generalizability of predictions. When training cross-studyreplicable prediction models, it is critical to decide between merging andtreating the studies separately. We study boosting algorithms in the presenceof potential heterogeneity in predictor-outcome relationships across studiesand compare two multi-study learning strategies: 1) merging all the studies andtraining a single model, and 2) multi-study ensembling, which involves traininga separate model on each study and ensembling the resulting predictions. In theregression setting, we provide theoretical guidelines based on an analyticaltransition point to determine whether it is more beneficial to merge or toensemble for boosting with linear learners. In addition, we characterize abias-variance decomposition of estimation error for boosting withcomponent-wise linear learners. We verify the theoretical transition pointresult in simulation and illustrate how it can guide the decision on mergingvs. ensembling in an application to breast cancer gene expression data.",1
"Rethinking Optimization with Differentiable Simulation from a Global Perspective Differentiable simulation is a promising toolkit for fast gradient-basedpolicy optimization and system identification. However, existing approaches todifferentiable simulation have largely tackled scenarios where obtaining smoothgradients has been relatively easy, such as systems with mostly smoothdynamics. In this work, we study the challenges that differentiable simulationpresents when it is not feasible to expect that a single descent reaches aglobal optimum, which is often a problem in contact-rich scenarios. We analyzethe optimization landscapes of diverse scenarios that contain both rigid bodiesand deformable objects. In dynamic environments with highly deformable objectsand fluids, differentiable simulators produce rugged landscapes withnonetheless useful gradients in some parts of the space. We propose a methodthat combines Bayesian optimization with semi-local 'leaps' to obtain a globalsearch method that can use gradients effectively, while also maintaining robustperformance in regions with noisy gradients. We show that our approachoutperforms several gradient-based and gradient-free baselines on an extensiveset of experiments in simulation, and also validate the method usingexperiments with a real robot and deformables. Videos and supplementarymaterials are available at",1
"On minimax density estimation via measure transport We study the convergence properties, in Hellinger and related distances, ofnonparametric density estimators based on measure transport. These estimatorsrepresent the measure of interest as the pushforward of a chosen referencedistribution under a transport map, where the map is chosen via a maximumlikelihood objective (equivalently, minimizing an empirical Kullback-Leiblerloss) or a penalized version thereof. We establish concentration inequalitiesfor a general class of penalized measure transport estimators, by combiningtechniques from M-estimation with analytical properties of the transport-baseddensity representation. We then demonstrate the implications of our theory forthe case of triangular Knothe-Rosenblatt (KR) transports on the $d$-dimensionalunit cube, and show that both penalized and unpenalized versions of suchestimators achieve minimax optimal convergence rates over Hlder classes ofdensities. Specifically, we establish optimal rates for unpenalizednonparametric maximum likelihood estimation over bounded Hlder-type balls,and then for certain Sobolev-penalized estimators and sieved waveletestimators.",1
"Alternating minimization for generalized rank one matrix sensing: Sharp predictions from a random initialization We consider the problem of estimating the factors of a rank-$1$ matrix withi.i.d. Gaussian, rank-$1$ measurements that are nonlinearly transformed andcorrupted by noise. Considering two prototypical choices for the nonlinearity,we study the convergence properties of a natural alternating update rule forthis nonconvex optimization problem starting from a random initialization. Weshow sharp convergence guarantees for a sample-split version of the algorithmby deriving a deterministic recursion that is accurate even in high-dimensionalproblems. Notably, while the infinite-sample population update is uninformativeand suggests exact recovery in a single step, the algorithm -- and ourdeterministic prediction -- converges geometrically fast from a randominitialization. Our sharp, non-asymptotic analysis also exposes several otherfine-grained properties of this problem, including how the nonlinearity andnoise level affect convergence behavior.",1
"Nonparametric regression with modified ReLU networks We consider regression estimation with modified ReLU neural networks in whichnetwork weight matrices are first modified by a function $\alpha$ before beingmultiplied by input vectors. We give an example of continuous, piecewise linearfunction $\alpha$ for which the empirical risk minimizers over the classes ofmodified ReLU networks with $l_1$ and squared $l_2$ penalties attain, up to alogarithmic factor, the minimax rate of prediction of unknown $\beta$-smoothfunction.",1
"Uniform Stability for First-Order Empirical Risk Minimization We consider the problem of designing uniformly stable first-orderoptimization algorithms for empirical risk minimization. Uniform stability isoften used to obtain generalization error bounds for optimization algorithms,and we are interested in a general approach to achieve it. For Euclideangeometry, we suggest a black-box conversion which given a smooth optimizationalgorithm, produces a uniformly stable version of the algorithm whilemaintaining its convergence rate up to logarithmic factors. Using thisreduction we obtain a (nearly) optimal algorithm for smooth optimization withconvergence rate $\widetilde{O}(1/T^2)$ and uniform stability $O(T^2/n)$,resolving an open problem of Chen et al. (2018); Attia and Koren (2021). Formore general geometries, we develop a variant of Mirror Descent for smoothoptimization with convergence rate $\widetilde{O}(1/T)$ and uniform stability$O(T/n)$, leaving open the question of devising a general conversion method asin the Euclidean case.",1
Deep Hedging: Continuous Reinforcement Learning for Hedging of General Portfolios across Multiple Risk Aversions We present a method for finding optimal hedging policies for arbitraryinitial portfolios and market states. We develop a novel actor-critic algorithmfor solving general risk-averse stochastic control problems and use it to learnhedging strategies across multiple risk aversion levels simultaneously. Wedemonstrate the effectiveness of the approach with a numerical example in astochastic volatility environment.,1
"Variance estimation in graphs with the fused lasso We study the problem of variance estimation in general graph-structuredproblems. First, we develop a linear time estimator for the homoscedastic casethat can consistently estimate the variance in general graphs. We show that ourestimator attains minimax rates for the chain and 2D grid graphs when the meansignal has a total variation with canonical scaling. Furthermore, we providegeneral upper bounds on the mean squared error performance of the fused lassoestimator in general graphs under a moment condition and a bound on the tailbehavior of the errors. These upper bounds allow us to generalize for broaderclasses of distributions, such as sub-Exponential, many existing results on thefused lasso that are only known to hold with the assumption that errors aresub-Gaussian random variables. Exploiting our upper bounds, we then study asimple total variation regularization estimator for estimating the signal ofvariances in the heteroscedastic case. Our results show that the varianceestimator attains minimax rates for estimating signals of bounded variation ingrid graphs, $K$-nearest neighbor graphs with very mild assumptions, and it isconsistent for estimating the variances in any connected graph. In addition,extensive numerical results show that our proposed estimators performreasonably well in a variety of graph-structured models.",1
"Mean field Variational Inference via Wasserstein Gradient Flow Variational inference (VI) provides an appealing alternative to traditionalsampling-based approaches for implementing Bayesian inference due to itsconceptual simplicity, statistical accuracy and computational scalability.However, common variational approximation schemes, such as the mean-field (MF)approximation, require certain conjugacy structure to facilitate efficientcomputation, which may add unnecessary restrictions to the viable priordistribution family and impose further constraints on the variationalapproximation family. In this work, we develop a general computationalframework for implementing MF-VI via Wasserstein gradient flow (WGF), agradient flow over the space of probability measures. When specialized toBayesian latent variable models, we analyze the algorithmic convergence of analternating minimization scheme based on a time-discretized WGF forimplementing the MF approximation. In particular, the proposed algorithmresembles a distributional version of EM algorithm, consisting of an E-step ofupdating the latent variable variational distribution and an M-step ofconducting steepest descent over the variational distribution of parameters.Our theoretical analysis relies on optimal transport theory and subdifferentialcalculus in the space of probability measures. We prove the exponentialconvergence of the time-discretized WGF for minimizing a generic objectivefunctional given strict convexity along generalized geodesics. We also providea new proof of the exponential contraction of the variational distributionobtained from the MF approximation by using the fixed-point equation of thetime-discretized WGF. We apply our method and theory to two classic Bayesianlatent variable models, the Gaussian mixture model and the mixture ofregression model. Numerical experiments are also conducted to compliment thetheoretical findings under these two models.",1
"Ultra-low latency recurrent neural network inference on FPGAs for physics applications with hls4ml Recurrent neural networks have been shown to be effective architectures formany tasks in high energy physics, and thus have been widely adopted. Their usein low-latency environments has, however, been limited as a result of thedifficulties of implementing recurrent architectures on field-programmable gatearrays (FPGAs). In this paper we present an implementation of two types ofrecurrent neural network layers -- long short-term memory and gated recurrentunit -- within the hls4ml framework. We demonstrate that our implementation iscapable of producing effective designs for both small and large models, and canbe customized to meet specific design requirements for inference latencies andFPGA resources. We show the performance and synthesized designs for multipleneural networks, many of which are trained specifically for jet identificationtasks at the CERN Large Hadron Collider.",1
"Energy Trees: Regression and Classification With Structured and Mixed-Type Covariates The continuous growth of data complexity requires methods and models thatadequately account for non-trivial structures, as any simplification may induceloss of information. Many analytical tools have been introduced to work withcomplex data objects in their original form, but such tools can typically dealwith single-type variables only. In this work, we propose Energy Trees as amodel for regression and classification tasks where covariates are potentiallyboth structured and of different types. Energy Trees incorporate EnergyStatistics to generalize Conditional Trees, from which they inheritstatistically sound foundations, interpretability, scale invariance, and lackof distributional assumptions. We focus on functions and graphs as structuredcovariates and we show how the model can be easily adapted to work with almostany other type of variable. Through an extensive simulation study, we highlightthe good performance of our proposal in terms of variable selection androbustness to overfitting. Finally, we validate the model's predictive abilitythrough two empirical analyses with human biological data.",1
"Distributed Online System Identification for LTI Systems Using Reverse Experience Replay Identification of linear time-invariant (LTI) systems plays an important rolein control and reinforcement learning. Both asymptotic and finite-time offlinesystem identification are well-studied in the literature. For online systemidentification, the idea of stochastic-gradient descent with reverse experiencereplay (SGD-RER) was recently proposed, where the data sequence is stored inseveral buffers and the stochastic-gradient descent (SGD) update performsbackward in each buffer to break the time dependency between data points.Inspired by this work, we study distributed online system identification of LTIsystems over a multi-agent network. We consider agents as identical LTIsystems, and the network goal is to jointly estimate the system parameters byleveraging the communication between agents. We propose DSGD-RER, a distributedvariant of the SGD-RER algorithm, and theoretically characterize theimprovement of the estimation error with respect to the network size. Ournumerical experiments certify the reduction of estimation error as the networksize grows.",1
"Riemannian Diffusion Schrdinger Bridge Score-based generative models exhibit state of the art performance on densityestimation and generative modeling tasks. These models typically assume thatthe data geometry is flat, yet recent extensions have been developed tosynthesize data living on Riemannian manifolds. Existing methods to acceleratesampling of diffusion models are typically not applicable in the Riemanniansetting and Riemannian score-based methods have not yet been adapted to theimportant task of interpolation of datasets. To overcome these issues, weintroduce \emph{Riemannian Diffusion Schrdinger Bridge}. Our proposed methodgeneralizes Diffusion Schrdinger Bridge introduced in\cite{debortoli2021neurips} to the non-Euclidean setting and extends Riemannianscore-based models beyond the first time reversal. We validate our proposedmethod on synthetic data and real Earth and climate data.",1
"Fairness-aware Network Revenue Management with Demand Learning In addition to maximizing the total revenue, decision-makers in lots ofindustries would like to guarantee fair consumption across different resourcesand avoid saturating certain resources. Motivated by these practical needs,this paper studies the price-based network revenue management problem with bothdemand learning and fairness concern about the consumption across differentresources. We introduce the regularized revenue, i.e., the total revenue with afairness regularization, as our objective to incorporate fairness into therevenue maximization goal. We propose a primal-dual-type online policy with theUpper-Confidence-Bound (UCB) demand learning method to maximize the regularizedrevenue. We adopt several innovative techniques to make our algorithm a unifiedand computationally efficient framework for the continuous price set and a wideclass of fairness regularizers. Our algorithm achieves a worst-case regret of$\tilde O(N^{5/2}\sqrt{T})$, where $N$ denotes the number of products and $T$denotes the number of time periods. Numerical experiments in a few NRM examplesdemonstrate the effectiveness of our algorithm for balancing revenue andfairness.",1
"Classifying Crop Types using Gaussian Bayesian Models and Neural Networks on GHISACONUS USGS data from NASA Hyperspectral Satellite Imagery Hyperspectral Imagining is a type of digital imaging in which each pixelcontains typically hundreds of wavelengths of light providing spectroscopicinformation about the materials present in the pixel. In this paper we provideclassification methods for determining crop type in the USGS GHISACONUS data,which contains around 7,000 pixel spectra from the five major U.S. agriculturalcrops (winter wheat, rice, corn, soybeans, and cotton) collected by the NASAHyperion satellite, and includes the spectrum, geolocation, crop type, andstage of growth for each pixel. We apply standard LDA and QDA as well asBayesian custom versions that compute the joint probability of crop type andstage, and then the marginal probability for crop type, outperforming thenon-Bayesian methods. We also test a single layer neural network with dropouton the data, which performs comparable to LDA and QDA but not as well as theBayesian methods.",1
"Fast Composite Optimization and Statistical Recovery in Federated Learning As a prevalent distributed learning paradigm, Federated Learning (FL) trainsa global model on a massive amount of devices with infrequent communication.This paper investigates a class of composite optimization and statisticalrecovery problems in the FL setting, whose loss function consists of adata-dependent smooth loss and a non-smooth regularizer. Examples includesparse linear regression using Lasso, low-rank matrix recovery using nuclearnorm regularization, etc. In the existing literature, federated compositeoptimization algorithms are designed only from an optimization perspectivewithout any statistical guarantees. In addition, they do not consider commonlyused (restricted) strong convexity in statistical recovery problems. We advancethe frontiers of this problem from both optimization and statisticalperspectives. From optimization upfront, we propose a new algorithm named\textit{Fast Federated Dual Averaging} for strongly convex and smooth loss andestablish state-of-the-art iteration and communication complexity in thecomposite setting. In particular, we prove that it enjoys a fast rate, linearspeedup, and reduced communication rounds. From statistical upfront, forrestricted strongly convex and smooth loss, we design another algorithm, namely\textit{Multi-stage Federated Dual Averaging}, and prove a high probabilitycomplexity bound with linear speedup up to optimal statistical precision.Experiments in both synthetic and real data demonstrate that our methodsperform better than other baselines. To the best of our knowledge, this is thefirst work providing fast optimization algorithms and statistical recoveryguarantees for composite problems in FL.",1
"Instance-optimal PAC Algorithms for Contextual Bandits In the stochastic contextual bandit setting, regret-minimizing algorithmshave been extensively researched, but their instance-minimizing best-armidentification counterparts remain seldom studied. In this work, we focus onthe stochastic bandit problem in the $(\epsilon,\delta)$-$\textit{PAC}$setting: given a policy class $\Pi$ the goal of the learner is to return apolicy $\pi\in \Pi$ whose expected reward is within $\epsilon$ of the optimalpolicy with probability greater than $1-\delta$. We characterize the first$\textit{instance-dependent}$ PAC sample complexity of contextual banditsthrough a quantity $\rho_{\Pi}$, and provide matching upper and lower bounds interms of $\rho_{\Pi}$ for the agnostic and linear contextual best-armidentification settings. We show that no algorithm can be simultaneouslyminimax-optimal for regret minimization and instance-dependent PAC for best-armidentification. Our main result is a new instance-optimal and computationallyefficient algorithm that relies on a polynomial number of calls to an argmaxoracle.",1
"Kernel-based Federated Learning with Personalization We consider federated learning with personalization, where in addition to aglobal objective, each client is also interested in maximizing a personalizedlocal objective. We consider this problem under a general continuous actionspace setting where the objective functions belong to a reproducing kernelHilbert space. We propose algorithms based on surrogate Gaussian process (GP)models that achieve the optimal regret order (up to polylogarithmic factors).Furthermore, we show that the sparse approximations of the GP modelssignificantly reduce the communication cost across clients.",1
"Benign, Tempered, or Catastrophic: A Taxonomy of Overfitting The practical success of overparameterized neural networks has motivated therecent scientific study of interpolating methods, which perfectly fit theirtraining data. Certain interpolating methods, including neural networks, canfit noisy training data without catastrophically bad test performance, indefiance of standard intuitions from statistical learning theory. Aiming toexplain this, a body of recent work has studied $\textit{benign overfitting}$,a phenomenon where some interpolating methods approach Bayes optimality, evenin the presence of noise. In this work we argue that while benign overfittinghas been instructive and fruitful to study, many real interpolating methodslike neural networks $\textit{do not fit benignly}$: modest noise in thetraining set causes nonzero (but non-infinite) excess risk at test time,implying these models are neither benign nor catastrophic but rather fall in anintermediate regime. We call this intermediate regime $\textit{temperedoverfitting}$, and we initiate its systematic study. We first explore thisphenomenon in the context of kernel (ridge) regression (KR) by obtainingconditions on the ridge parameter and kernel eigenspectrum under which KRexhibits each of the three behaviors. We find that kernels with powerlawspectra, including Laplace kernels and ReLU neural tangent kernels, exhibittempered overfitting. We then empirically study deep neural networks throughthe lens of our taxonomy, and find that those trained to interpolation aretempered, while those stopped early are benign. We hope our work leads to amore refined understanding of overfitting in modern learning.",1
"Video Coding Using Learned Latent GAN Compression We propose in this paper a new paradigm for facial video compression. Weleverage the generative capacity of GANs such as StyleGAN to represent andcompress a video, including intra and inter compression. Each frame is invertedin the latent space of StyleGAN, from which the optimal compression is learned.To do so, a diffeomorphic latent representation is learned using a normalizingflows model, where an entropy model can be optimized for image coding. Inaddition, we propose a new perceptual loss that is more efficient than othercounterparts. Finally, an entropy model for video inter coding with residual isalso learned in the previously constructed latent representation. Our method(SGANC) is simple, faster to train, and achieves better results for image andvideo coding compared to state-of-the-art codecs such as VTM, AV1, and recentdeep learning techniques. In particular, it drastically minimizes perceptualdistortion at low bit rates.",1
"Uncertainty-aware Mixed-variable Machine Learning for Materials Design Data-driven design shows the promise of accelerating materials discovery butis challenging due to the prohibitive cost of searching the vast design spaceof chemistry, structure, and synthesis methods. Bayesian Optimization (BO)employs uncertainty-aware machine learning models to select promising designsto evaluate, hence reducing the cost. However, BO with mixed numerical andcategorical variables, which is of particular interest in materials design, hasnot been well studied. In this work, we survey frequentist and Bayesianapproaches to uncertainty quantification of machine learning with mixedvariables. We then conduct a systematic comparative study of their performancesin BO using a popular representative model from each group, the randomforest-based Lolo model (frequentist) and the latent variable Gaussian processmodel (Bayesian). We examine the efficacy of the two models in the optimizationof mathematical functions, as well as properties of structural and functionalmaterials, where we observe performance differences as related to problemdimensionality and complexity. By investigating the machine learning models'predictive and uncertainty estimation capabilities, we provide interpretationsof the observed performance differences. Our results provide practical guidanceon choosing between frequentist and Bayesian uncertainty-aware machine learningmodels for mixed-variable BO in materials design.",1
"(Nearly) Optimal Private Linear Regression via Adaptive Clipping We study the problem of differentially private linear regression where eachdata point is sampled from a fixed sub-Gaussian style distribution. We proposeand analyze a one-pass mini-batch stochastic gradient descent method(DP-AMBSSGD) where points in each iteration are sampled without replacement.Noise is added for DP but the noise standard deviation is estimated online.Compared to existing $(\epsilon, \delta)$-DP techniques which have sub-optimalerror bounds, DP-AMBSSGD is able to provide nearly optimal error bounds interms of key parameters like dimensionality $d$, number of points $N$, and thestandard deviation $\sigma$ of the noise in observations. For example, when the$d$-dimensional covariates are sampled i.i.d. from the normal distribution,then the excess error of DP-AMBSSGD due to privacy is $\frac{\sigma^2d}{N}(1+\frac{d}{\epsilon^2 N})$, i.e., the error is meaningful when number ofsamples $N= \Omega(d \log d)$ which is the standard operative regime for linearregression. In contrast, error bounds for existing efficient methods in thissetting are: $\mathcal{O}\big(\frac{d^3}{\epsilon^2 N^2}\big)$, even for$\sigma=0$. That is, for constant $\epsilon$, the existing techniques require$N=\Omega(d\sqrt{d})$ to provide a non-trivial result.",1
"Feed-Forward Source-Free Latent Domain Adaptation via Cross-Attention We study the highly practical but comparatively under-studied problem oflatent-domain adaptation, where a source model should be adapted to a targetdataset that contains a mixture of unlabelled domain-relevant anddomain-irrelevant examples. Furthermore, motivated by the requirements for dataprivacy and the need for embedded and resource-constrained devices of all kindsto adapt to local data distributions, we focus on the setting of feed-forwardsource-free domain adaptation, where adaptation should not require access tothe source dataset, and also be back propagation-free. Our solution is tometa-learn a network capable of embedding the mixed-relevance target datasetand dynamically adapting inference for target examples using cross-attention.The resulting framework leads to consistent improvement on strong ERMbaselines. We also show that our framework sometimes even improves on the upperbound of domain-supervised adaptation, where only domain-relevant instances areprovided for adaptation. This suggests that human annotated domain labels maynot always be optimal, and raises the possibility of doing better throughautomated instance selection.",1
"Contextual Decision Trees Focusing on Random Forests, we propose a multi-armed contextual banditrecommendation framework for feature-based selection of a single shallow treeof the learned ensemble. The trained system, which works on top of the RandomForest, dynamically identifies a base predictor that is responsible forproviding the final output. In this way, we obtain local interpretations byobserving the rules of the recommended tree. The carried out experiments revealthat our dynamic method is superior to an independent fitted CART decision treeand comparable to the whole black-box Random Forest in terms of predictiveperformances.",1
"Blessing of Nonconvexity in Deep Linear Models: Depth Flattens the Optimization Landscape Around the True Solution This work characterizes the effect of depth on the optimization landscape oflinear regression, showing that, despite their nonconvexity, deeper models havemore desirable optimization landscape. We consider a robust andover-parameterized setting, where a subset of measurements are grosslycorrupted with noise and the true linear model is captured via an $N$-layerlinear neural network. On the negative side, we show that this problem\textit{does not} have a benign landscape: given any $N\geq 1$, with constantprobability, there exists a solution corresponding to the ground truth that isneither local nor global minimum. However, on the positive side, we prove that,for any $N$-layer model with $N\geq 2$, a simple sub-gradient method becomesoblivious to such ``problematic'' solutions; instead, it converges to abalanced solution that is not only close to the ground truth but also enjoys aflat local landscape, thereby eschewing the need for early stopping. Lastly,we empirically verify that the desirable optimization landscape of deepermodels extends to other robust learning tasks, including deep matrix recoveryand deep ReLU networks with $\ell_1$-loss.",1
"A Generative Framework for Personalized Learning and Estimation: Theory, Algorithms, and Privacy A distinguishing characteristic of federated learning is that the (local)client data could have statistical heterogeneity. This heterogeneity hasmotivated the design of personalized learning, where individual (personalized)models are trained, through collaboration. There have been variouspersonalization methods proposed in literature, with seemingly very differentforms and methods ranging from use of a single global model for localregularization and model interpolation, to use of multiple global models forpersonalized clustering, etc. In this work, we begin with a generativeframework that could potentially unify several different algorithms as well assuggest new algorithms. We apply our generative framework to personalizedestimation, and connect it to the classical empirical Bayes' methodology. Wedevelop private personalized estimation under this framework. We then use ourgenerative framework for learning, which unifies several known personalized FLalgorithms and also suggests new ones; we propose and study a new algorithmAdaPeD based on a Knowledge Distillation, which numerically outperforms severalknown algorithms. We also develop privacy for personalized learning methodswith guarantees for user-level privacy and composition. We numerically evaluatethe performance as well as the privacy for both the estimation and learningproblems, demonstrating the advantages of our proposed methods.",1
"Long Term Fairness for Minority Groups via Performative Distributionally Robust Optimization Fairness researchers in machine learning (ML) have coalesced around severalfairness criteria which provide formal definitions of what it means for an MLmodel to be fair. However, these criteria have some serious limitations. Weidentify four key shortcomings of these formal fairness criteria, and aim tohelp to address them by extending performative prediction to include adistributionally robust objective.",1
"Size and depth of monotone neural networks: interpolation and approximation Monotone functions and data sets arise in a variety of applications. We studythe interpolation problem for monotone data sets: The input is a monotone dataset with $n$ points, and the goal is to find a size and depth efficientmonotone neural network, with non negative parameters and threshold units, thatinterpolates the data set. We show that there are monotone data sets thatcannot be interpolated by a monotone network of depth $2$. On the other hand,we prove that for every monotone data set with $n$ points in $\mathbb{R}^d$,there exists an interpolating monotone network of depth $4$ and size $O(nd)$.Our interpolation result implies that every monotone function over $[0,1]^d$can be approximated arbitrarily well by a depth-4 monotone network, improvingthe previous best-known construction of depth $d+1$. Finally, building onresults from Boolean circuit complexity, we show that the inductive bias ofhaving positive parameters can lead to a super-polynomial blow-up in the numberof neurons when approximating monotone functions.",1
"ASR Error Detection via Audio-Transcript entailment Despite improved performances of the latest Automatic Speech Recognition(ASR) systems, transcription errors are still unavoidable. These errors canhave a considerable impact in critical domains such as healthcare, when used tohelp with clinical documentation. Therefore, detecting ASR errors is a criticalfirst step in preventing further error propagation to downstream applications.To this end, we propose a novel end-to-end approach for ASR error detectionusing audio-transcript entailment. To the best of our knowledge, we are thefirst to frame this problem as an end-to-end entailment task between the audiosegment and its corresponding transcript segment. Our intuition is that thereshould be a bidirectional entailment between audio and transcript when there isno recognition error and vice versa. The proposed model utilizes an acousticencoder and a linguistic encoder to model the speech and transcriptrespectively. The encoded representations of both modalities are fused topredict the entailment. Since doctor-patient conversations are used in ourexperiments, a particular emphasis is placed on medical terms. Our proposedmodel achieves classification error rates (CER) of 26.2% on all transcriptionerrors and 23% on medical errors specifically, leading to improvements upon astrong baseline by 12% and 15.4%, respectively.",1
"Estimating value at risk: LSTM vs. GARCH Estimating value-at-risk on time series data with possibly heteroscedasticdynamics is a highly challenging task. Typically, we face a small data problemin combination with a high degree of non-linearity, causing difficulties forboth classical and machine-learning estimation algorithms. In this paper, wepropose a novel value-at-risk estimator using a long short-term memory (LSTM)neural network and compare its performance to benchmark GARCH estimators.",1
"Fuzzy Clustering by Hyperbolic Smoothing We propose a novel method for building fuzzy clusters of large data sets,using a smoothing numerical approach. The usual sum-of-squares criterion isrelaxed so the search for good fuzzy partitions is made on a continuous space,rather than a combinatorial space as in classical methods \cite{Hartigan}. Thesmoothing allows a conversion from a strongly non-differentiable problem intodifferentiable subproblems of optimization without constraints of lowdimension, by using a differentiable function of infinite class. For theimplementation of the algorithm we used the statistical software $R$ and theresults obtained were compared to the traditional fuzzy $C$--means method,proposed by Bezdek.",1
"Scalable Bayesian Inference for Detection and Deblending in Astronomical Images We present a new probabilistic method for detecting, deblending, andcataloging astronomical sources called the Bayesian Light Source Separator(BLISS). BLISS is based on deep generative models, which embed neural networkswithin a Bayesian model. For posterior inference, BLISS uses a new form ofvariational inference known as Forward Amortized Variational Inference. TheBLISS inference routine is fast, requiring a single forward pass of the encodernetworks on a GPU once the encoder networks are trained. BLISS can performfully Bayesian inference on megapixel images in seconds, and produces highlyaccurate catalogs. BLISS is highly extensible, and has the potential todirectly answer downstream scientific questions in addition to producingprobabilistic catalogs.",1
"Collaborative Uncertainty Benefits Multi-Agent Multi-Modal Trajectory Forecasting In multi-modal multi-agent trajectory forecasting, two major challenges havenot been fully tackled: 1) how to measure the uncertainty brought by theinteraction module that causes correlations among the predicted trajectories ofmultiple agents; 2) how to rank the multiple predictions and select the optimalpredicted trajectory. In order to handle these challenges, this work firstproposes a novel concept, collaborative uncertainty (CU), which models theuncertainty resulting from interaction modules. Then we build a generalCU-aware regression framework with an original permutation-equivariantuncertainty estimator to do both tasks of regression and uncertaintyestimation. Further, we apply the proposed framework to current SOTAmulti-agent multi-modal forecasting systems as a plugin module, which enablesthe SOTA systems to 1) estimate the uncertainty in the multi-agent multi-modaltrajectory forecasting task; 2) rank the multiple predictions and select theoptimal one based on the estimated uncertainty. We conduct extensiveexperiments on a synthetic dataset and two public large-scale multi-agenttrajectory forecasting benchmarks. Experiments show that: 1) on the syntheticdataset, the CU-aware regression framework allows the model to appropriatelyapproximate the ground-truth Laplace distribution; 2) on the multi-agenttrajectory forecasting benchmarks, the CU-aware regression framework steadilyhelps SOTA systems improve their performances. Specially, the proposedframework helps VectorNet improve by 262 cm regarding the Final DisplacementError of the chosen optimal prediction on the nuScenes dataset; 3) formulti-agent multi-modal trajectory forecasting systems, prediction uncertaintyis positively correlated with future stochasticity; and 4) the estimated CUvalues are highly related to the interactive information among agents.",1
"Journal Impact Factor and Peer Review Thoroughness and Helpfulness: A Supervised Machine Learning Study The journal impact factor (JIF) is often equated with journal quality and thequality of the peer review of the papers submitted to the journal. We examinedthe association between the content of peer review and JIF by analysing 10,000peer review reports submitted to 1,644 medical and life sciences journals. Tworesearchers hand-coded a random sample of 2,000 sentences. We then trainedmachine learning models to classify all 187,240 sentences as contributing ornot contributing to content categories. We examined the association between tengroups of journals defined by JIF deciles and the content of peer reviews usinglinear mixed-effects models, adjusting for the length of the review. The JIFranged from 0.21 to 74.70. The length of peer reviews increased from the lowest(median number of words 185) to the JIF group (387 words). The proportion ofsentences allocated to different content categories varied widely, even withinJIF groups. For thoroughness, sentences on 'Materials and Methods' were morecommon in the highest JIF journals than in the lowest JIF group (difference of7.8 percentage points; 95% CI 4.9 to 10.7%). The trend for 'Presentation andReporting' went in the opposite direction, with the highest JIF journals givingless emphasis to such content (difference -8.9%; 95% CI -11.3 to -6.5%). Forhelpfulness, reviews for higher JIF journals devoted less attention to'Suggestion and Solution' and provided fewer Examples than lower impact factorjournals. No, or only small differences were evident for other contentcategories. In conclusion, peer review in journals with higher JIF tends to bemore thorough in discussing the methods used but less helpful in terms ofsuggesting solutions and providing examples. Differences were modest andvariability high, indicating that the JIF is a bad predictor for the quality ofpeer review of an individual manuscript.",1
"Contextual Bandits with Large Action Spaces: Made Practical A central problem in sequential decision making is to develop algorithms thatare practical and computationally efficient, yet support the use of flexible,general-purpose models. Focusing on the contextual bandit problem, recentprogress provides provably efficient algorithms with strong empiricalperformance when the number of possible alternatives (actions) is small, butguarantees for decision making in large, continuous action spaces have remainedelusive, leading to a significant gap between theory and practice. We presentthe first efficient, general-purpose algorithm for contextual bandits withcontinuous, linearly structured action spaces. Our algorithm makes use ofcomputational oracles for (i) supervised learning, and (ii) optimization overthe action space, and achieves sample complexity, runtime, and memoryindependent of the size of the action space. In addition, it is simple andpractical. We perform a large-scale empirical evaluation, and show that ourapproach typically enjoys superior performance and efficiency compared tostandard baselines.",1
"POET: Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging Fine-tuning models on edge devices like mobile phones would enableprivacy-preserving personalization over sensitive data. However, edge traininghas historically been limited to relatively small models with simplearchitectures because training is both memory and energy intensive. We presentPOET, an algorithm to enable training large neural networks on memory-scarcebattery-operated edge devices. POET jointly optimizes the integrated searchsearch spaces of rematerialization and paging, two algorithms to reduce thememory consumption of backpropagation. Given a memory budget and a run-timeconstraint, we formulate a mixed-integer linear program (MILP) forenergy-optimal training. Our approach enables training significantly largermodels on embedded devices while reducing energy consumption while notmodifying mathematical correctness of backpropagation. We demonstrate that itis possible to fine-tune both ResNet-18 and BERT within the memory constraintsof a Cortex-M class embedded device while outperforming current edge trainingmethods in energy efficiency. POET is an open-source project available at",1
"When Does Differentially Private Learning Not Suffer in High Dimensions? Large pretrained models can be privately fine-tuned to achieve performanceapproaching that of non-private models. A common theme in these results is thesurprising observation that high-dimensional models can achieve favorableprivacy-utility trade-offs. This seemingly contradicts known results on themodel-size dependence of differentially private convex learning and raises thefollowing research question: When does the performance of differentiallyprivate learning not degrade with increasing model size? We identify that themagnitudes of gradients projected onto subspaces is a key factor thatdetermines performance. To precisely characterize this for private convexlearning, we introduce a condition on the objective that we term restrictedLipschitz continuity and derive improved bounds for the excess empirical andpopulation risks that are dimension-independent under additional conditions. Weempirically show that in private fine-tuning of large language models,gradients evaluated near a local optimum are mostly controlled by a fewprincipal components. This behavior is similar to conditions under which weobtain dimension-independent bounds in convex settings. Our theoretical andempirical results together provide a possible explanation for recent successesin large-scale private fine-tuning.",1
"Inference of Regulatory Networks Through Temporally Sparse Data A major goal in genomics is to properly capture the complex dynamicalbehaviors of gene regulatory networks (GRNs). This includes inferring thecomplex interactions between genes, which can be used for a wide range ofgenomics analyses, including diagnosis or prognosis of diseases and findingeffective treatments for chronic diseases such as cancer. Boolean networks haveemerged as a successful class of models for capturing the behavior of GRNs. Inmost practical settings, inference of GRNs should be achieved through limitedand temporally sparse genomics data. A large number of genes in GRNs leads to alarge possible topology candidate space, which often cannot be exhaustivelysearched due to the limitation in computational resources. This paper developsa scalable and efficient topology inference for GRNs using Bayesianoptimization and kernel-based methods. Rather than an exhaustive search overpossible topologies, the proposed method constructs a Gaussian Process (GP)with a topology-inspired kernel function to account for correlation in thelikelihood function. Then, using the posterior distribution of the GP model,the Bayesian optimization efficiently searches for the topology with thehighest likelihood value by optimally balancing between exploration andexploitation. The performance of the proposed method is demonstrated throughcomprehensive numerical experiments using a well-known mammalian cell-cyclenetwork.",1
"pGMM Kernel Regression and Comparisons with Boosted Trees In this work, we demonstrate the advantage of the pGMM (``powered generalizedmin-max'') kernel in the context of (ridge) regression. In recent priorstudies, the pGMM kernel has been extensively evaluated for classificationtasks, for logistic regression, support vector machines, as well as deep neuralnetworks. In this paper, we provide an experimental study on ridge regression,to compare the pGMM kernel regression with the ordinary ridge linear regressionas well as the RBF kernel ridge regression. Perhaps surprisingly, even withouta tuning parameter (i.e., $p=1$ for the power parameter of the pGMM kernel),the pGMM kernel already performs well. Furthermore, by tuning the parameter$p$, this (deceptively simple) pGMM kernel even performs quite comparably toboosted trees.",1
"Variational Neural Networks Bayesian Neural Networks (BNNs) provide a tool to estimate the uncertainty ofa neural network by considering a distribution over weights and samplingdifferent models for each input. In this paper, we propose a method foruncertainty estimation in neural networks called Variational Neural Networkthat, instead of considering a distribution over weights, generates parametersfor the output distribution of a layer by transforming its inputs withlearnable sub-layers. In uncertainty quality estimation experiments, we showthat VNNs achieve better uncertainty quality than Monte Carlo Dropout or BayesBy Backpropagation methods.",1
"PRoA: A Probabilistic Robustness Assessment against Functional Perturbations In safety-critical deep learning applications robustness measurement is avital pre-deployment phase. However, existing robustness verification methodsare not sufficiently practical for deploying machine learning systems in thereal world. On the one hand, these methods attempt to claim that noperturbations can ``fool'' deep neural networks (DNNs), which may be toostringent in practice. On the other hand, existing works rigorously consider$L_p$ bounded additive perturbations on the pixel space, althoughperturbations, such as colour shifting and geometric transformations, are morepractically and frequently occurring in the real world. Thus, from thepractical standpoint, we present a novel and general {\it probabilisticrobustness assessment method} (PRoA) based on the adaptive concentration, andit can measure the robustness of deep learning models against functionalperturbations. PRoA can provide statistical guarantees on the probabilisticrobustness of a model, \textit{i.e.}, the probability of failure encountered bythe trained model after deployment. Our experiments demonstrate theeffectiveness and flexibility of PRoA in terms of evaluating the probabilisticrobustness against a broad range of functional perturbations, and PRoA canscale well to various large-scale deep neural networks compared to existingstate-of-the-art baselines. For the purpose of reproducibility, we release ourtool on GitHub: \url{",1
"Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models Low-order functional ANOVA (fANOVA) models have been rediscovered in themachine learning (ML) community under the guise of inherently interpretablemachine learning. Explainable Boosting Machines or EBM (Lou et al. 2013) andGAMI-Net (Yang et al. 2021) are two recently proposed ML algorithms for fittingfunctional main effects and second-order interactions. We propose a newalgorithm, called GAMI-Tree, that is similar to EBM, but has a number offeatures that lead to better performance. It uses model-based trees as baselearners and incorporates a new interaction filtering method that is better atcapturing the underlying interactions. In addition, our iterative trainingmethod converges to a model with better predictive performance, and theembedded purification ensures that interactions are hierarchically orthogonalto main effects. The algorithm does not need extensive tuning, and ourimplementation is fast and efficient. We use simulated and real datasets tocompare the performance and interpretability of GAMI-Tree with EBM andGAMI-Net.",1
"Jackknife Variability Estimation For Randomized Matrix Computations Randomized algorithms based on sketching have become a workhorse tool inlow-rank matrix approximation. To use these algorithms safely in applications,they should be coupled with diagnostics to assess the quality of approximation.To meet this need, this paper proposes a jackknife resampling method toestimate the variability of the output of a randomized matrix computation. Thevariability estimate can recognize that a computation requires additional dataor that the computation is intrinsically unstable. As examples, the paperstudies jackknife estimates for two randomized low-rank matrix approximationalgorithms. In each case, the operation count for the jackknife estimate isindependent of the dimensions of the target matrix. In numerical experiments,the estimator accurately assesses variability and also provides anorder-of-magnitude estimate of the mean-square error.",1
"Orthogonalization of data via Gromov-Wasserstein type feedback for clustering and visualization In this paper we propose an adaptive approach for clustering andvisualization of data by an orthogonalization process. Starting with the datapoints being represented by a Markov process using the diffusion map framework,the method adaptively increase the orthogonality of the clusters by applying afeedback mechanism inspired by the Gromov-Wasserstein distance. This mechanismiteratively increases the spectral gap and refines the orthogonality of thedata to achieve a clustering with high specificity. By using the diffusion mapframework and representing the relation between data points using transitionprobabilities, the method is robust with respect to both the underlyingdistance, noise in the data and random initialization. We prove that the methodconverges globally to a unique fixpoint for certain parameter values. We alsopropose a related approach where the transition probabilities in the Markovprocess are required to be doubly stochastic, in which case the methodgenerates a minimizer to a nonconvex optimization problem. We apply the methodon cryo-electron microscopy image data from biopharmaceutical manufacturingwhere we can confirm biologically relevant insights related to therapeuticefficacy. We consider an example with morphological variations of genepackaging and confirm that the method produces biologically meaningfulclustering results consistent with human expert classification.",1
"Semantic uncertainty intervals for disentangled latent spaces Meaningful uncertainty quantification in computer vision requires reasoningabout semantic information -- say, the hair color of the person in a photo orthe location of a car on the street. To this end, recent breakthroughs ingenerative modeling allow us to represent semantic information in disentangledlatent spaces, but providing uncertainties on the semantic latent variables hasremained challenging. In this work, we provide principled uncertainty intervalsthat are guaranteed to contain the true semantic factors for any underlyinggenerative model. The method does the following: (1) it uses quantileregression to output a heuristic uncertainty interval for each element in thelatent space (2) calibrates these uncertainties such that they contain the truevalue of the latent for a new, unseen input. The endpoints of these calibratedintervals can then be propagated through the generator to produce interpretableuncertainty visualizations for each semantic factor. This technique reliablycommunicates semantically meaningful, principled, and instance-adaptiveuncertainty in inverse problems like image super-resolution and imagecompletion.",1
"Improved Global Guarantees for the Nonconvex Burer--Monteiro Factorization via Rank Overparameterization We consider minimizing a twice-differentiable, $L$-smooth, and $\mu$-stronglyconvex objective $\phi$ over an $n\times n$ positive semidefinite matrix$M\succeq0$, under the assumption that the minimizer $M^{\star}$ has low rank$r^{\star}\ll n$. Following the Burer--Monteiro approach, we instead minimizethe nonconvex objective $f(X)=\phi(XX^{T})$ over a factor matrix $X$ of size$n\times r$. This substantially reduces the number of variables from $O(n^{2})$to as few as $O(n)$ and also enforces positive semidefiniteness for free, butat the cost of giving up the convexity of the original problem. In this paper,we prove that if the search rank $r\ge r^{\star}$ is overparameterized by aconstant factor with respect to the true rank $r^{\star}$, namely as in$r>\frac{1}{4}(L/\mu-1)^{2}r^{\star}$, then despite nonconvexity, localoptimization is guaranteed to globally converge from any initial point to theglobal optimum. This significantly improves upon a previous rankoverparameterization threshold of $r\ge n$, which is known to be sharp if$\phi$ is allowed to be nonsmooth and/or non-strongly convex, but wouldincrease the number of variables back up to $O(n^{2})$. Conversely, withoutrank overparameterization, we prove that such a global guarantee is possible ifand only if $\phi$ is almost perfectly conditioned, with a condition number of$L/\mu<3$. Therefore, we conclude that a small amount of overparameterizationcan lead to large improvements in theoretical guarantees for the nonconvexBurer--Monteiro factorization.",1
"Twitmo: A Twitter Data Topic Modeling and Visualization Package for R We present Twitmo, a package that provides a broad range of methods tocollect, pre-process, analyze and visualize geo-tagged Twitter data. Twitmoenables the user to collect geo-tagged Tweets from Twitter and and provides acomprehensive and user-friendly toolbox to generate topic distributions fromLatent Dirichlet Allocations (LDA), correlated topic models (CTM) andstructural topic models (STM). Functions are included for pre-processing oftext, model building and prediction. In addition, one of the innovations of thepackage is the automatic pooling of Tweets into longer pseudo-documents usinghashtags and cosine similarities for better topic coherence. The packageadditionally comes with functionality to visualize collected data sets andfitted models in static as well as interactive ways and offers built-in supportfor model visualizations via LDAvis providing great convenience for researchersin this area. The Twitmo package is an innovative toolbox that can be used toanalyze public discourse of various topics, political parties or persons ofinterest in space and time.",1
"Joint Application of the Target Trial Causal Framework and Machine Learning Modeling to Optimize Antibiotic Therapy: Use Case on Acute Bacterial Skin and Skin Structure Infections due to Methicillin-resistant Staphylococcus aureus Bacterial infections are responsible for high mortality worldwide.Antimicrobial resistance underlying the infection, and multifaceted patient'sclinical status can hamper the correct choice of antibiotic treatment.Randomized clinical trials provide average treatment effect estimates but arenot ideal for risk stratification and optimization of therapeutic choice, i.e.,individualized treatment effects (ITE). Here, we leverage large-scaleelectronic health record data, collected from Southern US academic clinics, toemulate a clinical trial, i.e., 'target trial', and develop a machine learningmodel of mortality prediction and ITE estimation for patients diagnosed withacute bacterial skin and skin structure infection (ABSSSI) due tomethicillin-resistant Staphylococcus aureus (MRSA). ABSSSI-MRSA is achallenging condition with reduced treatment options - vancomycin is thepreferred choice, but it has non-negligible side effects. First, we usepropensity score matching to emulate the trial and create a treatmentrandomized (vancomycin vs. other antibiotics) dataset. Next, we use this datato train various machine learning methods (including boosted/LASSO logisticregression, support vector machines, and random forest) and choose the bestmodel in terms of area under the receiver characteristic (AUC) throughbootstrap validation. Lastly, we use the models to calculate ITE and identifypossible averted deaths by therapy change. The out-of-bag tests indicate thatSVM and RF are the most accurate, with AUC of 81% and 78%, respectively, butBLR/LASSO is not far behind (76%). By calculating the counterfactuals using theBLR/LASSO, vancomycin increases the risk of death, but it shows a largevariation (odds ratio 1.2, 95% range 0.4-3.8) and the contribution to outcomeprobability is modest. Instead, the RF exhibits stronger changes in ITE,suggesting more complex treatment heterogeneity.",1
"On the instrumental variable estimation with many weak and invalid instruments We discuss the fundamental issue of identification in linear instrumentalvariable (IV) models with unknown IV validity. We revisit the popular majorityand plurality rules and show that no identification condition can be if andonly if in general. With the assumption of the sparsest rule, which isequivalent to the plurality rule but becomes operational in computationalgorithms, we investigate and prove the advantages of non-convex penalizedapproaches over other IV estimators based on two-step selections, in terms ofselection consistency and accommodation for individually weak IVs. Furthermore,we propose a surrogate sparsest penalty that aligns with the identificationcondition and provides oracle sparse structure simultaneously. Desirabletheoretical properties are derived for the proposed estimator with weaker IVstrength conditions compared to the previous literature. Finite sampleproperties are demonstrated using simulations and the selection and estimationmethod is applied to an empirical study concerning the effect of trade oneconomic growth.",1
"Parallel APSM for Fast and Adaptive Digital SIC in Full-Duplex Transceivers with Nonlinearity This paper presents a kernel-based adaptive filter that is applied for thedigital domain self-interference cancellation (SIC) in a transceiver operatingin full-duplex (FD) mode. In FD, the benefit of simultaneous transmission andreceiving of signals comes at the price of strong self-interference (SI). Inthis work, we are primarily interested in suppressing the SI using an adaptivefilter namely adaptive projected subgradient method (APSM) in a reproducingkernel Hilbert space (RKHS) of functions. Using the projection concept as apowerful tool, APSM is used to model and consequently remove the SI. Alow-complexity and fast-tracking algorithm is provided taking advantage ofparallel projections as well as the kernel trick in RKHS. The performance ofthe proposed method is evaluated on real measurement data. The methodillustrates the good performance of the proposed adaptive filter, compared tothe known popular benchmarks. They demonstrate that the kernel-based algorithmachieves a favorable level of digital SIC while enabling parallelcomputation-based implementation within a rich and nonlinear function space,thanks to the employed adaptive filtering method.",1
"The Poisson binomial mechanism for secure and private federated learning We introduce the Poisson Binomial mechanism (PBM), a discrete differentialprivacy mechanism for distributed mean estimation (DME) with applications tofederated learning and analytics. We provide a tight analysis of its privacyguarantees, showing that it achieves the same privacy-accuracy trade-offs asthe continuous Gaussian mechanism. Our analysis is based on a novel bound onthe Rnyi divergence of two Poisson binomial distributions that may be ofindependent interest.",1
"A Federated Cox Model with Non-Proportional Hazards Recent research has shown the potential for neural networks to improve uponclassical survival models such as the Cox model, which is widely used inclinical practice. Neural networks, however, typically rely on data that arecentrally available, whereas healthcare data are frequently held in securesilos. We present a federated Cox model that accommodates this data setting andalso relaxes the proportional hazards assumption, allowing time-varyingcovariate effects. In this latter respect, our model does not require explicitspecification of the time-varying effects, reducing upfront organisationalcosts compared to previous works. We experiment with publicly availableclinical datasets and demonstrate that the federated model is able to performas well as a standard model.",1
"Intrinsic dimension estimation for discrete metrics Real world-datasets characterized by discrete features are ubiquitous: fromcategorical surveys to clinical questionnaires, from unweighted networks to DNAsequences. Nevertheless, the most common unsupervised dimensional reductionmethods are designed for continuous spaces, and their use for discrete spacescan lead to errors and biases. In this letter we introduce an algorithm toinfer the intrinsic dimension (ID) of datasets embedded in discrete spaces. Wedemonstrate its accuracy on benchmark datasets, and we apply it to analyze ametagenomic dataset for species fingerprinting, finding a surprisingly smallID, of order 2. This suggests that evolutive pressure acts on a low-dimensionalmanifold despite the high-dimensionality of sequences' space.",1
Graph Neural Network Bandits We consider the bandit optimization problem with the reward function definedover graph-structured data.,1
"Fast computation of rankings from pairwise comparisons We study the ranking of individuals, teams, or objects on the basis ofpairwise comparisons using the Bradley-Terry model. Maximum-likelihoodestimates of rankings within this model are commonly made using a simpleiterative algorithm first introduced by Zermelo almost a century ago. Here wedescribe an alternative and similarly simple iteration that solves the sameproblem much faster -- over a hundred times faster in some cases. Wedemonstrate this algorithm with applications to a range of example data setsand derive some results regarding its convergence.",1
"Flowsheet synthesis through hierarchical reinforcement learning and graph neural networks Process synthesis experiences a disruptive transformation accelerated bydigitization and artificial intelligence. We propose a reinforcement learningalgorithm for chemical process design based on a state-of-the-art actor-criticlogic. Our proposed algorithm represents chemical processes as graphs and usesgraph convolutional neural networks to learn from process graphs. Inparticular, the graph neural networks are implemented within the agentarchitecture to process the states and make decisions. Moreover, we implement ahierarchical and hybrid decision-making process to generate flowsheets, whereunit operations are placed iteratively as discrete decisions and correspondingdesign variables are selected as continuous decisions. We demonstrate thepotential of our method to design economically viable flowsheets in anillustrative case study comprising equilibrium reactions, azeotropicseparation, and recycles. The results show quick learning in discrete,continuous, and hybrid action spaces. Due to the flexible architecture of theproposed reinforcement learning agent, the method is predestined to includelarge action-state spaces and an interface to process simulators in futureresearch.",1
"Adversarial Sign-Corrupted Isotonic Regression Classical univariate isotonic regression involves nonparametric estimationunder a monotonicity constraint of the true signal. We consider a variation ofthis generating process, which we term adversarial sign-corrupted isotonic(\texttt{ASCI}) regression. Under this \texttt{ASCI} setting, the adversary hasfull access to the true isotonic responses, and is free to sign-corrupt them.Estimating the true monotonic signal given these sign-corrupted responses is ahighly challenging task. Notably, the sign-corruptions are designed to violatemonotonicity, and possibly induce heavy dependence between the corruptedresponse terms. In this sense, \texttt{ASCI} regression may be viewed as anadversarial stress test for isotonic regression. Our motivation is driven byunderstanding whether efficient robust estimation of the monotone signal isfeasible under this adversarial setting. We develop \texttt{ASCIFIT}, athree-step estimation procedure under the \texttt{ASCI} setting. The\texttt{ASCIFIT} procedure is conceptually simple, easy to implement withexisting software, and consists of applying the \texttt{PAVA} with crucial pre-and post-processing corrections. We formalize this procedure, and demonstrateits theoretical guarantees in the form of sharp high probability upper boundsand minimax lower bounds. We illustrate our findings with detailed simulations.",1
"Estimation of Non-Crossing Quantile Regression Process with Deep ReQU Neural Networks We propose a penalized nonparametric approach to estimating the quantileregression process (QRP) in a nonseparable model using rectifier quadratic unit(ReQU) activated deep neural networks and introduce a novel penalty function toenforce non-crossing of quantile regression curves. We establish thenon-asymptotic excess risk bounds for the estimated QRP and derive the meanintegrated squared error for the estimated QRP under mild smoothness andregularity conditions. To establish these non-asymptotic risk and estimationerror bounds, we also develop a new error bound for approximating $C^s$ smoothfunctions with $s >0$ and their derivatives using ReQU activated neuralnetworks. This is a new approximation result for ReQU networks and is ofindependent interest and may be useful in other problems. Our numericalexperiments demonstrate that the proposed method is competitive with oroutperforms two existing methods, including methods using reproducing kernelsand random forests, for nonparametric quantile regression.",1
"Multi-Model Federated Learning with Provable Guarantees Federated Learning (FL) is a variant of distributed learning where edgedevices collaborate to learn a model without sharing their data with thecentral server or each other. We refer to the process of training multipleindependent models simultaneously in a federated setting using a common pool ofclients as multi-model FL. In this work, we propose two variants of the popularFedAvg algorithm for multi-model FL, with provable convergence guarantees. Wefurther show that for the same amount of computation, multi-model FL can havebetter performance than training each model separately. We supplement ourtheoretical results with experiments in strongly convex, convex, and non-convexsettings.",1
"Latent Variable Models for Bayesian Causal Discovery Learning predictors that do not rely on spurious correlations involvesbuilding causal representations. However, learning such a representation isvery challenging. We, therefore, formulate the problem of learning a causalrepresentation from high dimensional data and study causal recovery withsynthetic data. This work introduces a latent variable decoder model, DecoderBCD, for Bayesian causal discovery and performs experiments in mildlysupervised and unsupervised settings. We present a series of syntheticexperiments to characterize important factors for causal discovery and showthat using known intervention targets as labels helps in unsupervised Bayesianinference over structure and parameters of linear Gaussian additive noiselatent structural causal models.",1
"Edge Augmentation on Disconnected Graphs via Eigenvalue Elevation The graph-theoretical task of determining most likely inter-community edgesbased on disconnected subgraphs' intra-community connectivity is proposed. Analgorithm is developed for this edge augmentation task, based on elevating thezero eigenvalues of graph's spectrum. Upper bounds for eigenvalue elevationamplitude and for the corresponding augmented edge density are derived and areauthenticated with simulation on random graphs. The algorithm worksconsistently across synthetic and real networks, yielding desirable performanceat connecting graph components. Edge augmentation reverse-engineers graphpartition under different community detection methods (Girvan-Newman method,greedy modularity maximization, label propagation, Louvain method, and fluidcommunity), in most cases producing inter-community edges at >50% frequency.",1
"A Universal Trade-off Between the Model Size, Test Loss, and Training Loss of Linear Predictors In this work we establish an algorithm and distribution independentnon-asymptotic trade-off between the model size, excess test loss, and trainingloss of linear predictors. Specifically, we show that models that perform wellon the test data (have low excess loss) are either classical -- have trainingloss close to the noise level, or are modern -- have a much larger number ofparameters compared to the minimum needed to fit the training data exactly.",1
"Reliable amortized variational inference with physics-based latent distribution correction Bayesian inference for high-dimensional inverse problems is challenged by thecomputational costs of the forward operator and the selection of an appropriateprior distribution. Amortized variational inference addresses these challengeswhere a neural network is trained to approximate the posterior distributionover existing pairs of model and data. When fed previously unseen data andnormally distributed latent samples as input, the pretrained deep neuralnetwork -- in our case a conditional normalizing flow -- provides posteriorsamples with virtually no cost. However, the accuracy of this approach relieson the availability of high-fidelity training data, which seldom exists ingeophysical inverse problems due to the heterogeneous structure of the Earth.In addition, accurate amortized variational inference requires the observeddata to be drawn from the training data distribution. As such, we propose toincrease the resilience of amortized variational inference when faced with datadistribution shift via a physics-based correction to the conditionalnormalizing flow latent distribution. To accomplish this, instead of a standardGaussian latent distribution, we parameterize the latent distribution by aGaussian distribution with an unknown mean and diagonal covariance. Theseunknown quantities are then estimated by minimizing the Kullback-Leiblerdivergence between the corrected and true posterior distributions. Whilegeneric and applicable to other inverse problems, by means of a seismic imagingexample, we show that our correction step improves the robustness of amortizedvariational inference with respect to changes in number of source experiments,noise variance, and shifts in the prior distribution. This approach provides aseismic image with limited artifacts and an assessment of its uncertainty withapproximately the same cost as five reverse-time migrations.",1
"MAPIE: an open-source library for distribution-free uncertainty quantification Estimating uncertainties associated with the predictions of Machine Learning(ML) models is of crucial importance to assess their robustness and predictivepower. In this submission, we introduce MAPIE (Model Agnostic PredictionInterval Estimator), an open-source Python library that quantifies theuncertainties of ML models for single-output regression and multi-classclassification tasks. MAPIE implements conformal prediction methods, allowingthe user to easily compute uncertainties with strong theoretical guarantees onthe marginal coverages and with mild assumptions on the model or on theunderlying data distribution. MAPIE is hosted on scikit-learn-contrib and isfully scikit-learn-compatible. As such, it accepts any type of regressor orclassifier coming with a scikit-learn API. The library is available at:",1
"Correcting Model Bias with Sparse Implicit Processes Model selection in machine learning (ML) is a crucial part of the Bayesianlearning procedure. Model choice may impose strong biases on the resultingpredictions, which can hinder the performance of methods such as Bayesianneural networks and neural samplers. On the other hand, newly proposedapproaches for Bayesian ML exploit features of approximate inference infunction space with implicit stochastic processes (a generalization of Gaussianprocesses). The approach of Sparse Implicit Processes (SIP) is particularlysuccessful in this regard, since it is fully trainable and achieves flexiblepredictions. Here, we expand on the original experiments to show that SIP iscapable of correcting model bias when the data generating mechanism differsstrongly from the one implied by the model. We use synthetic datasets to showthat SIP is capable of providing predictive distributions that reflect the databetter than the exact predictions of the initial, but wrongly assumed model.",1
"Delayed Feedback in Generalised Linear Bandits Revisited The stochastic generalised linear bandit is a well-understood model forsequential decision-making problems, with many algorithms achievingnear-optimal regret guarantees under immediate feedback. However, in many realworld settings, the requirement that the reward is observed immediately is notapplicable. In this setting, standard algorithms are no longer theoreticallyunderstood. We study the phenomenon of delayed rewards in a theoretical mannerby introducing a delay between selecting an action and receiving the reward.Subsequently, we show that an algorithm based on the optimistic principleimproves on existing approaches for this setting by eliminating the need forprior knowledge of the delay distribution and relaxing assumptions on thedecision set and the delays. This also leads to improving the regret guaranteesfrom $ \widetilde O(\sqrt{dT}\sqrt{d + \mathbb{E}[\tau]})$ to $ \widetildeO(d\sqrt{T} + d^{3/2}\mathbb{E}[\tau])$, where $\mathbb{E}[\tau]$ denotes theexpected delay, $d$ is the dimension and $T$ the time horizon and we havesuppressed logarithmic terms. We verify our theoretical results throughexperiments on simulated data.",1
"Towards understanding how momentum improves generalization in deep learning Stochastic gradient descent (SGD) with momentum is widely used for trainingmodern deep learning architectures. While it is well-understood that usingmomentum can lead to faster convergence rate in various settings, it has alsobeen observed that momentum yields higher generalization. Prior work argue thatmomentum stabilizes the SGD noise during training and this leads to highergeneralization. In this paper, we adopt another perspective and firstempirically show that gradient descent with momentum (GD+M) significantlyimproves generalization compared to gradient descent (GD) in some deep learningproblems. From this observation, we formally study how momentum improvesgeneralization. We devise a binary classification setting where a one-hiddenlayer (over-parameterized) convolutional neural network trained with GD+Mprovably generalizes better than the same network trained with GD, when bothalgorithms are similarly initialized. The key insight in our analysis is thatmomentum is beneficial in datasets where the examples share some feature butdiffer in their margin. Contrary to GD that memorizes the small margin data,GD+M still learns the feature in these data thanks to its historical gradients.Lastly, we empirically validate our theoretical findings.",1
"Efficient Search of Multiple Neural Architectures with Different Complexities via Importance Sampling Neural architecture search (NAS) aims to automate architecture designprocesses and improve the performance of deep neural networks. Platform-awareNAS methods consider both performance and complexity and can findwell-performing architectures with low computational resources. Althoughordinary NAS methods result in tremendous computational costs owing to therepetition of model training, one-shot NAS, which trains the weights of asupernetwork containing all candidate architectures only once during the searchprocess, has been reported to result in a lower search cost. This study focuseson the architecture complexity-aware one-shot NAS that optimizes the objectivefunction composed of the weighted sum of two metrics, such as the predictiveperformance and number of parameters. In existing methods, the architecturesearch process must be run multiple times with different coefficients of theweighted sum to obtain multiple architectures with different complexities. Thisstudy aims at reducing the search cost associated with finding multiplearchitectures. The proposed method uses multiple distributions to generatearchitectures with different complexities and updates each distribution usingthe samples obtained from multiple distributions based on importance sampling.The proposed method allows us to obtain multiple architectures with differentcomplexities in a single architecture search, resulting in reducing the searchcost. The proposed method is applied to the architecture search ofconvolutional neural networks on the CIAFR-10 and ImageNet datasets.Consequently, compared with baseline methods, the proposed method findsmultiple architectures with varying complexities while requiring lesscomputational effort.",1
"Differentially Private Estimation via Statistical Depth Constructing a differentially private (DP) estimator requires deriving themaximum influence of an observation, which can be difficult in the absence ofexogenous bounds on the input data or the estimator, especially in highdimensional settings. This paper shows that standard notions of statisticaldepth, i.e., halfspace depth and regression depth, are particularlyadvantageous in this regard, both in the sense that the maximum influence of asingle observation is easy to analyze and that this value is typically low.This is used to motivate new approximate DP location and regression estimatorsusing the maximizers of these two notions of statistical depth. A morecomputationally efficient variant of the approximate DP regression estimator isalso provided. Also, to avoid requiring that users specify a priori bounds onthe estimates and/or the observations, variants of these DP mechanisms aredescribed that satisfy random differential privacy (RDP), which is a relaxationof differential privacy provided by Hall, Wasserman, and Rinaldo (2013). Wealso provide simulations of the two DP regression methods proposed here. Theproposed estimators appear to perform favorably relative to the existing DPregression methods we consider in these simulations when either the sample sizeis at least 100-200 or the privacy-loss budget is sufficiently high.",1
"Online Lewis Weight Sampling The seminal work of Cohen and Peng introduced Lewis weight sampling to thetheoretical computer science community, yielding fast row sampling algorithmsfor approximating $d$-dimensional subspaces of $\ell_p$ up to $(1+\epsilon)$error. Several works have extended this important primitive to other settings,including the online coreset, sliding window, and adversarial streaming models.However, these results are only for $p\in\{1,2\}$, and results for $p=1$require a suboptimal $\tilde O(d^2/\epsilon^2)$ samples.",1
"Causal Graphs Underlying Generative Models: Path to Learning with Limited Data Training generative models that capture rich semantics of the data andinterpreting the latent representations encoded by such models are veryimportant problems in unsupervised learning. In this work, we provide a simplealgorithm that relies on perturbation experiments on latent codes of apre-trained generative autoencoder to uncover a causal graph that is implied bythe generative model. We leverage pre-trained attribute classifiers and performperturbation experiments to check for influence of a given latent variable on asubset of attributes. Given this, we show that one can fit an effective causalgraph that models a structural equation model between latent codes taken asexogenous variables and attributes taken as observed variables. One interestingaspect is that a single latent variable controls multiple overlapping subsetsof attributes unlike conventional approach that tries to impose fullindependence. Using a pre-trained RNN-based generative autoencoder trained on adataset of peptide sequences, we demonstrate that the learnt causal graph fromour algorithm between various attributes and latent codes can be used topredict a specific property for sequences which are unseen. We compareprediction models trained on either all available attributes or only the onesin the Markov blanket and empirically show that in both the unsupervised andsupervised regimes, typically, using the predictor that relies on Markovblanket attributes generalizes better for out-of-distribution sequences.",1
"Partial Disentanglement via Mechanism Sparsity Disentanglement via mechanism sparsity was introduced recently as aprincipled approach to extract latent factors without supervision when thecausal graph relating them in time is sparse, and/or when actions are observedand affect them sparsely. However, this theory applies only to ground-truthgraphs satisfying a specific criterion. In this work, we introduce ageneralization of this theory which applies to any ground-truth graph andspecifies qualitatively how disentangled the learned representation is expectedto be, via a new equivalence relation over models we call consistency. Thisequivalence captures which factors are expected to remain entangled and whichare not based on the specific form of the ground-truth graph. We call thisweaker form of identifiability partial disentanglement. The graphical criterionthat allows complete disentanglement, proposed in an earlier work, can bederived as a special case of our theory. Finally, we enforce graph sparsitywith constrained optimization and illustrate our theory and algorithm insimulations.",1
"Comparing Feature Importance and Rule Extraction for Interpretability on Text Data Complex machine learning algorithms are used more and more often in criticaltasks involving text data, leading to the development of interpretabilitymethods. Among local methods, two families have emerged: those computingimportance scores for each feature and those extracting simple logical rules.In this paper we show that using different methods can lead to unexpectedlydifferent explanations, even when applied to simple models for which we wouldexpect qualitative coincidence. To quantify this effect, we propose a newapproach to compare explanations produced by different methods.",1
"Private Convex Optimization in General Norms We propose a new framework for differentially private optimization of convexfunctions which are Lipschitz in an arbitrary norm $\normx{\cdot}$. Ouralgorithms are based on a regularized exponential mechanism which samples fromthe density $\propto \exp(-k(F+\mu r))$ where $F$ is the empirical loss and $r$is a regularizer which is strongly convex with respect to $\normx{\cdot}$,generalizing a recent work of \cite{GLL22} to non-Euclidean settings. We showthat this mechanism satisfies Gaussian differential privacy and solves bothDP-ERM (empirical risk minimization) and DP-SCO (stochastic convexoptimization), by using localization tools from convex geometry. Our frameworkis the first to apply to private convex optimization in general normed spaces,and directly recovers non-private SCO rates achieved by mirror descent, as theprivacy parameter $\eps \to \infty$. As applications, for Lipschitzoptimization in $\ell_p$ norms for all $p \in (1, 2)$, we obtain the firstoptimal privacy-utility tradeoffs; for $p = 1$, we improve tradeoffs obtainedby the recent works \cite{AsiFKT21, BassilyGN21} by at least a logarithmicfactor. Our $\ell_p$ norm and Schatten-$p$ norm optimization frameworks arecomplemented with polynomial-time samplers whose query complexity we explicitlybound.",1
"Future-Dependent Value-Based Off-Policy Evaluation in POMDPs We study off-policy evaluation (OPE) for partially observable MDPs (POMDPs)with general function approximation. Existing methods such as sequentialimportance sampling estimators and fitted-Q evaluation suffer from the curse ofhorizon in POMDPs. To circumvent this problem, we develop a novel model-freeOPE method by introducing future-dependent value functions that take futureproxies as inputs. Future-dependent value functions play similar roles asclassical value functions in fully-observable MDPs. We derive a new Bellmanequation for future-dependent value functions as conditional moment equationsthat use history proxies as instrumental variables. We further propose aminimax learning method to learn future-dependent value functions using the newBellman equation. We obtain the PAC result, which implies our OPE estimator isconsistent as long as futures and histories contain sufficient informationabout latent states, and the Bellman completeness. Finally, we extend ourmethods to learning of dynamics and establish the connection between ourapproach and the well-known spectral learning methods in POMDPs.",1
"Improved conformalized quantile regression Conformalized quantile regression is a procedure that inherits the advantagesof conformal prediction and quantile regression. That is, we use quantileregression to estimate the true conditional quantile and then apply a conformalstep on a calibration set to ensure marginal coverage. In this way, we getadaptive prediction intervals that account for heteroscedasticity. However, theaforementioned conformal step lacks adaptiveness as described in (Romano etal., 2019). To overcome this limitation, instead of applying a single conformalstep after estimating conditional quantiles with quantile regression, wepropose to cluster the explanatory variables weighted by their permutationimportance with an optimized k-means and apply k conformal steps. To show thatthis improved version outperforms the classic version of conformalized quantileregression and is more adaptive to heteroscedasticity, we extensively comparethe prediction intervals of both in open datasets.",1
"Rewiring Networks for Graph Neural Network Training Using Discrete Geometry Information over-squashing is a phenomenon of inefficient informationpropagation between distant nodes on networks. It is an important problem thatis known to significantly impact the training of graph neural networks (GNNs),as the receptive field of a node grows exponentially. To mitigate this problem,a preprocessing procedure known as rewiring is often applied to the inputnetwork. In this paper, we investigate the use of discrete analogues ofclassical geometric notions of curvature to model information flow on networksand rewire them. We show that these classical notions achieve state-of-the-artperformance in GNN training accuracy on a variety of real-world networkdatasets. Moreover, compared to the current state-of-the-art, these classicalnotions exhibit a clear advantage in computational runtime by several orders ofmagnitude.",1
"Homomorphism Autoencoder -- Learning Group Structured Representations from Observed Transitions How can we acquire world models that veridically represent the outside worldboth in terms of what is there and in terms of how our actions affect it? Canwe acquire such models by interacting with the world, and can we statemathematical desiderata for their relationship with a hypothetical realityexisting outside our heads? As machine learning is moving towardsrepresentations containing not just observational but also interventionalknowledge, we study these problems using tools from representation learning andgroup theory. Under the assumption that our actuators act upon the world, wepropose methods to learn internal representations of not just sensoryinformation but also of actions that modify our sensory representations in away that is consistent with the actions and transitions in the world. We use anautoencoder equipped with a group representation linearly acting on its latentspace, trained on 2-step reconstruction such as to enforce a suitablehomomorphism property on the group representation. Compared to existing work,our approach makes fewer assumptions on the group representation and on whichtransformations the agent can sample from the group. We motivate our methodtheoretically, and demonstrate empirically that it can learn the correctrepresentation of the groups and the topology of the environment. We alsocompare its performance in trajectory prediction with previous methods.",1
"Neural Stein critics with staged $L^2$-regularization Learning to differentiate model distributions from observed data is afundamental problem in statistics and machine learning, and high-dimensionaldata remains a challenging setting for such problems. Metrics that quantify thedisparity in probability distributions, such as the Stein discrepancy, play animportant role in statistical testing in high dimensions. In this paper, weconsider the setting where one wishes to distinguish between data sampled froman unknown probability distribution and a nominal model distribution. Whilerecent studies revealed that the optimal $L^2$-regularized Stein critic equalsthe difference of the score functions of two probability distributions up to amultiplicative constant, we investigate the role of $L^2$ regularization whentraining a neural network Stein discrepancy critic function. Motivated by theNeural Tangent Kernel theory of training neural networks, we develop a novelstaging procedure for the weight of regularization over training time. Thisleverages the advantages of highly-regularized training at early times whilealso empirically delaying overfitting. Theoretically, we relate the trainingdynamic with large regularization weight to the kernel regression optimizationof lazy training regime in early training times. The benefit of the staged$L^2$ regularization is demonstrated on simulated high dimensional distributiondrift data and an application to evaluating generative models of image data.",1
"D-CBRS: Accounting For Intra-Class Diversity in Continual Learning Continual learning -- accumulating knowledge from a sequence of learningexperiences -- is an important yet challenging problem. In this paradigm, themodel's performance for previously encountered instances may substantially dropas additional data are seen. When dealing with class-imbalanced data,forgetting is further exacerbated. Prior work has proposed replay-basedapproaches which aim at reducing forgetting by intelligently storing instancesfor future replay. Although Class-Balancing Reservoir Sampling (CBRS) has beensuccessful in dealing with imbalanced data, the intra-class diversity has notbeen accounted for, implicitly assuming that each instance of a class isequally informative. We present Diverse-CBRS (D-CBRS), an algorithm that allowsus to consider within class diversity when storing instances in the memory. Ourresults show that D-CBRS outperforms state-of-the-art memory managementcontinual learning algorithms on data sets with considerable intra-classdiversity.",1
"Selection of the Most Probable Best We consider an expected-value ranking and selection problem where all ksolutions' simulation outputs depend on a common uncertain input model. Giventhat the uncertainty of the input model is captured by a probability simplex ona finite support, we define the most probable best (MPB) to be the solutionwhose probability of being optimal is the largest. To devise an efficientsampling algorithm to find the MPB, we first derive a lower bound to the largedeviation rate of the probability of falsely selecting the MPB, then formulatean optimal computing budget allocation (OCBA) problem to find the optimalstatic sampling ratios for all solution-input model pairs that maximize thelower bound. We devise a series of sequential algorithms that applyinterpretable and computationally efficient sampling rules and prove theirsampling ratios achieve the optimality conditions for the OCBA problem as thesimulation budget increases. The algorithms are benchmarked against astate-of-the-art sequential sampling algorithm designed for contextual rankingand selection problems and demonstrated to have superior empirical performancesat finding the MPB.",1
"Generalized Identifiability Bounds for Mixture Models with Grouped Samples Recent work has shown that finite mixture models with $m$ components areidentifiable, while making no assumptions on the mixture components, so long asone has access to groups of samples of size $2m-1$ which are known to come fromthe same mixture component. In this work we generalize that result and showthat, if every subset of $k$ mixture components of a mixture model are linearlyindependent, then that mixture model is identifiable with only $(2m-1)/(k-1)$samples per group. We further show that this value cannot be improved. We provean analogous result for a stronger form of identifiability known asdeterminedness along with a corresponding lower bound. This independenceassumption almost surely holds if mixture components are chosen randomly from a$k$-dimensional space. We describe some implications of our results formultinomial mixture models and topic modeling.",1
"Black and Gray Box Learning of Amplitude Equations: Application to Phase Field Systems We present a data-driven approach to learning surrogate models for amplitudeequations, and illustrate its application to interfacial dynamics of phasefield systems. In particular, we demonstrate learning effective partialdifferential equations describing the evolution of phase field interfaces fromfull phase field data. We illustrate this on a model phase field system, whereanalytical approximate equations for the dynamics of the phase field interface(a higher order eikonal equation and its approximation, the Kardar-Parisi-Zhang(KPZ) equation) are known. For this system, we discuss data-driven approachesfor the identification of equations that accurately describe the frontinterface dynamics. When the analytical approximate models mentioned abovebecome inaccurate, as we move beyond the region of validity of the underlyingassumptions, the data-driven equations outperform them. In these regimes, goingbeyond black-box identification, we explore different approaches to learndata-driven corrections to the analytically approximate models, leading toeffective gray box partial differential equations.",1
"Statistical Inference with Stochastic Gradient Algorithms Stochastic gradient algorithms are widely used for both optimization andsampling in large-scale learning and inference problems. However, in practice,tuning these algorithms is typically done using heuristics and trial-and-errorrather than rigorous, generalizable theory. To address this gap between theoryand practice, we novel insights into the effect of tuning parameters bycharacterizing the large-sample behavior of iterates of a very general class ofpreconditioned stochastic gradient algorithms with fixed step size. In theoptimization setting, our results show that iterate averaging with a largefixed step size can result in statistically efficient approximation of the(local) M-estimator. In the sampling context, our results show that withappropriate choices of tuning parameters, the limiting stationary covariancecan match either the Bernstein--von Mises limit of the posterior, adjustmentsto the posterior for model misspecification, or the asymptotic distribution ofthe MLE; and that with a naive tuning the limit corresponds to none of these.Moreover, we argue that an essentially independent sample from the stationarydistribution can be obtained after a fixed number of passes over the dataset.We validate our asymptotic results in realistic finite-sample regimes viaseveral experiments using simulated and real data. Overall, we demonstrate thatproperly tuned stochastic gradient algorithms with constant step size offer acomputationally efficient and statistically robust approach to obtaining pointestimates or posterior-like samples.",1
"Forget-me-not! Contrastive Critics for Mitigating Posterior Collapse Variational autoencoders (VAEs) suffer from posterior collapse, where thepowerful neural networks used for modeling and inference optimize the objectivewithout meaningfully using the latent representation. We introduce inferencecritics that detect and incentivize against posterior collapse by requiringcorrespondence between latent variables and the observations. By connecting thecritic's objective to the literature in self-supervised contrastiverepresentation learning, we show both theoretically and empirically thatoptimizing inference critics increases the mutual information betweenobservations and latents, mitigating posterior collapse. This approach isstraightforward to implement and requires significantly less training time thanprior methods, yet obtains competitive results on three established datasets.Overall, the approach lays the foundation to bridge the previously disconnectedframeworks of contrastive learning and probabilistic modeling with variationalautoencoders, underscoring the benefits both communities may find at theirintersection.",1
"Lagrangian Density Space-Time Deep Neural Network Topology As a network-based functional approximator, we have proposed a LagrangianDensity Space-Time Deep Neural Networks (LDDNN) topology. It is qualified forunsupervised training and learning to predict the dynamics of underlyingphysical science governed phenomena. The prototypical network respects thefundamental conservation laws of nature through the succinctly describedLagrangian and Hamiltonian density of the system by a given data-set ofgeneralized nonlinear partial differential equations. The objective is toparameterize the Lagrangian density over a neural network and directly learnfrom it through data instead of hand-crafting an exact time-dependent Actionsolution of Lagrangian density for the physical system. With this novelapproach, can understand and open up the information inference aspect of theBlack-box deep machine learning representation for the physical dynamics ofnature by constructing custom-tailored network interconnect topologies,activation, and loss/cost functions based on the underlying physicaldifferential operators. This article will discuss statistical physicsinterpretation of neural networks in the Lagrangian and Hamiltonian domains.",1
"Efficient Real-world Testing of Causal Decision Making via Bayesian Experimental Design for Contextual Optimisation The real-world testing of decisions made using causal machine learning modelsis an essential prerequisite for their successful application. We focus onevaluating and improving contextual treatment assignment decisions: these arepersonalised treatments applied to e.g. customers, each with their owncontextual information, with the aim of maximising a reward. In this paper weintroduce a model-agnostic framework for gathering data to evaluate and improvecontextual decision making through Bayesian Experimental Design. Specifically,our method is used for the data-efficient evaluation of the regret of pasttreatment assignments. Unlike approaches such as A/B testing, our method avoidsassigning treatments that are known to be highly sub-optimal, whilst engagingin some exploration to gather pertinent information. We achieve this byintroducing an information-based design objective, which we optimiseend-to-end. Our method applies to discrete and continuous treatments. Comparingour information-theoretic approach to baselines in several simulation studiesdemonstrates the superior performance of our proposed approach.",1
"Meta-Learning a Real-Time Tabular AutoML Method For Small Data We present TabPFN, an AutoML method that is competitive with the state of theart on small tabular datasets while being over 1,000$\times$ faster. Our methodis very simple: it is fully entailed in the weights of a single neural network,and a single forward pass directly yields predictions for a new dataset. OurAutoML method is meta-learned using the Transformer-based Prior-Data FittedNetwork (PFN) architecture and approximates Bayesian inference with a priorthat is based on assumptions of simplicity and causal structures. The priorcontains a large space of structural causal models and Bayesian neural networkswith a bias for small architectures and thus low complexity. Furthermore, weextend the PFN approach to differentiably calibrate the prior's hyperparameterson real data. By doing so, we separate our abstract prior assumptions fromtheir heuristic calibration on real data. Afterwards, the calibratedhyperparameters are fixed and TabPFN can be applied to any new tabular datasetat the push of a button. Finally, on 30 datasets from the OpenML-CC18 suite weshow that our method outperforms boosted trees and performs on par with complexstate-of-the-art AutoML systems with predictions produced in less than asecond. We provide all our code and our final trained TabPFN in thesupplementary materials.",1
"Measuring and signing fairness as performance under multiple stakeholder distributions As learning machines increase their influence on decisions concerning humanlives, analyzing their fairness properties becomes a subject of centralimportance. Yet, our best tools for measuring the fairness of learning systemsare rigid fairness metrics encapsulated as mathematical one-liners, offerlimited power to the stakeholders involved in the prediction task, and are easyto manipulate when we exhort excessive pressure to optimize them. To advancethese issues, we propose to shift focus from shaping fairness metrics tocurating the distributions of examples under which these are computed. Inparticular, we posit that every claim about fairness should be immediatelyfollowed by the tagline Fair under what examples, and collected by whom?. Byhighlighting connections to the literature in domain generalization, we proposeto measure fairness as the ability of the system to generalize under multiplestress tests -- distributions of examples with social relevance. We encourageeach stakeholder to curate one or multiple stress tests containing examplesreflecting their (possibly conflicting) interests. The machine passes or failseach stress test by falling short of or exceeding a pre-defined metric value.The test results involve all stakeholders in a discussion about how to improvethe learning system, and provide flexible assessments of fairness dependent oncontext and based on interpretable data. We provide full implementationguidelines for stress testing, illustrate both the benefits and shortcomings ofthis framework, and introduce a cryptographic scheme to enable a degree ofprediction accountability from system providers.",1
"Finite-time High-probability Bounds for Polyak-Ruppert Averaged Iterates of Linear Stochastic Approximation This paper provides a finite-time analysis of linear stochastic approximation(LSA) algorithms with fixed step size, a core method in statistics and machinelearning. LSA is used to compute approximate solutions of a $d$-dimensionallinear system $\bar{\mathbf{A}} \theta = \bar{\mathbf{b}}$, for which$(\bar{\mathbf{A}}, \bar{\mathbf{b}})$ can only be estimated through(asymptotically) unbiased observations$\{(\mathbf{A}(Z_n),\mathbf{b}(Z_n))\}_{n \in \mathbb{N}}$. We consider herethe case where $\{Z_n\}_{n \in \mathbb{N}}$ is an i.i.d. sequence or auniformly geometrically ergodic Markov chain, and derive $p$-moments inequalityand high probability bounds for the iterates defined by LSA and itsPolyak-Ruppert averaged version. More precisely, we establish bounds of order$(p \alpha t_{\operatorname{mix}})^{1/2}d^{1/p}$ on the $p$-th moment of thelast iterate of LSA. In this formula $\alpha$ is the step size of the procedureand $t_{\operatorname{mix}}$ is the mixing time of the underlying chain($t_{\operatorname{mix}}=1$ in the i.i.d. setting). We then prove finite-timeinstance-dependent bounds on the Polyak-Ruppert averaged sequence of iterates.These results are sharp in the sense that the leading term we obtain matchesthe local asymptotic minimax limit, including tight dependence on theparameters $(d,t_{\operatorname{mix}})$ in the higher order terms.",1
"A Near-Optimal Primal-Dual Method for Off-Policy Learning in CMDP As an important framework for safe Reinforcement Learning, the ConstrainedMarkov Decision Process (CMDP) has been extensively studied in the recentliterature. However, despite the rich results under various on-policy learningsettings, there still lacks some essential understanding of the offline CMDPproblems, in terms of both the algorithm design and the information theoreticsample complexity lower bound. In this paper, we focus on solving the CMDPproblems where only offline data are available. By adopting the concept of thesingle-policy concentrability coefficient $C^*$, we establish an$\Omega\left(\frac{\min\left\{|\mathcal{S}||\mathcal{A}|,|\mathcal{S}|+I\right\}C^*}{(1-\gamma)^3\epsilon^2}\right)$ sample complexity lower bound for theoffline CMDP problem, where $I$ stands for the number of constraints. Byintroducing a simple but novel deviation control mechanism, we propose anear-optimal primal-dual learning algorithm called DPDL. This algorithmprovably guarantees zero constraint violation and its sample complexity matchesthe above lower bound except for an $\tilde{\mathcal{O}}((1-\gamma)^{-1})$factor. Comprehensive discussion on how to deal with the unknown constant $C^*$and the potential asynchronous structure on the offline dataset are alsoincluded.",1
"Breaking Feedback Loops in Recommender Systems with Causal Inference Recommender systems play a key role in shaping modern web ecosystems. Thesesystems alternate between (1) making recommendations (2) collecting userresponses to these recommendations, and (3) retraining the recommendationalgorithm based on this feedback. During this process the recommender systeminfluences the user behavioral data that is subsequently used to update it,thus creating a feedback loop. Recent work has shown that feedback loops maycompromise recommendation quality and homogenize user behavior, raising ethicaland performance concerns when deploying recommender systems. To address theseissues, we propose the Causal Adjustment for Feedback Loops (CAFL), analgorithm that provably breaks feedback loops using causal inference and can beapplied to any recommendation algorithm that optimizes a training loss. Ourmain observation is that a recommender system does not suffer from feedbackloops if it reasons about causal quantities, namely the interventiondistributions of recommendations on user ratings. Moreover, we can calculatethis intervention distribution from observational data by adjusting for therecommender system's predictions of user preferences. Using simulatedenvironments, we demonstrate that CAFL improves recommendation quality whencompared to prior correction methods.",1
"Efficient One Sided Kolmogorov Approximation We present an efficient algorithm that, given a discrete random variable $X$and a number $m$, computes a random variable whose support is of size at most$m$ and whose Kolmogorov distance from $X$ is minimal, also for the one-sidedKolmogorov approximation. We present some variants of the algorithm, analysetheir correctness and computational complexity, and present a detailedempirical evaluation that shows how they performs in practice. The mainapplication that we examine, which is our motivation for this work, isestimation of the probability missing deadlines in series-parallel schedules.Since exact computation of these probabilities is NP-hard, we propose to usethe algorithms described in this paper to obtain an approximation.",1
"Making Linear MDPs Practical via Contrastive Representation Learning It is common to address the curse of dimensionality in Markov decisionprocesses (MDPs) by exploiting low-rank representations. This motivates much ofthe recent theoretical study on linear MDPs. However, most approaches require agiven representation under unrealistic assumptions about the normalization ofthe decomposition or introduce unresolved computational challenges in practice.Instead, we consider an alternative definition of linear MDPs thatautomatically ensures normalization while allowing efficient representationlearning via contrastive estimation. The framework also admitsconfidence-adjusted index algorithms, enabling an efficient and principledapproach to incorporating optimism or pessimism in the face of uncertainty. Tothe best of our knowledge, this provides the first practical representationlearning method for linear MDPs that achieves both strong theoreticalguarantees and empirical performance. Theoretically, we prove that the proposedalgorithm is sample efficient in both the online and offline settings.Empirically, we demonstrate superior performance over existing state-of-the-artmodel-based and model-free algorithms on several benchmarks.",1
"Contextual Bandits with Smooth Regret: Efficient Learning in Continuous Action Spaces Designing efficient general-purpose contextual bandit algorithms that workwith large -- or even continuous -- action spaces would facilitate applicationto important scenarios such as information retrieval, recommendation systems,and continuous control. While obtaining standard regret guarantees can behopeless, alternative regret notions have been proposed to tackle the largeaction setting. We propose a smooth regret notion for contextual bandits, whichdominates previously proposed alternatives. We design a statistically andcomputationally efficient algorithm -- for the proposed smooth regret -- thatworks with general function approximation under standard supervised oracles. Wealso present an adaptive algorithm that automatically adapts to any smoothnesslevel. Our algorithms can be used to recover the previous minimax/Paretooptimal guarantees under the standard regret, e.g., in bandit problems withmultiple best arms and Lipschitz/H{}lder bandits. We conduct large-scaleempirical evaluations demonstrating the efficacy of our proposed algorithms.",1
"Multiple Robust Learning for Recommendation In recommender systems, a common problem is the presence of various biases inthe collected data, which deteriorates the generalization ability of therecommendation models and leads to inaccurate predictions. Doubly robust (DR)learning has been studied in many tasks in RS, with the advantage that unbiasedlearning can be achieved when either a single imputation or a single propensitymodel is accurate. In this paper, we propose a multiple robust (MR) estimatorthat can take the advantage of multiple candidate imputation and propensitymodels to achieve unbiasedness. Specifically, the MR estimator is unbiased whenany of the imputation or propensity models, or a linear combination of thesemodels is accurate. Theoretical analysis shows that the proposed MR is anenhanced version of DR when only having a single imputation and propensitymodel, and has a smaller bias. Inspired by the generalization error bound ofMR, we further propose a novel multiple robust learning approach withstabilization. We conduct extensive experiments on real-world andsemi-synthetic datasets, which demonstrates the superiority of the proposedapproach over state-of-the-art methods.",1
"Learning to Increase the Power of Conditional Randomization Tests The model-X conditional randomization test is a generic framework forconditional independence testing, unlocking new possibilities to discoverfeatures that are conditionally associated with a response of interest whilecontrolling type-I error rates. An appealing advantage of this test is that itcan work with any machine learning model to design powerful test statistics. Inturn, the common practice in the model-X literature is to form a test statisticusing machine learning models, trained to maximize predictive accuracy with thehope to attain a test with good power. However, the ideal goal here is to drivethe model (during training) to maximize the power of the test, not merely thepredictive accuracy. In this paper, we bridge this gap by introducing, for thefirst time, novel model-fitting schemes that are designed to explicitly improvethe power of model-X tests. This is done by introducing a new cost functionthat aims at maximizing the test statistic used to measure violations ofconditional independence. Using synthetic and real data sets, we demonstratethat the combination of our proposed loss function with various base predictivemodels (lasso, elastic net, and deep neural networks) consistently increasesthe number of correct discoveries obtained, while maintaining type-I errorrates under control.",1
"Tree ensemble kernels for Bayesian optimization with known constraints over mixed-feature spaces Tree ensembles can be well-suited for black-box optimization tasks such asalgorithm tuning and neural architecture search, as they achieve goodpredictive performance with little to no manual tuning, naturally handlediscrete feature spaces, and are relatively insensitive to outliers in thetraining data. Two well-known challenges in using tree ensembles for black-boxoptimization are (i) effectively quantifying model uncertainty for explorationand (ii) optimizing over the piece-wise constant acquisition function. Toaddress both points simultaneously, we propose using the kernel interpretationof tree ensembles as a Gaussian Process prior to obtain model varianceestimates, and we develop a compatible optimization formulation for theacquisition function. The latter further allows us to seamlessly integrateknown constraints to improve sampling efficiency by consideringdomain-knowledge in engineering settings and modeling search space symmetries,e.g., hierarchical relationships in neural architecture search. Our frameworkperforms as well as state-of-the-art methods for unconstrained black-boxoptimization over continuous/discrete features and outperforms competingmethods for problems combining mixed-variable feature spaces and known inputconstraints.",1
"Statistical Inference with Stochastic Gradient Algorithms Stochastic gradient algorithms are widely used for both optimization andsampling in large-scale learning and inference problems. However, in practice,tuning these algorithms is typically done using heuristics and trial-and-errorrather than rigorous, generalizable theory. To address this gap between theoryand practice, we novel insights into the effect of tuning parameters bycharacterizing the large-sample behavior of iterates of a very general class ofpreconditioned stochastic gradient algorithms with fixed step size. In theoptimization setting, our results show that iterate averaging with a largefixed step size can result in statistically efficient approximation of the(local) M-estimator. In the sampling context, our results show that withappropriate choices of tuning parameters, the limiting stationary covariancecan match either the Bernstein--von Mises limit of the posterior, adjustmentsto the posterior for model misspecification, or the asymptotic distribution ofthe MLE; and that with a naive tuning the limit corresponds to none of these.Moreover, we argue that an essentially independent sample from the stationarydistribution can be obtained after a fixed number of passes over the dataset.We validate our asymptotic results in realistic finite-sample regimes viaseveral experiments using simulated and real data. Overall, we demonstrate thatproperly tuned stochastic gradient algorithms with constant step size offer acomputationally efficient and statistically robust approach to obtaining pointestimates or posterior-like samples.",1
"Finite-time High-probability Bounds for Polyak-Ruppert Averaged Iterates of Linear Stochastic Approximation This paper provides a finite-time analysis of linear stochastic approximation(LSA) algorithms with fixed step size, a core method in statistics and machinelearning. LSA is used to compute approximate solutions of a $d$-dimensionallinear system $\bar{\mathbf{A}} \theta = \bar{\mathbf{b}}$, for which$(\bar{\mathbf{A}}, \bar{\mathbf{b}})$ can only be estimated through(asymptotically) unbiased observations$\{(\mathbf{A}(Z_n),\mathbf{b}(Z_n))\}_{n \in \mathbb{N}}$. We consider herethe case where $\{Z_n\}_{n \in \mathbb{N}}$ is an i.i.d. sequence or auniformly geometrically ergodic Markov chain, and derive $p$-moments inequalityand high probability bounds for the iterates defined by LSA and itsPolyak-Ruppert averaged version. More precisely, we establish bounds of order$(p \alpha t_{\operatorname{mix}})^{1/2}d^{1/p}$ on the $p$-th moment of thelast iterate of LSA. In this formula $\alpha$ is the step size of the procedureand $t_{\operatorname{mix}}$ is the mixing time of the underlying chain($t_{\operatorname{mix}}=1$ in the i.i.d. setting). We then prove finite-timeinstance-dependent bounds on the Polyak-Ruppert averaged sequence of iterates.These results are sharp in the sense that the leading term we obtain matchesthe local asymptotic minimax limit, including tight dependence on theparameters $(d,t_{\operatorname{mix}})$ in the higher order terms.",1
"Gradients should stay on Path: Better Estimators of the Reverse- and Forward KL Divergence for Normalizing Flows We propose an algorithm to estimate the path-gradient of both the reverse andforward Kullback-Leibler divergence for an arbitrary manifestly invertiblenormalizing flow. The resulting path-gradient estimators are straightforward toimplement, have lower variance, and lead not only to faster convergence oftraining but also to better overall approximation results compared to standardtotal gradient estimators. We also demonstrate that path-gradient training isless susceptible to mode-collapse. In light of our results, we expect thatpath-gradient estimators will become the new standard method to trainnormalizing flows for variational inference.",1
"Probabilistic Reconciliation of Count Time Series We propose a principled method for the reconciliation of any probabilisticbase forecasts. We show how probabilistic reconciliation can be obtained bymerging, via Bayes' rule, the information contained in the base forecast forthe bottom and the upper time series. We illustrate our method on a toyhierarchy, showing how our framework allows the probabilistic reconciliation ofany base forecast. We perform experiment in the reconciliation of temporalhierarchies of count time series, obtaining major improvements compared toprobabilistic reconciliation based on the Gaussian or the truncated Gaussiandistribution.",1
"Lazy Estimation of Variable Importance for Large Neural Networks As opaque predictive models increasingly impact many areas of modern life,interest in quantifying the importance of a given input variable for making aspecific prediction has grown. Recently, there has been a proliferation ofmodel-agnostic methods to measure variable importance (VI) that analyze thedifference in predictive power between a full model trained on all variablesand a reduced model that excludes the variable(s) of interest. A bottleneckcommon to these methods is the estimation of the reduced model for eachvariable (or subset of variables), which is an expensive process that oftendoes not come with theoretical guarantees. In this work, we propose a fast andflexible method for approximating the reduced model with important inferentialguarantees. We replace the need for fully retraining a wide neural network by alinearization initialized at the full model parameters. By adding a ridge-likepenalty to make the problem convex, we prove that when the ridge penaltyparameter is sufficiently large, our method estimates the variable importancemeasure with an error rate of $O(\frac{1}{\sqrt{n}})$ where $n$ is the numberof training samples. We also show that our estimator is asymptotically normal,enabling us to provide confidence bounds for the VI estimates. We demonstratethrough simulations that our method is fast and accurate under severaldata-generating regimes, and we demonstrate its real-world applicability on aseasonal climate forecasting example.",1
"Personalized PCA: Decoupling Shared and Unique Features In this paper, we tackle a significant challenge in PCA: heterogeneity. Whendata are collected from different sources with heterogeneous trends while stillsharing some congruency, it is critical to extract shared knowledge whileretaining unique features of each source. To this end, we propose personalizedPCA (PerPCA), which uses mutually orthogonal global and local principalcomponents to encode both unique and shared features. We show that, under mildconditions, both unique and shared features can be identified and recovered bya constrained optimization problem, even if the covariance matrices areimmensely different. Also, we design a fully federated algorithm inspired bydistributed Stiefel gradient descent to solve the problem. The algorithmintroduces a new group of operations called generalized retractions to handleorthogonality constraints, and only requires global PCs to be shared acrosssources. We prove the linear convergence of the algorithm under suitableassumptions. Comprehensive numerical experiments highlight PerPCA's superiorperformance in feature extraction and prediction from heterogeneous datasets.As a systematic approach to decouple shared and unique features fromheterogeneous datasets, PerPCA finds applications in several tasks includingvideo segmentation, topic extraction, and distributed clustering.",1
"Variance estimation in graphs with the fused lasso We study the problem of variance estimation in general graph-structuredproblems. First, we develop a linear time estimator for the homoscedastic casethat can consistently estimate the variance in general graphs. We show that ourestimator attains minimax rates for the chain and 2D grid graphs when the meansignal has a total variation with canonical scaling. Furthermore, we providegeneral upper bounds on the mean squared error performance of the fused lassoestimator in general graphs under a moment condition and a bound on the tailbehavior of the errors. These upper bounds allow us to generalize for broaderclasses of distributions, such as sub-Exponential, many existing results on thefused lasso that are only known to hold with the assumption that errors aresub-Gaussian random variables. Exploiting our upper bounds, we then study asimple total variation regularization estimator for estimating the signal ofvariances in the heteroscedastic case. Our results show that the varianceestimator attains minimax rates for estimating signals of bounded variation ingrid graphs, $K$-nearest neighbor graphs with very mild assumptions, and it isconsistent for estimating the variances in any connected graph. In addition,extensive numerical results show that our proposed estimators performreasonably well in a variety of graph-structured models.",1
"Fully Decentralized Model-based Policy Optimization for Networked Systems Reinforcement learning algorithms require a large amount of samples; thisoften limits their real-world applications on even simple tasks. Such achallenge is more outstanding in multi-agent tasks, as each step of operationis more costly requiring communications or shifting or resources. This workaims to improve data efficiency of multi-agent control by model-based learning.We consider networked systems where agents are cooperative and communicate onlylocally with their neighbors, and propose the decentralized model-based policyoptimization framework (DMPO). In our method, each agent learns a dynamic modelto predict future states and broadcast their predictions by communication, andthen the policies are trained under the model rollouts. To alleviate the biasof model-generated data, we restrain the model usage for generating myopicrollouts, thus reducing the compounding error of model generation. To pertainthe independence of policy update, we introduce extended value function andtheoretically prove that the resulting policy gradient is a close approximationto true policy gradients. We evaluate our algorithm on several benchmarks forintelligent transportation systems, which are connected autonomous vehiclecontrol tasks (Flow and CACC) and adaptive traffic signal control (ATSC).Empirically results show that our method achieves superior data efficiency andmatches the performance of model-free methods using true models.",1
"Learning Bellman Complete Representations for Offline Policy Evaluation We study representation learning for Offline Reinforcement Learning (RL),focusing on the important task of Offline Policy Evaluation (OPE). Recent workshows that, in contrast to supervised learning, realizability of the Q-functionis not enough for learning it. Two sufficient conditions for sample-efficientOPE are Bellman completeness and coverage. Prior work often assumes thatrepresentations satisfying these conditions are given, with results beingmostly theoretical in nature. In this work, we propose BCRL, which directlylearns from data an approximately linear Bellman complete representation withgood coverage. With this learned representation, we perform OPE using LeastSquare Policy Evaluation (LSPE) with linear functions in our learnedrepresentation. We present an end-to-end theoretical analysis, showing that ourtwo-stage algorithm enjoys polynomial sample complexity provided somerepresentation in the rich class considered is linear Bellman complete.Empirically, we extensively evaluate our algorithm on challenging, image-basedcontinuous control tasks from the Deepmind Control Suite. We show ourrepresentation enables better OPE compared to previous representation learningmethods developed for off-policy RL (e.g., CURL, SPR). BCRL achieve competitiveOPE error with the state-of-the-art method Fitted Q-Evaluation (FQE), and beatsFQE when evaluating beyond the initial state distribution. Our ablations showthat both linear Bellman complete and coverage components of our method arecrucial.",1
"Model Selection in Reinforcement Learning with General Function Approximations We consider model selection for classic Reinforcement Learning (RL)environments -- Multi Armed Bandits (MABs) and Markov Decision Processes (MDPs)-- under general function approximations. In the model selection framework, wedo not know the function classes, denoted by $\mathcal{F}$ and $\mathcal{M}$,where the true models -- reward generating function for MABs and and transitionkernel for MDPs -- lie, respectively. Instead, we are given $M$ nested function(hypothesis) classes such that true models are contained in at-least one suchclass. In this paper, we propose and analyze efficient model selectionalgorithms for MABs and MDPs, that \emph{adapt} to the smallest function class(among the nested $M$ classes) containing the true underlying model. Under aseparability assumption on the nested hypothesis classes, we show that thecumulative regret of our adaptive algorithms match to that of an oracle whichknows the correct function classes (i.e., $\cF$ and $\cM$) a priori.Furthermore, for both the settings, we show that the cost of model selection isan additive term in the regret having weak (logarithmic) dependence on thelearning horizon $T$.",1
"Better Methods and Theory for Federated Learning: Compression, Client Selection and Heterogeneity Federated learning (FL) is an emerging machine learning paradigm involvingmultiple clients, e.g., mobile phone devices, with an incentive to collaboratein solving a machine learning problem coordinated by a central server. FL wasproposed in 2016 by Konen et al. and McMahan et al. as a viableprivacy-preserving alternative to traditional centralized machine learningsince, by construction, the training data points are decentralized and nevertransferred by the clients to a central server. Therefore, to a certain degree,FL mitigates the privacy risks associated with centralized data collection.",1
"Single Model Uncertainty Estimation via Stochastic Data Centering We are interested in estimating the uncertainties of deep neural networks,which play an important role in many scientific and engineering problems. Inthis paper, we present a striking new finding that an ensemble of neuralnetworks with the same weight initialization, trained on datasets that areshifted by a constant bias gives rise to slightly inconsistent trained models,where the differences in predictions are a strong indicator of epistemicuncertainties. Using the neural tangent kernel (NTK), we demonstrate that thisphenomena occurs in part because the NTK is not shift-invariant. Since this isachieved via a trivial input transformation, we show that it can therefore beapproximated using just a single neural network -- using a technique that wecall $\Delta-$UQ -- that estimates uncertainty around prediction bymarginalizing out the effect of the biases. We show that $\Delta-$UQ'suncertainty estimates are superior to many of the current methods on a varietyof benchmarks -- outlier rejection, calibration under distribution shift, andsequential design optimization of black box functions.",1
"LETS-GZSL: A Latent Embedding Model for Time Series Generalized Zero Shot Learning One of the recent developments in deep learning is generalized zero-shotlearning (GZSL), which aims to recognize objects from both seen and unseenclasses, when only the labeled examples from seen classes are provided. Overthe past couple of years, GZSL has picked up traction and several models havebeen proposed to solve this problem. Whereas an extensive amount of research onGZSL has been carried out in fields such as computer vision and naturallanguage processing, no such research has been carried out to deal with timeseries data. GZSL is used for applications such as detecting abnormalities fromECG and EEG data and identifying unseen classes from sensor, spectrograph andother devices' data. In this regard, we propose a Latent Embedding for TimeSeries - GZSL (LETS-GZSL) model that can solve the problem of GZSL for timeseries classification (TSC). We utilize an embedding-based approach and combineit with attribute vectors to predict the final class labels. We report ourresults on the widely popular UCR archive datasets. Our framework is able toachieve a harmonic mean value of at least 55% on most of the datasets exceptwhen the number of unseen classes is greater than 3 or the amount of data isvery low (less than 100 training examples).",1
"Local manifold learning and its link to domain-based physics knowledge In many reacting flow systems, the thermo-chemical state-space is known orassumed to evolve close to a low-dimensional manifold (LDM). Various approachesare available to obtain those manifolds and subsequently express the originalhigh-dimensional space with fewer parameterizing variables. Principal componentanalysis (PCA) is one of the dimensionality reduction methods that can be usedto obtain LDMs. PCA does not make prior assumptions about the parameterizingvariables and retrieves them empirically from the training data. In this paper,we show that PCA applied in local clusters of data (local PCA) is capable ofdetecting the intrinsic parameterization of the thermo-chemical state-space. Wefirst demonstrate that utilizing three common combustion models of varyingcomplexity: the Burke-Schumann model, the chemical equilibrium model and thehomogeneous reactor. Parameterization of these models is known a priori whichallows for benchmarking with the local PCA approach. We further extend theapplication of local PCA to a more challenging case of a turbulent non-premixed$n$-heptane/air jet flame for which the parameterization is no longer obvious.Our results suggest that meaningful parameterization can be obtained also formore complex datasets. We show that local PCA finds variables that can belinked to local stoichiometry, reaction progress and soot formation processes.",1
"Pavlov Learning Machines As well known, Hebb's learning traces its origin in Pavlov's ClassicalConditioning, however, while the former has been extensively modelled in thepast decades (e.g., by Hopfield model and countless variations on theme), asfor the latter modelling has remained largely unaddressed so far; further, abridge between these two pillars is totally lacking. The main difficultytowards this goal lays in the intrinsically different scales of the informationinvolved: Pavlov's theory is about correlations among \emph{concepts} that are(dynamically) stored in the synaptic matrix as exemplified by the celebratedexperiment starring a dog and a ring bell; conversely, Hebb's theory is aboutcorrelations among pairs of adjacent neurons as summarized by the famousstatement {\em neurons that fire together wire together}. In this paper we relyon stochastic-process theory and model neural and synaptic dynamics viaLangevin equations, to prove that -- as long as we keep neurons' and synapses'timescales largely split -- Pavlov mechanism spontaneously takes place andultimately gives rise to synaptic weights that recover the Hebbian kernel.",1
"Learning Counterfactually Invariant Predictors We propose a method to learn predictors that are invariant undercounterfactual changes of certain covariates. This method is useful when theprediction target is causally influenced by covariates that should not affectthe predictor output. For instance, an object recognition model may beinfluenced by position, orientation, or scale of the object itself. We addressthe problem of training predictors that are explicitly counterfactuallyinvariant to changes of such covariates. We propose a model-agnosticregularization term based on conditional kernel mean embeddings, to enforcecounterfactual invariance during training. We prove the soundness of ourmethod, which can handle mixed categorical and continuous multi-variateattributes. Empirical results on synthetic and real-world data demonstrate theefficacy of our method in a variety of settings.",1
"SPRT-based Efficient Best Arm Identification in Stochastic Bandits This paper investigates the best arm identification (BAI) problem instochastic multi-armed bandits in the fixed confidence setting. The generalclass of the exponential family of bandits is considered. The state-of-the-artalgorithms for the exponential family of bandits face computational challenges.To mitigate these challenges, a novel framework is proposed, which views theBAI problem as sequential hypothesis testing, and is amenable to tractableanalysis for the exponential family of bandits. Based on this framework, a BAIalgorithm is designed that leverages the canonical sequential probability ratiotests. This algorithm has three features for both settings: (1) its samplecomplexity is asymptotically optimal, (2) it is guaranteed to be $\delta-$PAC,and (3) it addresses the computational challenge of the state-of-the-artapproaches. Specifically, these approaches, which are focused only on theGaussian setting, require Thompson sampling from the arm that is deemed thebest and a challenger arm. This paper analytically shows that identifying thechallenger is computationally expensive and that the proposed algorithmcircumvents it. Finally, numerical experiments are provided to support theanalysis.",1
"Improving the Accuracy of Marginal Approximations in Likelihood-Free Inference via Localisation Likelihood-free methods are an essential tool for performing inference forimplicit models which can be simulated from, but for which the correspondinglikelihood is intractable. However, common likelihood-free methods do not scalewell to a large number of model parameters. A promising approach tohigh-dimensional likelihood-free inference involves estimating low-dimensionalmarginal posteriors by conditioning only on summary statistics believed to beinformative for the low-dimensional component, and then combining thelow-dimensional approximations in some way. In this paper, we demonstrate thatsuch low-dimensional approximations can be surprisingly poor in practice forseemingly intuitive summary statistic choices. We describe an idealizedlow-dimensional summary statistic that is, in principle, suitable for marginalestimation. However, a direct approximation of the idealized choice isdifficult in practice. We thus suggest an alternative approach to marginalestimation which is easier to implement and automate. Given an initial choiceof low-dimensional summary statistic that might only be informative about amarginal posterior location, the new method improves performance by firstcrudely localising the posterior approximation using all the summary statisticsto ensure global identifiability, followed by a second step that hones in on anaccurate low-dimensional approximation using the low-dimensional summarystatistic. We show that the posterior this approach targets can be representedas a logarithmic pool of posterior distributions based on the low-dimensionaland full summary statistics, respectively. The good performance of our methodis illustrated in several examples.",1
"Variational Flow Graphical Model This paper introduces a novel approach to embed flow-based models withhierarchical structures. The proposed framework is named Variational FlowGraphical (VFG) Model. VFGs learn the representation of high dimensional datavia a message-passing scheme by integrating flow-based functions throughvariational inference. By leveraging the expressive power of neural networks,VFGs produce a representation of the data using a lower dimension, thusovercoming the drawbacks of many flow-based models, usually requiring a highdimensional latent space involving many trivial variables. Aggregation nodesare introduced in the VFG models to integrate forward-backward hierarchicalinformation via a message passing scheme. Maximizing the evidence lower bound(ELBO) of data likelihood aligns the forward and backward messages in eachaggregation node achieving a consistency node state. Algorithms have beendeveloped to learn model parameters through gradient updating regarding theELBO objective.",1
"Randomly pivoted Cholesky: Practical approximation of a kernel matrix with few entry evaluations Randomly pivoted Cholesky (RPCholesky) is a natural algorithm for computing arank-k approximation of an N x N positive semidefinite (psd) matrix. RPCholeskycan be implemented with just a few lines of code. It requires only (k+1)N entryevaluations and O(k^2 N) additional arithmetic operations. This paper offersthe first serious investigation of its experimental and theoretical behavior.Empirically, RPCholesky matches or improves on the performance of alternativealgorithms for low-rank psd approximation. Furthermore, RPCholesky provablyachieves near-optimal approximation guarantees. The simplicity, effectiveness,and robustness of this algorithm strongly support its use in scientificcomputing and machine learning applications.",1
"Volatility Based Kernels and Moving Average Means for Accurate Forecasting with Gaussian Processes A broad class of stochastic volatility models are defined by systems ofstochastic differential equations. While these models have seen widespreadsuccess in domains such as finance and statistical climatology, they typicallylack an ability to condition on historical data to produce a true posteriordistribution. To address this fundamental limitation, we show how to re-cast aclass of stochastic volatility models as a hierarchical Gaussian process (GP)model with specialized covariance functions. This GP model retains theinductive biases of the stochastic volatility model while providing theposterior predictive distribution given by GP inference. Within this framework,we take inspiration from well studied domains to introduce a new class ofmodels, Volt and Magpie, that significantly outperform baselines in stock andwind speed forecasting, and naturally extend to the multitask setting.",1
"Continuous-time Analysis for Variational Inequalities: An Overview and Desiderata Algorithms that solve zero-sum games, multi-objective agent objectives, or,more generally, variational inequality (VI) problems are notoriously unstableon general problems. Owing to the increasing need for solving such problems inmachine learning, this instability has been highlighted in recent years as asignificant research challenge. In this paper, we provide an overview of recentprogress in the use of continuous-time perspectives in the analysis and designof methods targeting the broad VI problem class. Our presentation drawsparallels between single-objective problems and multi-objective problems,highlighting the challenges of the latter. We also formulate various desideratafor algorithms that apply to general VIs and we argue that achieving thesedesiderata may profit from an understanding of the associated continuous-timedynamics.",1
"Adaptive Step-Size Methods for Compressed SGD Compressed Stochastic Gradient Descent (SGD) algorithms have been recentlyproposed to address the communication bottleneck in distributed anddecentralized optimization problems, such as those that arise in federatedmachine learning. Existing compressed SGD algorithms assume the use ofnon-adaptive step-sizes(constant or diminishing) to provide theoreticalconvergence guarantees. Typically, the step-sizes are fine-tuned in practice tothe dataset and the learning algorithm to provide good empirical performance.Such fine-tuning might be impractical in many learning scenarios, and it istherefore of interest to study compressed SGD using adaptive step-sizes.Motivated by prior work on adaptive step-size methods for SGD to train neuralnetworks efficiently in the uncompressed setting, we develop an adaptivestep-size method for compressed SGD. In particular, we introduce a scalingtechnique for the descent step in compressed SGD, which we use to establishorder-optimal convergence rates for convex-smooth and strong convex-smoothobjectives under an interpolation condition and for non-convex objectives undera strong growth condition. We also show through simulation examples thatwithout this scaling, the algorithm can fail to converge. We presentexperimental results on deep neural networks for real-world datasets, andcompare the performance of our proposed algorithm with previously proposedcompressed SGD methods in literature, and demonstrate improved performance onResNet-18, ResNet-34 and DenseNet architectures for CIFAR-100 and CIFAR-10datasets at various levels of compression.",1
"Characterizing the Effect of Class Imbalance on the Learning Dynamics Data imbalance is a common problem in the machine learning literature thatcan have a critical effect on the performance of a model. Various solutionsexist - such as the ones that focus on resampling or data generation - buttheir impact on the convergence of gradient-based optimizers used in deeplearning is not understood. We here elucidate the significant negative impactof data imbalance on learning, showing that the learning curves for minorityand majority classes follow sub-optimal trajectories when training with agradient-based optimizer. The reason is not only that the gradient signalneglects the minority classes, but also that the minority classes are subjectto a larger directional noise, which slows their learning by an amount relatedto the imbalance ratio. To address this problem, we propose a new algorithmicsolution, for which we provide a detailed analysis of its convergence behavior.We show both theoretically and empirically that this new algorithm exhibits abetter behavior with more stable learning curves for each class, as well as abetter generalization performance.",1
"Recommendation Systems with Distribution-Free Reliability Guarantees When building recommendation systems, we seek to output a helpful set ofitems to the user. Under the hood, a ranking model predicts which of twocandidate items is better, and we must distill these pairwise comparisons intothe user-facing output. However, a learned ranking model is never perfect, sotaking its predictions at face value gives no guarantee that the user-facingoutput is reliable. Building from a pre-trained ranking model, we show how toreturn a set of items that is rigorously guaranteed to contain mostly gooditems. Our procedure endows any ranking model with rigorous finite-samplecontrol of the false discovery rate (FDR), regardless of the (unknown) datadistribution. Moreover, our calibration algorithm enables the easy andprincipled integration of multiple objectives in recommender systems. As anexample, we show how to optimize for recommendation diversity subject to auser-specified level of FDR control, circumventing the need to specify ad hocweights of a diversity loss against an accuracy loss. Throughout, we focus onthe problem of learning to rank a set of possible recommendations, evaluatingour methods on the Yahoo! Learning to Rank and MSMarco datasets.",1
"Supervising Embedding Algorithms Using the Stress While classical scaling, just like principal component analysis, isparameter-free, most other methods for embedding multivariate data require theselection of one or several parameters. This tuning can be difficult due to theunsupervised nature of the situation. We propose a simple, almost obvious,approach to supervise the choice of tuning parameter(s): minimize a notion ofstress. We substantiate this choice by reference to rigidity theory. We extenda result by Aspnes et al. (IEEE Mobile Computing, 2006), showing that generalrandom geometric graphs are trilateration graphs with high probability. And weprovide a stability result  la Anderson et al. (SIAM Discrete Mathematics,2010). We illustrate this approach in the context of the MDS-MAP(P) algorithmof Shang and Ruml (IEEE INFOCOM, 2004). As a prototypical patch-stitchingmethod, it requires the choice of patch size, and we use the stress to makethat choice data-driven. In this context, we perform a number of experiments toillustrate the validity of using the stress as the basis for tuning parameterselection. In so doing, we uncover a bias-variance tradeoff, which is aphenomenon which may have been overlooked in the multidimensional scalingliterature. By turning MDS-MAP(P) into a method for manifold learning, weobtain a local version of Isomap for which the minimization of the stress mayalso be used for parameter tuning.",1
"Information Processing Equalities and the Information-Risk Bridge We introduce two new classes of measures of information for statisticalexperiments which generalise and subsume $\phi$-divergences, integralprobability metrics, $\mathfrak{N}$-distances (MMD), and $(f,\Gamma)$divergences between two or more distributions. This enables us to derive asimple geometrical relationship between measures of information and the Bayesrisk of a statistical decision problem, thus extending the variational$\phi$-divergence representation to multiple distributions in an entirelysymmetric manner. The new families of divergence are closed under the action ofMarkov operators which yields an information processing equality which is arefinement and generalisation of the classical data processing inequality. Thisequality gives insight into the significance of the choice of the hypothesisclass in classical risk minimization.",1
"A clinically motivated self-supervised approach for content-based image retrieval of CT liver images Deep learning-based approaches for content-based image retrieval (CBIR) of CTliver images is an active field of research, but suffers from some criticallimitations. First, they are heavily reliant on labeled data, which can bechallenging and costly to acquire. Second, they lack transparency andexplainability, which limits the trustworthiness of deep CBIR systems. Weaddress these limitations by (1) proposing a self-supervised learning frameworkthat incorporates domain-knowledge into the training procedure and (2)providing the first representation learning explainability analysis in thecontext of CBIR of CT liver images. Results demonstrate improved performancecompared to the standard self-supervised approach across several metrics, aswell as improved generalisation across datasets. Further, we conduct the firstrepresentation learning explainability analysis in the context of CBIR, whichreveals new insights into the feature extraction process. Lastly, we perform acase study with cross-examination CBIR that demonstrates the usability of ourproposed framework. We believe that our proposed framework could play a vitalrole in creating trustworthy deep CBIR systems that can successfully takeadvantage of unlabeled data.",1
"Causal Fairness Analysis Decision-making systems based on AI and machine learning have been usedthroughout a wide range of real-world scenarios, including healthcare, lawenforcement, education, and finance. It is no longer far-fetched to envision afuture where autonomous systems will be driving entire business decisions and,more broadly, supporting large-scale decision-making infrastructure to solvesociety's most challenging problems. Issues of unfairness and discriminationare pervasive when decisions are being made by humans, and remain (or arepotentially amplified) when decisions are made using machines with littletransparency, accountability, and fairness. In this paper, we introduce aframework for \textit{causal fairness analysis} with the intent of filling inthis gap, i.e., understanding, modeling, and possibly solving issues offairness in decision-making settings. The main insight of our approach will beto link the quantification of the disparities present on the observed data withthe underlying, and often unobserved, collection of causal mechanisms thatgenerate the disparity in the first place, challenge we call the FundamentalProblem of Causal Fairness Analysis (FPCFA). In order to solve the FPCFA, westudy the problem of decomposing variations and empirical measures of fairnessthat attribute such variations to structural mechanisms and different units ofthe population. Our effort culminates in the Fairness Map, which is the firstsystematic attempt to organize and explain the relationship between differentcriteria found in the literature. Finally, we study which causal assumptionsare minimally needed for performing causal fairness analysis and propose aFairness Cookbook, which allows data scientists to assess the existence ofdisparate impact and disparate treatment.",1
"Seeking the Truth Beyond the Data. An Unsupervised Machine Learning Approach Clustering is an unsupervised machine learning methodology where unlabeledelements/objects are grouped together aiming to the construction ofwell-established clusters that their elements are classified according to theirsimilarity. The goal of this process is to provide a useful aid to theresearcher that will help her/him to identify patterns among the data. Dealingwith large databases, such patterns may not be easily detectable without thecontribution of a clustering algorithm. This article provides a deepdescription of the most widely used clustering methodologies accompanied byuseful presentations concerning suitable parameter selection andinitializations. Simultaneously, this article not only represents a reviewhighlighting the major elements of examined clustering techniques butemphasizes the comparison of these algorithms' clustering efficiency based on 3datasets, revealing their existing weaknesses and capabilities through accuracyand complexity, during the confrontation of discrete and continuousobservations. The produced results help us extract valuable conclusions aboutthe appropriateness of the examined clustering techniques in accordance withthe dataset's size.",1
"Variational Inference of overparameterized Bayesian Neural Networks: a theoretical and empirical study This paper studies the Variational Inference (VI) used for training BayesianNeural Networks (BNN) in the overparameterized regime, i.e., when the number ofneurons tends to infinity. More specifically, we consider overparameterizedtwo-layer BNN and point out a critical issue in the mean-field VI training.This problem arises from the decomposition of the lower bound on the evidence(ELBO) into two terms: one corresponding to the likelihood function of themodel and the second to the Kullback-Leibler (KL) divergence between the priordistribution and the variational posterior. In particular, we show boththeoretically and empirically that there is a trade-off between these two termsin the overparameterized regime only when the KL is appropriately re-scaledwith respect to the ratio between the the number of observations and neurons.We also illustrate our theoretical results with numerical experiments thathighlight the critical choice of this ratio.",1
"Uncertainty-Aware Learning Against Label Noise on Imbalanced Datasets Learning against label noise is a vital topic to guarantee a reliableperformance for deep neural networks. Recent research usually refers to dynamicnoise modeling with model output probabilities and loss values, and thenseparates clean and noisy samples. These methods have gained notable success.However, unlike cherry-picked data, existing approaches often cannot performwell when facing imbalanced datasets, a common scenario in the real world. Wethoroughly investigate this phenomenon and point out two major issues thathinder the performance, i.e., \emph{inter-class loss distribution discrepancy}and \emph{misleading predictions due to uncertainty}. The first issue is thatexisting methods often perform class-agnostic noise modeling. However, lossdistributions show a significant discrepancy among classes under classimbalance, and class-agnostic noise modeling can easily get confused with noisysamples and samples in minority classes. The second issue refers to that modelsmay output misleading predictions due to epistemic uncertainty and aleatoricuncertainty, thus existing methods that rely solely on the output probabilitiesmay fail to distinguish confident samples. Inspired by our observations, wepropose an Uncertainty-aware Label Correction framework~(ULC) to handle labelnoise on imbalanced datasets. First, we perform epistemic uncertainty-awareclass-specific noise modeling to identify trustworthy clean samples andrefine/discard highly confident true/corrupted labels. Then, we introducealeatoric uncertainty in the subsequent learning process to prevent noiseaccumulation in the label noise modeling process. We conduct experiments onseveral synthetic and real-world datasets. The results demonstrate theeffectiveness of the proposed method, especially on imbalanced datasets.",1
"Learning Optimal Transport Between two Empirical Distributions with Normalizing Flows Optimal transport (OT) provides effective tools for comparing and mappingprobability measures. We propose to leverage the flexibility of neural networksto learn an approximate optimal transport map. More precisely, we present a newand original method to address the problem of transporting a finite set ofsamples associated with a first underlying unknown distribution towards anotherfinite set of samples drawn from another unknown distribution. We show that aparticular instance of invertible neural networks, namely the normalizingflows, can be used to approximate the solution of this OT problem between apair of empirical distributions. To this aim, we propose to relax the Mongeformulation of OT by replacing the equality constraint on the push-forwardmeasure by the minimization of the corresponding Wasserstein distance. Thepush-forward operator to be retrieved is then restricted to be a normalizingflow which is trained by optimizing the resulting cost function. This approachallows the transport map to be discretized as a composition of functions. Eachof these functions is associated to one sub-flow of the network, whose outputprovides intermediate steps of the transport between the original and targetmeasures. This discretization yields also a set of intermediate barycentersbetween the two measures of interest. Experiments conducted on toy examples aswell as a challenging task of unsupervised translation demonstrate the interestof the proposed method. Finally, some experiments show that the proposedapproach leads to a good approximation of the true OT.",1
"Markovian Gaussian Process Variational Autoencoders Deep generative models are widely used for modelling high-dimensional timeseries, such as video animations, audio and climate data. Sequentialvariational autoencoders have been successfully considered for manyapplications, with many variant models relying on discrete-time methods andrecurrent neural networks (RNNs). On the other hand, continuous-time methodshave recently gained attraction, especially in the context ofirregularly-sampled time series, where they can better handle the data thandiscrete-time methods. One such class are Gaussian process variationalautoencoders (GPVAEs), where the VAE prior is set as a Gaussian process (GPs),allowing inductive biases to be explicitly encoded via the kernel function andinterpretability of the latent space. However, a major limitation of GPVAEs isthat it inherits the same cubic computational cost as GPs. In this work, weleverage the equivalent discrete state space representation of Markovian GPs toenable a linear-time GP solver via Kalman filtering and smoothing. We show viacorrupt and missing frames tasks that our method performs favourably,especially on the latter where it outperforms RNN-based models.",1
"Shrinkage Estimation of Higher Order Bochner Integrals We consider shrinkage estimation of higher order Hilbert space valued Bochnerintegrals in a non-parametric setting. We propose estimators that shrink the$U$-statistic estimator of the Bochner integral towards a pre-specified targetelement in the Hilbert space. Depending on the degeneracy of the kernel of the$U$-statistic, we construct consistent shrinkage estimators with fast rates ofconvergence, and develop oracle inequalities comparing the risks of the the$U$-statistic estimator and its shrinkage version. Surprisingly, we show thatthe shrinkage estimator designed by assuming complete degeneracy of the kernelof the $U$-statistic is a consistent estimator even when the kernel is notcomplete degenerate. This work subsumes and improves upon Krikamol et al.,2016, JMLR and Zhou et al., 2019, JMVA, which only handle mean element andcovariance operator estimation in a reproducing kernel Hilbert space. We alsospecialize our results to normal mean estimation and show that for $d\ge 3$,the proposed estimator strictly improves upon the sample mean in terms of themean squared error.",1
"AMLB: an AutoML Benchmark Comparing different AutoML frameworks is notoriously challenging and oftendone incorrectly. We introduce an open and extensible benchmark that followsbest practices and avoids common mistakes when comparing AutoML frameworks. Weconduct a thorough comparison of 9 well-known AutoML frameworks across 71classification and 33 regression tasks. The differences between the AutoMLframeworks are explored with a multi-faceted analysis, evaluating modelaccuracy, its trade-offs with inference time, and framework failures. We alsouse Bradley-Terry trees to discover subsets of tasks where the relative AutoMLframework rankings differ. The benchmark comes with an open-source tool thatintegrates with many AutoML frameworks and automates the empirical evaluationprocess end-to-end: from framework installation and resource allocation toin-depth evaluation. The benchmark uses public data sets, can be easilyextended with other AutoML frameworks and tasks, and has a website withup-to-date results.",1
"Look beyond labels: Incorporating functional summary information in Bayesian neural networks Bayesian deep learning offers a principled approach to train neural networksthat accounts for both aleatoric and epistemic uncertainty. In variationalinference, priors are often specified over the weight parameters, but they donot capture the true prior knowledge in large and complex neural networkarchitectures. We present a simple approach to incorporate summary informationabout the predicted probability (such as sigmoid or softmax score) outputs inBayesian neural networks (BNNs). The available summary information isincorporated as augmented data and modeled with a Dirichlet process, and wederive the corresponding \emph{Summary Evidence Lower BOund}. We show how themethod can inform the model about task difficulty or class imbalance. Extensiveempirical experiments show that, with negligible computational overhead, theproposed method yields a BNN with a better calibration of uncertainty.",1
"Stochastic Functional Analysis and Multilevel Vector Field Anomaly Detection Massive vector field datasets are common in multi-spectral optical and radarsensors and modern multimodal MRI data, among many other areas of application.In this paper we develop a novel stochastic functional analysis approach fordetecting anomalies based on the covariance structure of nominal stochasticbehavior across a domain with multi-band vector field data. An optimal vectorfield Karhunen-Loeve (KL) expansion is applied to such random field data. Aseries of multilevel orthogonal functional subspaces is constructed from thegeometry of the domain, adapted from the KL expansion. Detection is achieved byexamining the projection of the random field on the multilevel basis. Theanomalies can be quantified in suitable normed spaces based on local and globalinformation. In addition, reliable hypothesis tests are formed withcontrollable distributions that do not require prior assumptions on probabilitydistributions of the data. Only the covariance function is needed, which makesfor significantly simpler estimates. Furthermore this approach allowsstochastic vector-based fusion of anomalies without any loss of information.The method is applied to the important problem of deforestation and degradationin the Amazon forest. This is a complex non-monotonic process, as forests candegrade and recover. This particular problem is further compounded by thepresence of clouds that are hard to remove with current masking algorithms.Using multi-spectral satellite data from Sentinel 2, the multilevel filter isconstructed and anomalies are treated as deviations from the initial state ofthe forest. Forest anomalies are quantified with robust hypothesis tests anddistinguished from false variations such as cloud cover. Our approach shows theadvantage of using multiple bands of data in a vectorized complex, leading tobetter anomaly detection beyond the capabilities of scalar-based methods.",1
"Quantum Advantage in Variational Bayes Inference Variational Bayes (VB) inference algorithm is used widely to estimate boththe parameters and the unobserved hidden variables in generative statisticalmodels. The algorithm -- inspired by variational methods used in computationalphysics -- is iterative and can get easily stuck in local minima, even whenclassical techniques, such as deterministic annealing (DA), are used. We studya variational Bayes (VB) inference algorithm based on a non-traditional quantumannealing approach -- referred to as quantum annealing variational Bayes (QAVB)inference -- and show that there is indeed a quantum advantage to QAVB over itsclassical counterparts. In particular, we show that such better performance isrooted in key concepts from quantum mechanics: (i) the ground state of theHamiltonian of a quantum system -- defined from the given variational Bayes(VB) problem -- corresponds to an optimal solution for the minimization problemof the variational free energy at very low temperatures; (ii) such a groundstate can be achieved by a technique paralleling the quantum annealing process;and (iii) starting from this ground state, the optimal solution to the VBproblem can be achieved by increasing the heat bath temperature to unity, andthereby avoiding local minima introduced by spontaneous symmetry breakingobserved in classical physics based VB algorithms. We also show that the updateequations of QAVB can be potentially implemented using $\lceil \log K \rceil$qubits and $\mathcal{O} (K)$ operations per step. Thus, QAVB can match the timecomplexity of existing VB algorithms, while delivering higher performance.",1
"Probabilistic forecasting for geosteering in fluvial successions using a generative adversarial network Quantitative workflows utilizing real-time data to constrain ahead-of-bituncertainty have the potential to improve geosteering significantly. Fastupdates based on real-time data are essential when drilling in complexreservoirs with high uncertainties in pre-drill models. However, practicalassimilation of real-time data requires effective geological modeling andmathematically robust parameterization. We propose a generative adversarialdeep neural network (GAN), trained to reproduce geologically consistent 2Dsections of fluvial successions. Offline training produces a fast GAN-basedapproximation of complex geology parameterized as a 60-dimensional model vectorwith standard Gaussian distribution of each component. Probabilistic forecastsare generated using an ensemble of equiprobable model vector realizations. Aforward-modeling sequence, including a GAN, converts the initial (prior)ensemble of realizations into EM log predictions. An ensemble smootherminimizes statistical misfits between predictions and real-time data, yieldingan update of model vectors and reduced uncertainty around the well. Updates canbe then translated to probabilistic predictions of facies and resistivities.The present paper demonstrates a workflow for geosteering in an outcrop-based,synthetic fluvial succession. In our example, the method reduces uncertaintyand correctly predicts most major geological features up to 500 meters ahead ofdrill-bit.",1
"Jackknife Variability Estimation For Randomized Matrix Computations Randomized algorithms based on sketching have become a workhorse tool inlow-rank matrix approximation. To use these algorithms safely in applications,they should be coupled with diagnostics to assess the quality of approximation.To meet this need, this paper proposes a jackknife resampling method toestimate the variability of the output of a randomized matrix computation. Thevariability estimate can recognize that a computation requires additional dataor that the computation is intrinsically unstable. As examples, the paperstudies jackknife estimates for two randomized low-rank matrix approximationalgorithms. In each case, the operation count for the jackknife estimate isindependent of the dimensions of the target matrix. In numerical experiments,the estimator accurately assesses variability and also provides anorder-of-magnitude estimate of the mean-square error.",1
"On the Super-exponential Quantum Speedup of Equivariant Quantum Machine Learning Algorithms with SU($d$) Symmetry We introduce a framework of the equivariant convolutional algorithms which istailored for a number of machine-learning tasks on physical systems witharbitrary SU($d$) symmetries. It allows us to enhance a natural model ofquantum computation--permutational quantum computing (PQC) [Quantum Inf.Comput., 10, 470-497 (2010)] --and defines a more powerful model: PQC+. WhilePQC was shown to be effectively classically simulatable, we exhibit a problemwhich can be efficiently solved on PQC+ machine, whereas the best knownclassical algorithms runs in $O(n!n^2)$ time, thus providing strong evidenceagainst PQC+ being classically simulatable. We further discuss practicalquantum machine learning algorithms which can be carried out in the paradigm ofPQC+.",1
"Sliced-Wasserstein normalizing flows: beyond maximum likelihood training Despite their advantages, normalizing flows generally suffer from severalshortcomings including their tendency to generate unrealistic data (e.g.,images) and their failing to detect out-of-distribution data. One reason forthese deficiencies lies in the training strategy which traditionally exploits amaximum likelihood principle only. This paper proposes a new training paradigmbased on a hybrid objective function combining the maximum likelihood principle(MLE) and a sliced-Wasserstein distance. Results obtained on synthetic toyexamples and real image data sets show better generative abilities in terms ofboth likelihood and visual aspects of the generated samples. Reciprocally, theproposed approach leads to a lower likelihood of out-of-distribution data,demonstrating a greater data fidelity of the resulting flows.",1
"Hindsight Learning for MDPs with Exogenous Inputs We develop a reinforcement learning (RL) framework for applications that dealwith sequential decisions and exogenous uncertainty, such as resourceallocation and inventory management. In these applications, the uncertainty isonly due to exogenous variables like future demands. A popular approach is topredict the exogenous variables using historical data and then plan with thepredictions. However, this indirect approach requires high-fidelity modeling ofthe exogenous process to guarantee good downstream decision-making, which canbe impractical when the exogenous process is complex. In this work we proposean alternative approach based on hindsight learning which sidesteps modelingthe exogenous process. Our key insight is that, unlike Sim2Real RL, we canrevisit past decisions in the historical data and derive counterfactualconsequences for other actions in these applications. Our framework useshindsight-optimal actions as the policy training signal and has strongtheoretical guarantees on decision-making performance. We develop an algorithmusing our framework to allocate compute resources for real-world MicrosoftAzure workloads. The results show our approach learns better policies thandomain-specific heuristics and Sim2Real RL baselines.",1
"The Cosmic Graph: Optimal Information Extraction from Large-Scale Structure using Catalogues We present an implicit likelihood approach to quantifying cosmologicalinformation over discrete catalogue data, assembled as graphs. To do so, weexplore cosmological inference using mock dark matter halo catalogues. Weemploy Information Maximising Neural Networks (IMNNs) to quantify Fisherinformation extraction as a function of graph representation. We a) demonstratethe high sensitivity of modular graph structure to the underlying cosmology inthe noise-free limit, b) show that networks automatically combine mass andclustering information through comparisons to traditional statistics, c)demonstrate that graph neural networks can still extract information whencatalogues are subject to noisy survey cuts, and d) illustrate how nonlinearIMNN summaries can be used as asymptotically optimal compressed statistics forBayesian implicit likelihood inference. We reduce the area of joint $\Omega_m,\sigma_8$ parameter constraints with small ($\sim$100 object) halo cataloguesby a factor of 42 over the two-point correlation function, and demonstrate thatthe networks automatically combine mass and clustering information. This workutilises a new IMNN implementation over graph data in Jax, which can takeadvantage of either numerical or auto-differentiability. We also show thatgraph IMNNs successfully compress simulations far from the fiducial model atwhich the network is fitted, indicating a promising alternative to $n$-pointstatistics in catalogue-based analyses.",1
"JAWS: Predictive Inference Under Covariate Shift We propose \textbf{JAWS}, a series of wrapper methods for distribution-freeuncertainty quantification tasks under covariate shift, centered on our coremethod \textbf{JAW}, the \textbf{JA}ckknife+ \textbf{W}eighted withlikelihood-ratio weights. JAWS also includes computationally efficient\textbf{A}pproximations of JAW using higher-order influence functions:\textbf{JAWA}. Theoretically, we show that JAW relaxes the jackknife+'sassumption of data exchangeability to achieve the same finite-sample coverageguarantee even under covariate shift. JAWA further approaches the JAW guaranteein the limit of either the sample size or the influence function order undermild assumptions. Moreover, we propose a general approach to repurposing anydistribution-free uncertainty quantification method and its guarantees to thetask of risk assessment: a task that generates the estimated probability thatthe true label lies within a user-specified interval. We then propose\textbf{JAW-R} and \textbf{JAWA-R} as the repurposed versions of proposedmethods for \textbf{R}isk assessment. Practically, JAWS outperform thestate-of-the-art predictive inference baselines in a variety of biased realworld data sets for both interval-generation and risk-assessment auditingtasks.",1
"Uncertainty Calibration in Bayesian Neural Networks via Distance-Aware Priors As we move away from the data, the predictive uncertainty should increase,since a great variety of explanations are consistent with the little availableinformation. We introduce Distance-Aware Prior (DAP) calibration, a method tocorrect overconfidence of Bayesian deep learning models outside of the trainingdomain. We define DAPs as prior distributions over the model parameters thatdepend on the inputs through a measure of their distance from the training set.DAP calibration is agnostic to the posterior inference method, and it can beperformed as a post-processing step. We demonstrate its effectiveness againstseveral baselines in a variety of classification and regression problems,including benchmarks designed to test the quality of predictive distributionsaway from the data.",1
"Generalized Identifiability Bounds for Mixture Models with Grouped Samples Recent work has shown that finite mixture models with $m$ components areidentifiable, while making no assumptions on the mixture components, so long asone has access to groups of samples of size $2m-1$ which are known to come fromthe same mixture component. In this work we generalize that result and showthat, if every subset of $k$ mixture components of a mixture model are linearlyindependent, then that mixture model is identifiable with only $(2m-1)/(k-1)$samples per group. We further show that this value cannot be improved. We provean analogous result for a stronger form of identifiability known asdeterminedness along with a corresponding lower bound. This independenceassumption almost surely holds if mixture components are chosen randomly from a$k$-dimensional space. We describe some implications of our results formultinomial mixture models and topic modeling.",1
"FACT: High-Dimensional Random Forests Inference Random forests is one of the most widely used machine learning methods overthe past decade thanks to its outstanding empirical performance. Yet, becauseof its black-box nature, the results by random forests can be hard to interpretin many big data applications. Quantifying the usefulness of individualfeatures in random forests learning can greatly enhance its interpretability.Existing studies have shown that some popularly used feature importancemeasures for random forests suffer from the bias issue. In addition, there lackcomprehensive size and power analyses for most of these existing methods. Inthis paper, we approach the problem via hypothesis testing, and suggest aframework of the self-normalized feature-residual correlation test (FACT) forevaluating the significance of a given feature in the random forests model withbias-resistance property, where our null hypothesis concerns whether thefeature is conditionally independent of the response given all other features.Such an endeavor on random forests inference is empowered by some recentdevelopments on high-dimensional random forests consistency. The vanillaversion of our FACT test can suffer from the bias issue in the presence offeature dependency. We exploit the techniques of imbalancing and conditioningfor bias correction. We further incorporate the ensemble idea into the FACTstatistic through feature transformations for the enhanced power. Under afairly general high-dimensional nonparametric model setting with dependentfeatures, we formally establish that FACT can provide theoretically justifiedrandom forests feature p-values and enjoy appealing power through nonasymptoticanalyses. The theoretical results and finite-sample advantages of the newlysuggested method are illustrated with several simulation examples and aneconomic forecasting application in relation to COVID-19.",1
"A Certifiable Security Patch for Object Tracking in Self-Driving Systems via Historical Deviation Modeling Self-driving cars (SDC) commonly implement the perception pipeline to detectthe surrounding obstacles and track their moving trajectories, which lays theground for the subsequent driving decision making process. Although thesecurity of obstacle detection in SDC is intensively studied, not until veryrecently the attackers start to exploit the vulnerability of the trackingmodule. Compared with solely attacking the object detectors, this new attackstrategy influences the driving decision more effectively with less attackbudgets. However, little is known on whether the revealed vulnerability remainseffective in end-to-end self-driving systems and, if so, how to mitigate thethreat.",1
"Can Population-based Engagement Improve Personalisation? A Novel Dataset and Experiments This work explores how population-based engagement prediction can addresscold-start at scale in large learning resource collections. The paperintroduces i) VLE, a novel dataset that consists of content and video basedfeatures extracted from publicly available scientific video lectures coupledwith implicit and explicit signals related to learner engagement, ii) twostandard tasks related to predicting and ranking context-agnostic engagement invideo lectures with preliminary baselines and iii) a set of experiments thatvalidate the usefulness of the proposed dataset. Our experimental resultsindicate that the newly proposed VLE dataset leads to building context-agnosticengagement prediction models that are significantly performant than ones basedon previous datasets, mainly attributing to the increase of training examples.VLE dataset's suitability in building models towards Computer Science/Artificial Intelligence education focused on e-learning/ MOOC use-cases is alsoevidenced. Further experiments in combining the built model with apersonalising algorithm show promising improvements in addressing thecold-start problem encountered in educational recommenders. This is the largestand most diverse publicly available dataset to our knowledge that deals withlearner engagement prediction tasks. The dataset, helper tools, descriptivestatistics and example code snippets are available publicly.",1
"Differentially Private Graph Learning via Sensitivity-Bounded Personalized PageRank Personalized PageRank (PPR) is a fundamental tool in unsupervised learning ofgraph representations such as node ranking, labeling, and graph embedding.However, while data privacy is one of the most important recent concerns,existing PPR algorithms are not designed to protect user privacy. PPR is highlysensitive to the input graph edges: the difference of only one edge may cause abig change in the PPR vector, potentially leaking private user data.",1
"General Policy Evaluation and Improvement by Learning to Identify Few But Crucial States Learning to evaluate and improve policies is a core problem of ReinforcementLearning (RL). Traditional RL algorithms learn a value function defined for asingle policy. A recently explored competitive alternative is to learn a singlevalue function for many policies. Here we combine the actor-critic architectureof Parameter-Based Value Functions and the policy embedding of PolicyEvaluation Networks to learn a single value function for evaluating (and thushelping to improve) any policy represented by a deep neural network (NN). Themethod yields competitive experimental results. In continuous control problemswith infinitely many states, our value function minimizes its prediction errorby simultaneously learning a small set of `probing states' and a mapping fromactions produced in probing states to the policy's return. The method extractscrucial abstract knowledge about the environment in form of very few statessufficient to fully specify the behavior of many policies. A policy improvessolely by changing actions in probing states, following the gradient of thevalue function's predictions. Surprisingly, it is possible to clone thebehavior of a near-optimal policy in Swimmer-v3 and Hopper-v3 environments onlyby knowing how to act in 3 and 5 such learned states, respectively. Remarkably,our value function trained to evaluate NN policies is also invariant to changesof the policy architecture: we show that it allows for zero-shot learning oflinear policies competitive with the best policy seen during training. Our codeis public.",1
"Repairing Systematic Outliers by Learning Clean Subspaces in VAEs Data cleaning often comprises outlier detection and data repair. Systematicerrors result from nearly deterministic transformations that occur repeatedlyin the data, e.g. specific image pixels being set to default values orwatermarks. Consequently, models with enough capacity easily overfit to theseerrors, making detection and repair difficult. Seeing as a systematic outlieris a combination of patterns of a clean instance and systematic error patterns,our main insight is that inliers can be modelled by a smaller representation(subspace) in a model than outliers. By exploiting this, we propose CleanSubspace Variational Autoencoder (CLSVAE), a novel semi-supervised model fordetection and automated repair of systematic errors. The main idea is topartition the latent space and model inlier and outlier patterns separately.CLSVAE is effective with much less labelled data compared to previous relatedmodels, often with less than 2% of the data. We provide experiments using threeimage datasets in scenarios with different levels of corruption and labelledset sizes, comparing to relevant baselines. CLSVAE provides superior repairswithout human intervention, e.g. with just 0.25% of labelled data we see arelative error decrease of 58% compared to the closest baseline.",1
"Statistical and Computational Trade-offs in Variational Inference: A Case Study in Inferential Model Selection Variational inference has recently emerged as a popular alternative to theclassical Markov chain Monte Carlo (MCMC) in large-scale Bayesian inference.The core idea of variational inference is to trade statistical accuracy forcomputational efficiency. It aims to approximate the posterior, reducingcomputation costs but potentially compromising its statistical accuracy. Inthis work, we study this statistical and computational trade-off in variationalinference via a case study in inferential model selection. Focusing on Gaussianinferential models (a.k.a. variational approximating families) with diagonalplus low-rank precision matrices, we initiate a theoretical study of thetrade-offs in two aspects, Bayesian posterior inference error and frequentistuncertainty quantification error. From the Bayesian posterior inferenceperspective, we characterize the error of the variational posterior relative tothe exact posterior. We prove that, given a fixed computation budget, alower-rank inferential model produces variational posteriors with a higherstatistical approximation error, but a lower computational error; it reducesvariances in stochastic optimization and, in turn, accelerates convergence.From the frequentist uncertainty quantification perspective, we consider theprecision matrix of the variational posterior as an uncertainty estimate. Wefind that, relative to the true asymptotic precision, the variationalapproximation suffers from an additional statistical error originating from thesampling uncertainty of the data. Moreover, this statistical error becomes thedominant factor as the computation budget increases. As a consequence, forsmall datasets, the inferential model need not be full-rank to achieve optimalestimation error. We finally demonstrate these statistical and computationaltrade-offs inference across empirical studies, corroborating the theoreticalfindings.",1
"Ranking in Contextual Multi-Armed Bandits We study a ranking problem in the contextual multi-armed bandit setting. Alearning agent selects an ordered list of items at each time step and observesstochastic outcomes for each position. In online recommendation systems,showing an ordered list of the most attractive items would not be the bestchoice since both position and item dependencies result in a complicated rewardfunction. A very naive example is the lack of diversity when all the mostattractive items are from the same category. We model position and itemdependencies in the ordered list and design UCB and Thompson Sampling typealgorithms for this problem. We prove that the regret bound over $T$ rounds and$L$ positions is $\Tilde{O}(L\sqrt{d T})$, which has the same order as theprevious works with respect to $T$ and only increases linearly with $L$. Ourwork generalizes existing studies in several directions, including positiondependencies where position discount is a particular case, and proposes a moregeneral contextual bandit model.",1
"Guaranteed Discovery of Controllable Latent States with Multi-Step Inverse Models A person walking along a city street who tries to model all aspects of theworld would quickly be overwhelmed by a multitude of shops, cars, and peoplemoving in and out of view, following their own complex and inscrutabledynamics. Exploration and navigation in such an environment is an everydaytask, requiring no vast exertion of mental resources. Is it possible to turnthis fire hose of sensory information into a minimal latent state which isnecessary and sufficient for an agent to successfully act in the world? Weformulate this question concretely, and propose the Agent-Controllable StateDiscovery algorithm (AC-State), which has theoretical guarantees and ispractically demonstrated to discover the \textit{minimal controllable latentstate} which contains all of the information necessary for controlling theagent, while fully discarding all irrelevant information. This algorithmconsists of a multi-step inverse model (predicting actions from distantobservations) with an information bottleneck. AC-State enables localization,exploration, and navigation without reward or demonstrations. We demonstratethe discovery of controllable latent state in three domains: localizing a robotarm with distractions (e.g., changing lighting conditions and background),exploring in a maze alongside other agents, and navigating in the Matterporthouse simulator.",1
"Neural Posterior Estimation with Differentiable Simulators Simulation-Based Inference (SBI) is a promising Bayesian inference frameworkthat alleviates the need for analytic likelihoods to estimate posteriordistributions. Recent advances using neural density estimators in SBIalgorithms have demonstrated the ability to achieve high-fidelity posteriors,at the expense of a large number of simulations ; which makes their applicationpotentially very time-consuming when using complex physical simulations. Inthis work we focus on boosting the sample-efficiency of posterior densityestimation using the gradients of the simulator. We present a new method toperform Neural Posterior Estimation (NPE) with a differentiable simulator. Wedemonstrate how gradient information helps constrain the shape of the posteriorand improves sample-efficiency.",1
"Off-the-grid learning of sparse mixtures from a continuous dictionary We consider a general non-linear model where the signal is a finite mixtureof an unknown, possibly increasing, number of features issued from a continuousdictionary parameterized by a real nonlinear parameter. The signal is observedwith Gaussian (possibly correlated) noise in either a continuous or a discretesetup. We propose an off-the-grid optimization method, that is, a method whichdoes not use any discretization scheme on the parameter space, to estimate boththe non-linear parameters of the features and the linear parameters of themixture. We use recent results on the geometry of off-the-grid methods to giveminimal separation on the true underlying non-linear parameters such thatinterpolating certificate functions can be constructed. Using also tail boundsfor suprema of Gaussian processes we bound the prediction error with highprobability. Assuming that the certificate functions can be constructed, ourprediction error bound is up to log --factors similar to the rates attained bythe Lasso predictor in the linear regression model. We also establishconvergence rates that quantify with high probability the quality of estimationfor both the linear and the non-linear parameters.",1
"ManiFeSt: Manifold-based Feature Selection for Small Data Sets In this paper, we present a new method for few-sample supervised featureselection (FS). Our method first learns the manifold of the feature space ofeach class using kernels capturing multi-feature associations. Then, based onRiemannian geometry, a composite kernel is computed, extracting the differencesbetween the learned feature associations. Finally, a FS score based on spectralanalysis is proposed. Considering multi-feature associations makes our methodmultivariate by design. This in turn allows for the extraction of the hiddenmanifold underlying the features and avoids overfitting, facilitatingfew-sample FS. We showcase the efficacy of our method on illustrative examplesand several benchmarks, where our method demonstrates higher accuracy inselecting the informative features compared to competing methods. In addition,we show that our FS leads to improved classification and better generalizationwhen applied to test data.",1
"Holistic Robust Data-Driven Decisions The design of data-driven formulations for machine learning anddecision-making with good out-of-sample performance is a key challenge. Theobservation that good in-sample performance does not guarantee goodout-of-sample performance is generally known as overfitting. Practicaloverfitting can typically not be attributed to a single cause but instead iscaused by several factors all at once. We consider here three overfittingsources: (i) statistical error as a result of working with finite sample data,(ii) data noise which occurs when the data points are measured only with finiteprecision, and finally (iii) data misspecification in which a small fraction ofall data may be wholly corrupted. We argue that although existing data-drivenformulations may be robust against one of these three sources in isolation theydo not provide holistic protection against all overfitting sourcessimultaneously. We design a novel data-driven formulation which does guaranteesuch holistic protection and is furthermore computationally viable. Ourdistributionally robust optimization formulation can be interpreted as a novelcombination of a Kullback-Leibler and Levy-Prokhorov robust optimizationformulation. Finally, we show how in the context of classification andregression problems several popular regularized and robust formulations reduceto a particular case of our proposed more general formulation.",1
"Discover Life Skills for Planning with Bandits via Observing and Learning How the World Works We propose a novel approach for planning agents to compose abstract skillsvia observing and learning from historical interactions with the world. Ourframework operates in a Markov state-space model via a set of actions underunknown pre-conditions. We formulate skills as high-level abstract policiesthat propose action plans based on the current state. Each policy learns newplans by observing the states' transitions while the agent interacts with theworld. Such an approach automatically learns new plans to achieve specificintended effects, but the success of such plans is often dependent on thestates in which they are applicable. Therefore, we formulate the evaluation ofsuch plans as infinitely many multi-armed bandit problems, where we balance theallocation of resources on evaluating the success probability of existing armsand exploring new options. The result is a planner capable of automaticallylearning robust high-level skills under a noisy environment; such skillsimplicitly learn the action pre-condition without explicit knowledge. We showthat this planning approach is experimentally very competitive inhigh-dimensional state space domains.",1
ControlBurn: Nonlinear Feature Selection with Sparse Tree Ensembles ControlBurn is a Python package to construct feature-sparse tree ensemblesthat support nonlinear feature selection and interpretable machine learning.The algorithms in this package first build large tree ensembles that prioritizebasis functions with few features and then select a feature-sparse subset ofthese basis functions using a weighted lasso optimization criterion. Thepackage includes visualizations to analyze the features selected by theensemble and their impact on predictions. Hence ControlBurn offers the accuracyand flexibility of tree-ensemble models and the interpretability of sparsegeneralized additive models.,1
"Multiscale Causal Structure Learning The inference of causal structures from observed data plays a key role inunveiling the underlying dynamics of the system. This paper exposes a novelmethod, named Multiscale-Causal Structure Learning (MS-CASTLE), to estimate thestructure of linear causal relationships occurring at different time scales.Differently from existing approaches, MS-CASTLE takes explicitly into accountinstantaneous and lagged inter-relations between multiple time series,represented at different scales, hinging on stationary wavelet transform andnon-convex optimization. MS-CASTLE incorporates, as a special case, asingle-scale version named SS-CASTLE, which compares favorably in terms ofcomputational efficiency, performance and robustness with respect to the stateof the art onto synthetic data. We used MS-CASTLE to study the multiscalecausal structure of the risk of 15 global equity markets, during covid-19pandemic, illustrating how MS-CASTLE can extract meaningful information thanksto its multiscale analysis, outperforming SS-CASTLE. We found that the mostpersistent and strongest interactions occur at mid-term time resolutions.Moreover, we identified the stock markets that drive the risk during theconsidered period: Brazil, Canada and Italy. The proposed approach can beexploited by financial investors who, depending to their investment horizon,can manage the risk within equity portfolios from a causal perspective.",1
"Analysis of Catastrophic Forgetting for Random Orthogonal Transformation Tasks in the Overparameterized Regime Overparameterization is known to permit strong generalization performance inneural networks. In this work, we provide an initial theoretical analysis ofits effect on catastrophic forgetting in a continual learning setup. We showexperimentally that in permuted MNIST image classification tasks, thegeneralization performance of multilayer perceptrons trained by vanillastochastic gradient descent can be improved by overparameterization, and theextent of the performance increase achieved by overparameterization iscomparable to that of state-of-the-art continual learning algorithms. Weprovide a theoretical explanation of this effect by studying a qualitativelysimilar two-task linear regression problem, where each task is related by arandom orthogonal transformation. We show that when a model is trained on thetwo tasks in sequence without any additional regularization, the risk gain onthe first task is small if the model is sufficiently overparameterized.",1
"Uncertainty quantification for predictions of atomistic neural networks The value of uncertainty quantification on predictions for trained neuralnetworks (NNs) on quantum chemical reference data is quantitatively explored.For this, the architecture of the PhysNet NN was suitably modified and theresulting model was evaluated with different metrics to quantify calibration,quality of predictions, and whether prediction error and the predicteduncertainty can be correlated. The results from training on the QM9 databaseand evaluating data from the test set within and outside the distributionindicate that error and uncertainty are not linearly related. The resultsclarify that noise and redundancy complicate property prediction for moleculeseven in cases for which changes - e.g. double bond migration in two otherwiseidentical molecules - are small. The model was then applied to a real databaseof tautomerization reactions. Analysis of the distance between members infeature space combined with other parameters shows that redundant informationin the training dataset can lead to large variances and small errors whereasthe presence of similar but unspecific information returns large errors butsmall variances. This was, e.g., observed for nitro-containing aliphatic chainsfor which predictions were difficult although the training set containedseveral examples for nitro groups bound to aromatic molecules. This underlinesthe importance of the composition of the training data and provides chemicalinsight into how this affects the prediction capabilities of a ML model.Finally, the approach put forward can be used for information-based improvementof chemical databases for target applications through active learningoptimization.",1
"AGBoost: Attention-based Modification of Gradient Boosting Machine A new attention-based model for the gradient boosting machine (GBM) calledAGBoost (the attention-based gradient boosting) is proposed for solvingregression problems. The main idea behind the proposed AGBoost model is toassign attention weights with trainable parameters to iterations of GBM undercondition that decision trees are base learners in GBM. Attention weights aredetermined by applying properties of decision trees and by using the Huber'scontamination model which provides an interesting linear dependence betweentrainable parameters of the attention and the attention weights. Thispeculiarity allows us to train the attention weights by solving the standardquadratic optimization problem with linear constraints. The attention weightsalso depend on the discount factor as a tuning parameter, which determines howmuch the impact of the weight is decreased with the number of iterations.Numerical experiments performed for two types of base learners, originaldecision trees and extremely randomized trees with various regression datasetsillustrate the proposed model.",1
"Optimal precision for GANs When learning disconnected distributions, Generative adversarial networks(GANs) are known to face model misspecification. Indeed, a continuous mappingfrom a unimodal latent distribution to a disconnected one is impossible, soGANs necessarily generate samples outside of the support of the targetdistribution. This raises a fundamental question: what is the latent spacepartition that minimizes the measure of these areas? Building on a recentresult of geometric measure theory, we prove that an optimal GANs muststructure its latent space as a 'simplicial cluster' - a Voronoi partitionwhere cells are convex cones - when the dimension of the latent space is largerthan the number of modes. In this configuration, each Voronoi cell maps to adistinct mode of the data. We derive both an upper and a lower bound on theoptimal precision of GANs learning disconnected manifolds. Interestingly, thesetwo bounds have the same order of decrease: $\sqrt{\log m}$, $m$ being thenumber of modes. Finally, we perform several experiments to exhibit thegeometry of the latent space and experimentally show that GANs have a geometrywith similar properties to the theoretical one.",1
"Mathematical Foundations of Graph-Based Bayesian Semi-Supervised Learning In recent decades, science and engineering have been revolutionized by amomentous growth in the amount of available data. However, despite theunprecedented ease with which data are now collected and stored, labeling databy supplementing each feature with an informative tag remains to bechallenging. Illustrative tasks where the labeling process requires expertknowledge or is tedious and time-consuming include labeling X-rays with adiagnosis, protein sequences with a protein type, texts by their topic, tweetsby their sentiment, or videos by their genre. In these and numerous otherexamples, only a few features may be manually labeled due to cost and timeconstraints. How can we best propagate label information from a small number ofexpensive labeled features to a vast number of unlabeled ones? This is thequestion addressed by semi-supervised learning (SSL).",1
"How do tuna schools associate to dFADs? A study using echo-sounder buoys to identify global patterns Based on the data gathered by echo-sounder buoys attached to drifting FishAggregating Devices (dFADs) across tropical oceans, the current study applies aMachine Learning protocol to examine the temporal trends of tuna schools'association to drifting objects. Using a binary output, metrics typically usedin the literature were adapted to account for the fact that the entire tunaaggregation under the dFAD was considered. The median time it took tuna tocolonize the dFADs for the first time varied between 25 and 43 days, dependingon the ocean, and the longest soak and colonization times were registered inthe Pacific Ocean. The tuna schools' Continuous Residence Times were generallyshorter than Continuous Absence Times (median values between 5 and 7 days, and9 and 11 days, respectively), in line with the results found by previousstudies. Using a regression output, two novel metrics, namely aggregation timeand disaggregation time, were estimated to obtain further insight into thesymmetry of the aggregation process. Across all oceans, the time it took forthe tuna aggregation to depart from the dFADs was not significantly longer thanthe time it took for the aggregation to form. The value of these results in thecontext of the ecological trap hypothesis is discussed, and further analysesto enrich and make use of this data source are proposed.",1
