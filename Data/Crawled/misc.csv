title,abstract
X-Risk Analysis for AI Research,"Artificial intelligence (AI) has the potential to greatly improve society,
but as with any powerful technology, it comes with heightened risks and
responsibilities. Current AI research lacks a systematic discussion of how to
manage long-tail risks from AI systems, including speculative long-term risks.
Keeping in mind the potential benefits of AI, there is some concern that
building ever more intelligent and powerful AI systems could eventually result
in systems that are more powerful than us; some say this is like playing with
fire and speculate that this could create existential risks (x-risks). To add
precision and ground these discussions, we provide a guide for how to analyze
AI x-risk, which consists of three parts: First, we review how systems can be
made safer today, drawing on time-tested concepts from hazard analysis and
systems safety that have been designed to steer large processes in safer
directions. Next, we discuss strategies for having long-term impacts on the
safety of future systems. Finally, we discuss a crucial concept in making AI
systems safer by improving the balance between safety and general capabilities.
We hope this document and the presented concepts and tools serve as a useful
guide for understanding how to analyze AI x-risk."
Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks,"Artificial intelligence (AI) systems can provide many beneficial capabilities
but also risks of adverse events. Some AI systems could present risks of events
with very high or catastrophic consequences at societal scale. The US National
Institute of Standards and Technology (NIST) is developing the NIST Artificial
Intelligence Risk Management Framework (AI RMF) as voluntary guidance on AI
risk assessment and management for AI developers and others. For addressing
risks of events with catastrophic consequences, NIST indicated a need to
translate from high level principles to actionable risk management guidance."
AI Research Considerations for Human Existential Safety (ARCHES),"Framed in positive terms, this report examines how technical AI research
might be steered in a manner that is more attentive to humanity's long-term
prospects for survival as a species. In negative terms, we ask what existential
risks humanity might face from AI development in the next century, and by what
principles contemporary technical research might be directed to address those
risks."
Concrete Problems in AI Safety,"Rapid progress in machine learning and artificial intelligence (AI) has
brought increasing attention to the potential impacts of AI technologies on
society. In this paper we discuss one such potential impact: the problem of
accidents in machine learning systems, defined as unintended and harmful
behavior that may emerge from poor design of real-world AI systems. We present
a list of five practical research problems related to accident risk,
categorized according to whether the problem originates from having the wrong
objective function (""avoiding side effects"" and ""avoiding reward hacking""), an
objective function that is too expensive to evaluate frequently (""scalable
supervision""), or undesirable behavior during the learning process (""safe
exploration"" and ""distributional shift""). We review previous work in these
areas as well as suggesting research directions with a focus on relevance to
cutting-edge AI systems. Finally, we consider the high-level question of how to
think most productively about the safety of forward-looking applications of AI."
Unsolved Problems in ML Safety,"Machine learning (ML) systems are rapidly increasing in size, are acquiring
new capabilities, and are increasingly deployed in high-stakes settings. As
with other powerful technologies, safety for ML should be a leading research
priority. In response to emerging safety challenges in ML, such as those
introduced by recent large-scale models, we provide a new roadmap for ML Safety
and refine the technical problems that the field needs to address. We present
four problems ready for research, namely withstanding hazards (""Robustness""),
identifying hazards (""Monitoring""), reducing inherent model hazards
(""Alignment""), and reducing systemic hazards (""Systemic Safety""). Throughout,
we clarify each problem's motivation and provide concrete research directions."
