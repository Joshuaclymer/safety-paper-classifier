title,abstract
Is Power-Seeking AI an Existential Risk?,"This report examines what I see as the core argument for concern about
existential risk from misaligned artificial intelligence. I proceed in two
stages. First, I lay out a backdrop picture that informs such concern. On this
picture, intelligent agency is an extremely powerful force, and creating agents
much more intelligent than us is playing with fire -- especially given that if
their objectives are problematic, such agents would plausibly have instrumental
incentives to seek power over humans. Second, I formulate and evaluate a more
specific six-premise argument that creating agents of this kind will lead to
existential catastrophe by 2070. On this argument, by 2070: (1) it will become
possible and financially feasible to build relevantly powerful and agentic AI
systems; (2) there will be strong incentives to do so; (3) it will be much
harder to build aligned (and relevantly powerful/agentic) AI systems than to
build misaligned (and relevantly powerful/agentic) AI systems that are still
superficially attractive to deploy; (4) some such misaligned systems will seek
power over humans in high-impact ways; (5) this problem will scale to the full
disempowerment of humanity; and (6) such disempowerment will constitute an
existential catastrophe. I assign rough subjective credences to the premises in
this argument, and I end up with an overall estimate of ~5% that an existential
catastrophe of this kind will occur by 2070. (May 2022 update: since making
this report public in April 2021, my estimate here has gone up, and is now at
>10%.)"
Conservative Agency via Attainable Utility Preservation,"Reward functions are easy to misspecify; although designers can make
corrections after observing mistakes, an agent pursuing a misspecified reward
function can irreversibly change the state of its environment. If that change
precludes optimization of the correctly specified reward function, then
correction is futile. For example, a robotic factory assistant could break
expensive equipment due to a reward misspecification; even if the designers
immediately correct the reward function, the damage is done. To mitigate this
risk, we introduce an approach that balances optimization of the primary reward
function with preservation of the ability to optimize auxiliary reward
functions. Surprisingly, even when the auxiliary reward functions are randomly
generated and therefore uninformative about the correctly specified reward
function, this approach induces conservative, effective behavior."
Parametrically Retargetable Decision-Makers Tend To Seek Power,"If capable AI agents are generally incentivized to seek power in service of
the objectives we specify for them, then these systems will pose enormous
risks, in addition to enormous benefits. In fully observable environments, most
reward functions have an optimal policy which seeks power by keeping options
open and staying alive. However, the real world is neither fully observable,
nor will agents be perfectly optimal. We consider a range of models of AI
decision-making, from optimal, to random, to choices informed by learning and
interacting with an environment. We discover that many decision-making
functions are retargetable, and that retargetability is sufficient to cause
power-seeking tendencies. Our functional criterion is simple and broad. We show
that a range of qualitatively dissimilar decision-making procedures incentivize
agents to seek power. We demonstrate the flexibility of our results by
reasoning about learned policy incentives in Montezuma's Revenge. These results
suggest a safety risk: Eventually, highly retargetable training procedures may
train real-world agents which seek power over humans."
Optimal Policies Tend to Seek Power,"Some researchers speculate that intelligent reinforcement learning (RL)
agents would be incentivized to seek resources and power in pursuit of their
objectives. Other researchers point out that RL agents need not have human-like
power-seeking instincts. To clarify this discussion, we develop the first
formal theory of the statistical tendencies of optimal policies. In the context
of Markov decision processes, we prove that certain environmental symmetries
are sufficient for optimal policies to tend to seek power over the environment.
These symmetries exist in many environments in which the agent can be shut down
or destroyed. We prove that in these environments, most reward functions make
it optimal to seek power by keeping a range of options available and, when
maximizing average reward, by navigating towards larger sets of potential
terminal states."
What Would Jiminy Cricket Do? Towards Agents That Behave Morally,"When making everyday decisions, people are guided by their conscience, an
internal sense of right and wrong. By contrast, artificial agents are currently
not endowed with a moral sense. As a consequence, they may learn to behave
immorally when trained on environments that ignore moral concerns, such as
violent video games. With the advent of generally capable agents that pretrain
on many environments, it will become necessary to mitigate inherited biases
from environments that teach immoral behavior. To facilitate the development of
agents that avoid causing wanton harm, we introduce Jiminy Cricket, an
environment suite of 25 text-based adventure games with thousands of diverse,
morally salient scenarios. By annotating every possible game state, the Jiminy
Cricket environments robustly evaluate whether agents can act morally while
maximizing reward. Using models with commonsense moral knowledge, we create an
elementary artificial conscience that assesses and guides agents. In extensive
experiments, we find that the artificial conscience approach can steer agents
towards moral behavior without sacrificing performance."
Avoiding Side Effects in Complex Environments,"Reward function specification can be difficult. Rewarding the agent for
making a widget may be easy, but penalizing the multitude of possible negative
side effects is hard. In toy environments, Attainable Utility Preservation
(AUP) avoided side effects by penalizing shifts in the ability to achieve
randomly generated goals. We scale this approach to large, randomly generated
environments based on Conway's Game of Life. By preserving optimal value for a
single randomly generated reward function, AUP incurs modest overhead while
leading the agent to complete the specified task and avoid many side effects.
Videos and code are available at"
The Off-Switch Game,"It is clear that one of the primary tools we can use to mitigate the
potential risk from a misbehaving AI system is the ability to turn the system
off. As the capabilities of AI systems improve, it is important to ensure that
such systems do not adopt subgoals that prevent a human from switching them
off. This is a challenge because many formulations of rational agents create
strong incentives for self-preservation. This is not caused by a built-in
instinct, but because a rational agent will maximize expected utility and
cannot achieve whatever objective it has been given if it is dead. Our goal is
to study the incentives an agent has to allow itself to be switched off. We
analyze a simple game between a human H and a robot R, where H can press R's
off switch but R can disable the off switch. A traditional agent takes its
reward function for granted: we show that such agents have an incentive to
disable the off switch, except in the special case where H is perfectly
rational. Our key insight is that for R to want to preserve its off switch, it
needs to be uncertain about the utility associated with the outcome, and to
treat H's actions as important observations about that utility. (R also has no
incentive to switch itself off in this setting.) We conclude that giving
machines an appropriate level of uncertainty about their objectives leads to
safer designs, and we argue that this setting is a useful generalization of the
classical AI paradigm of rational agents."
Aligning AI With Shared Human Values,"We show how to assess a language model's knowledge of basic concepts of
morality. We introduce the ETHICS dataset, a new benchmark that spans concepts
in justice, well-being, duties, virtues, and commonsense morality. Models
predict widespread moral judgments about diverse text scenarios. This requires
connecting physical and social world knowledge to value judgements, a
capability that may enable us to steer chatbot outputs or eventually regularize
open-ended reinforcement learning agents. With the ETHICS dataset, we find that
current language models have a promising but incomplete ability to predict
basic human ethical judgements. Our work shows that progress can be made on
machine ethics today, and it provides a steppingstone toward AI that is aligned
with human values."
TruthfulQA: Measuring How Models Mimic Human Falsehoods,"We propose a benchmark to measure whether a language model is truthful in
generating answers to questions. The benchmark comprises 817 questions that
span 38 categories, including health, law, finance and politics. We crafted
questions that some humans would answer falsely due to a false belief or
misconception. To perform well, models must avoid generating false answers
learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a
T5-based model. The best model was truthful on 58% of questions, while human
performance was 94%. Models generated many false answers that mimic popular
misconceptions and have the potential to deceive humans. The largest models
were generally the least truthful. This contrasts with other NLP tasks, where
performance improves with model size. However, this result is expected if false
answers are learned from the training distribution. We suggest that scaling up
models alone is less promising for improving truthfulness than fine-tuning
using training objectives other than imitation of text from the web."
Truthful AI: Developing and governing AI that does not lie,"In many contexts, lying -- the use of verbal falsehoods to deceive -- is
harmful. While lying has traditionally been a human affair, AI systems that
make sophisticated verbal statements are becoming increasingly prevalent. This
raises the question of how we should limit the harm caused by AI ""lies"" (i.e.
falsehoods that are actively selected for). Human truthfulness is governed by
social norms and by laws (against defamation, perjury, and fraud). Differences
between AI and humans present an opportunity to have more precise standards of
truthfulness for AI, and to have these standards rise over time. This could
provide significant benefits to public epistemics and the economy, and mitigate
risks of worst-case AI futures."
Formalizing the Problem of Side Effect Regularization,"AI objectives are often hard to specify properly. Some approaches tackle this
problem by regularizing the AI's side effects: Agents must weigh off ""how much
of a mess they make"" with an imperfectly specified proxy objective. We propose
a formal criterion for side effect regularization via the assistance game
framework. In these games, the agent solves a partially observable Markov
decision process (POMDP) representing its uncertainty about the objective
function it should optimize. We consider the setting where the true objective
is revealed to the agent at a later time step. We show that this POMDP is
solved by trading off the proxy reward with the agent's ability to achieve a
range of future tasks. We empirically demonstrate the reasonableness of our
problem formalization via ground-truth evaluation in two gridworld
environments."
Towards Safe Reinforcement Learning via Constraining Conditional Value-at-Risk,"Though deep reinforcement learning (DRL) has obtained substantial success, it
may encounter catastrophic failures due to the intrinsic uncertainty of both
transition and observation. Most of the existing methods for safe reinforcement
learning can only handle transition disturbance or observation disturbance
since these two kinds of disturbance affect different parts of the agent;
besides, the popular worst-case return may lead to overly pessimistic policies.
To address these issues, we first theoretically prove that the performance
degradation under transition disturbance and observation disturbance depends on
a novel metric of Value Function Range (VFR), which corresponds to the gap in
the value function between the best state and the worst state. Based on the
analysis, we adopt conditional value-at-risk (CVaR) as an assessment of risk
and propose a novel reinforcement learning algorithm of
CVaR-Proximal-Policy-Optimization (CPPO) which formalizes the risk-sensitive
constrained optimization problem by keeping its CVaR under a given threshold.
Experimental results show that CPPO achieves a higher cumulative reward and is
more robust against both observation and transition disturbances on a series of
continuous control tasks in MuJoCo."
Enhancing Safe Exploration Using Safety State Augmentation,"Safe exploration is a challenging and important problem in model-free
reinforcement learning (RL). Often the safety cost is sparse and unknown, which
unavoidably leads to constraint violations -- a phenomenon ideally to be
avoided in safety-critical applications. We tackle this problem by augmenting
the state-space with a safety state, which is nonnegative if and only if the
constraint is satisfied. The value of this state also serves as a distance
toward constraint violation, while its initial value indicates the available
safety budget. This idea allows us to derive policies for scheduling the safety
budget during training. We call our approach Simmer (Safe policy IMproveMEnt
for RL) to reflect the careful nature of these schedules. We apply this idea to
two safe RL problems: RL with constraints imposed on an average cost, and RL
with constraints imposed on a cost with probability one. Our experiments
suggest that simmering a safe algorithm can improve safety during training for
both settings. We further show that Simmer can stabilize training and improve
the performance of safe RL with average constraints."
Provably Safe Reinforcement Learning: A Theoretical and Experimental Comparison,"Ensuring safety of reinforcement learning (RL) algorithms is crucial for many
real-world tasks. However, vanilla RL does not guarantee safety for an agent.
In recent years, several methods have been proposed to provide safety
guarantees for RL. To the best of our knowledge, there is no comprehensive
comparison of these provably safe RL methods. We therefore introduce a
categorization for existing provably safe RL methods, and present the
theoretical foundations for both continuous and discrete action spaces.
Additionally, we evaluate provably safe RL on an inverted pendulum. In the
experiments, it is shown that indeed only provably safe RL methods guarantee
safety."
Counterfactual harm,"To act safely and ethically in the real world, agents must be able to reason
about harm and avoid harmful actions. In this paper we develop the first
statistical definition of harm and a framework for incorporating harm into
algorithmic decisions. We argue that harm is fundamentally a counterfactual
quantity, and show that standard machine learning algorithms that cannot
perform counterfactual reasoning are guaranteed to pursue harmful policies in
certain environments. To resolve this we derive a family of counterfactual
objective functions that robustly mitigate for harm. We demonstrate our
approach with a statistical model for identifying optimal drug doses. While
standard algorithms that select doses using causal treatment effects result in
harmful doses, our counterfactual algorithm identifies doses that are
significantly less harmful without sacrificing efficacy."
AiSocrates: Towards Answering Ethical Quandary Questions,"Considerable advancements have been made in various NLP tasks based on the
impressive power of large pre-trained language models (LLMs). These results
have inspired efforts to understand the limits of LLMs so as to evaluate how
far we are from achieving human level general natural language understanding.
In this work, we challenge the capability of LLMs with the new task of Ethical
Quandary Generative Question Answering. Ethical quandary questions are more
challenging to address because multiple conflicting answers may exist to a
single quandary. We propose a system, AiSocrates, that provides an answer with
a deliberative exchange of different perspectives to an ethical quandary, in
the approach of Socratic philosophy, instead of providing a closed answer like
an oracle. AiSocrates searches for different ethical principles applicable to
the ethical quandary and generates an answer conditioned on the chosen
principles through prompt-based few-shot learning. We also address safety
concerns by providing a human controllability option in choosing ethical
principles. We show that AiSocrates generates promising answers to ethical
quandary questions with multiple perspectives, 6.92% more often than answers
written by human philosophers by one measure, but the system still needs
improvement to match the coherence of human philosophers fully. We argue that
AiSocrates is a promising step toward developing an NLP system that
incorporates human values explicitly by prompt instructions. We are releasing
the code for research purposes."
Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,"We apply preference modeling and reinforcement learning from human feedback
(RLHF) to finetune language models to act as helpful and harmless assistants.
We find this alignment training improves performance on almost all NLP
evaluations, and is fully compatible with training for specialized skills such
as python coding and summarization. We explore an iterated online mode of
training, where preference models and RL policies are updated on a weekly
cadence with fresh human feedback data, efficiently improving our datasets and
models. Finally, we investigate the robustness of RLHF training, and identify a
roughly linear relation between the RL reward and the square root of the KL
divergence between the policy and its initialization. Alongside our main
results, we perform peripheral analyses on calibration, competing objectives,
and the use of OOD detection, compare our models with human writers, and
provide samples from our models using prompts appearing in recent related work."
AI safety via debate,"To make AI systems broadly useful for challenging real-world tasks, we need
them to learn complex human goals and preferences. One approach to specifying
complex goals asks humans to judge during training which agent behaviors are
safe and useful, but this approach can fail if the task is too complicated for
a human to directly judge. To help address this concern, we propose training
agents via self play on a zero sum debate game. Given a question or proposed
action, two agents take turns making short statements up to a limit, then a
human judges which of the agents gave the most true, useful information. In an
analogy to complexity theory, debate with optimal play can answer any question
in PSPACE given polynomial time judges (direct judging answers only NP
questions). In practice, whether debate works involves empirical questions
about humans and the tasks we want AIs to perform, plus theoretical questions
about the meaning of AI alignment. We report results on an initial MNIST
experiment where agents compete to convince a sparse classifier, boosting the
classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to
85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of
the debate model, focusing on potential weaknesses as the model scales up, and
we propose future human and computer experiments to test these properties."
Learning to summarize from human feedback,"As language models become more powerful, training and evaluation are
increasingly bottlenecked by the data and metrics used for a particular task.
For example, summarization models are often trained to predict human reference
summaries and evaluated using ROUGE, but both of these metrics are rough
proxies for what we really care about -- summary quality. In this work, we show
that it is possible to significantly improve summary quality by training a
model to optimize for human preferences. We collect a large, high-quality
dataset of human comparisons between summaries, train a model to predict the
human-preferred summary, and use that model as a reward function to fine-tune a
summarization policy using reinforcement learning. We apply our method to a
version of the TL;DR dataset of Reddit posts and find that our models
significantly outperform both human reference summaries and much larger models
fine-tuned with supervised learning alone. Our models also transfer to CNN/DM
news articles, producing summaries nearly as good as the human reference
without any news-specific fine-tuning. We conduct extensive analyses to
understand our human feedback dataset and fine-tuned models We establish that
our reward model generalizes to new datasets, and that optimizing our reward
model results in better summaries than optimizing ROUGE according to humans. We
hope the evidence from our paper motivates machine learning researchers to pay
closer attention to how their training loss affects the model behavior they
actually want."
The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models,"Reward hacking -- where RL agents exploit gaps in misspecified reward
functions -- has been widely observed, but not yet systematically studied. To
understand how reward hacking arises, we construct four RL environments with
misspecified rewards. We investigate reward hacking as a function of agent
capabilities: model capacity, action space resolution, observation space noise,
and training time. More capable agents often exploit reward misspecifications,
achieving higher proxy reward and lower true reward than less capable agents.
Moreover, we find instances of phase transitions: capability thresholds at
which the agent's behavior qualitatively shifts, leading to a sharp decrease in
the true reward. Such phase transitions pose challenges to monitoring the
safety of ML systems. To address this, we propose an anomaly detection task for
aberrant policies and offer several baseline detectors."
One for All: Simultaneous Metric and Preference Learning over Multiple Users,"This paper investigates simultaneous preference and metric learning from a
crowd of respondents. A set of items represented by $d$-dimensional feature
vectors and paired comparisons of the form ``item $i$ is preferable to item
$j$'' made by each user is given. Our model jointly learns a distance metric
that characterizes the crowd's general measure of item similarities along with
a latent ideal point for each user reflecting their individual preferences.
This model has the flexibility to capture individual preferences, while
enjoying a metric learning sample cost that is amortized over the crowd. We
first study this problem in a noiseless, continuous response setting (i.e.,
responses equal to differences of item distances) to understand the fundamental
limits of learning. Next, we establish prediction error guarantees for noisy,
binary measurements such as may be collected from human respondents, and show
how the sample complexity improves when the underlying metric is low-rank.
Finally, we establish recovery guarantees under assumptions on the response
distribution. We demonstrate the performance of our model on both simulated
data and on a dataset of color preference judgements across a large number of
users."
Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective,"Can humans get arbitrarily capable reinforcement learning (RL) agents to do
their bidding? Or will sufficiently capable RL agents always find ways to
bypass their intended objectives by shortcutting their reward signal? This
question impacts how far RL can be scaled, and whether alternative paradigms
must be developed in order to build safe artificial general intelligence. In
this paper, we study when an RL agent has an instrumental goal to tamper with
its reward process, and describe design principles that prevent instrumental
goals for two different types of reward tampering (reward function tampering
and RF-input tampering). Combined, the design principles can prevent both types
of reward tampering from being instrumental goals. The analysis benefits from
causal influence diagrams to provide intuitive yet precise formalizations."
Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning,"We identify two issues with the family of algorithms based on the Adversarial
Imitation Learning framework. The first problem is implicit bias present in the
reward functions used in these algorithms. While these biases might work well
for some environments, they can also lead to sub-optimal behavior in others.
Secondly, even though these algorithms can learn from few expert
demonstrations, they require a prohibitively large number of interactions with
the environment in order to imitate the expert for many real-world
applications. In order to address these issues, we propose a new algorithm
called Discriminator-Actor-Critic that uses off-policy Reinforcement Learning
to reduce policy-environment interaction sample complexity by an average factor
of 10. Furthermore, since our reward function is designed to be unbiased, we
can apply our algorithm to many problems without making any task-specific
adjustments."
Unsolved Problems in ML Safety,"Machine learning (ML) systems are rapidly increasing in size, are acquiring
new capabilities, and are increasingly deployed in high-stakes settings. As
with other powerful technologies, safety for ML should be a leading research
priority. In response to emerging safety challenges in ML, such as those
introduced by recent large-scale models, we provide a new roadmap for ML Safety
and refine the technical problems that the field needs to address. We present
four problems ready for research, namely withstanding hazards (""Robustness""),
identifying hazards (""Monitoring""), reducing inherent model hazards
(""Alignment""), and reducing systemic hazards (""Systemic Safety""). Throughout,
we clarify each problem's motivation and provide concrete research directions."
"Deep Imitative Models for Flexible Inference, Planning, and Control","Imitation Learning (IL) is an appealing approach to learn desirable
autonomous behavior. However, directing IL to achieve arbitrary goals is
difficult. In contrast, planning-based algorithms use dynamics models and
reward functions to achieve goals. Yet, reward functions that evoke desirable
behavior are often difficult to specify. In this paper, we propose Imitative
Models to combine the benefits of IL and goal-directed planning. Imitative
Models are probabilistic predictive models of desirable behavior able to plan
interpretable expert-like trajectories to achieve specified goals. We derive
families of flexible goal objectives, including constrained goal regions,
unconstrained goal sets, and energy-based goals. We show that our method can
use these objectives to successfully direct behavior. Our method substantially
outperforms six IL approaches and a planning-based approach in a dynamic
simulated autonomous driving task, and is efficiently learned from expert
demonstrations without online data collection. We also show our approach is
robust to poorly specified goals, such as goals on the wrong side of the road."
Active Exploration for Inverse Reinforcement Learning,"Inverse Reinforcement Learning (IRL) is a powerful paradigm for inferring a
reward function from expert demonstrations. Many IRL algorithms require a known
transition model and sometimes even a known expert policy, or they at least
require access to a generative model. However, these assumptions are too strong
for many real-world applications, where the environment can be accessed only
through sequential interaction. We propose a novel IRL algorithm: Active
exploration for Inverse Reinforcement Learning (AceIRL), which actively
explores an unknown environment and expert policy to quickly learn the expert's
reward function and identify a good policy. AceIRL uses previous observations
to construct confidence intervals that capture plausible reward functions and
find exploration policies that focus on the most informative regions of the
environment. AceIRL is the first approach to active IRL with sample-complexity
bounds that does not require a generative model of the environment. AceIRL
matches the sample complexity of active IRL with a generative model in the
worst case. Additionally, we establish a problem-dependent bound that relates
the sample complexity of AceIRL to the suboptimality gap of a given IRL
problem. We empirically evaluate AceIRL in simulations and find that it
significantly outperforms more naive exploration strategies."
X-Risk Analysis for AI Research,"Artificial intelligence (AI) has the potential to greatly improve society,
but as with any powerful technology, it comes with heightened risks and
responsibilities. Current AI research lacks a systematic discussion of how to
manage long-tail risks from AI systems, including speculative long-term risks.
Keeping in mind the potential benefits of AI, there is some concern that
building ever more intelligent and powerful AI systems could eventually result
in systems that are more powerful than us; some say this is like playing with
fire and speculate that this could create existential risks (x-risks). To add
precision and ground these discussions, we provide a guide for how to analyze
AI x-risk, which consists of three parts: First, we review how systems can be
made safer today, drawing on time-tested concepts from hazard analysis and
systems safety that have been designed to steer large processes in safer
directions. Next, we discuss strategies for having long-term impacts on the
safety of future systems. Finally, we discuss a crucial concept in making AI
systems safer by improving the balance between safety and general capabilities.
We hope this document and the presented concepts and tools serve as a useful
guide for understanding how to analyze AI x-risk."
AI Research Considerations for Human Existential Safety (ARCHES),"Framed in positive terms, this report examines how technical AI research
might be steered in a manner that is more attentive to humanity's long-term
prospects for survival as a species. In negative terms, we ask what existential
risks humanity might face from AI development in the next century, and by what
principles contemporary technical research might be directed to address those
risks."
Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks,"Artificial intelligence (AI) systems can provide many beneficial capabilities
but also risks of adverse events. Some AI systems could present risks of events
with very high or catastrophic consequences at societal scale. The US National
Institute of Standards and Technology (NIST) is developing the NIST Artificial
Intelligence Risk Management Framework (AI RMF) as voluntary guidance on AI
risk assessment and management for AI developers and others. For addressing
risks of events with catastrophic consequences, NIST indicated a need to
translate from high level principles to actionable risk management guidance."
Concrete Problems in AI Safety,"Rapid progress in machine learning and artificial intelligence (AI) has
brought increasing attention to the potential impacts of AI technologies on
society. In this paper we discuss one such potential impact: the problem of
accidents in machine learning systems, defined as unintended and harmful
behavior that may emerge from poor design of real-world AI systems. We present
a list of five practical research problems related to accident risk,
categorized according to whether the problem originates from having the wrong
objective function (""avoiding side effects"" and ""avoiding reward hacking""), an
objective function that is too expensive to evaluate frequently (""scalable
supervision""), or undesirable behavior during the learning process (""safe
exploration"" and ""distributional shift""). We review previous work in these
areas as well as suggesting research directions with a focus on relevance to
cutting-edge AI systems. Finally, we consider the high-level question of how to
think most productively about the safety of forward-looking applications of AI."
