title,abstract
Forecasting Future World Events with Neural Networks,"Forecasting future world events is a challenging but valuable task. Forecasts
of climate, geopolitical conflict, pandemics and economic indicators help shape
policy and decision making. In these domains, the judgment of expert humans
contributes to the best forecasts. Given advances in language modeling, can
these forecasts be automated? To this end, we introduce Autocast, a dataset
containing thousands of forecasting questions and an accompanying news corpus.
Questions are taken from forecasting tournaments, ensuring high quality,
real-world importance, and diversity. The news corpus is organized by date,
allowing us to precisely simulate the conditions under which humans made past
forecasts (avoiding leakage from the future). Motivated by the difficulty of
forecasting numbers across orders of magnitude (e.g. global cases of COVID-19
in 2022), we also curate IntervalQA, a dataset of numerical questions and
metrics for calibration. We test language models on our forecasting task and
find that performance is far below a human expert baseline. However,
performance improves with increased model size and incorporation of relevant
information from the news corpus. In sum, Autocast poses a novel challenge for
large language models and improved performance could bring large practical
benefits."
Robustness Evaluation of Deep Unsupervised Learning Algorithms for Intrusion Detection Systems,"Recently, advances in deep learning have been observed in various fields,
including computer vision, natural language processing, and cybersecurity.
Machine learning (ML) has demonstrated its ability as a potential tool for
anomaly detection-based intrusion detection systems to build secure computer
networks. Increasingly, ML approaches are widely adopted than heuristic
approaches for cybersecurity because they learn directly from data. Data is
critical for the development of ML systems, and becomes potential targets for
attackers. Basically, data poisoning or contamination is one of the most common
techniques used to fool ML models through data. This paper evaluates the
robustness of six recent deep learning algorithms for intrusion detection on
contaminated data. Our experiments suggest that the state-of-the-art algorithms
used in this study are sensitive to data contamination and reveal the
importance of self-defense against data perturbation when developing novel
models, especially for intrusion detection systems."
Anomal-E: A Self-Supervised Network Intrusion Detection System based on Graph Neural Networks,"This paper investigates Graph Neural Networks (GNNs) application for
self-supervised network intrusion and anomaly detection. GNNs are a deep
learning approach for graph-based data that incorporate graph structures into
learning to generalise graph representations and output embeddings. As network
flows are naturally graph-based, GNNs are a suitable fit for analysing and
learning network behaviour. The majority of current implementations of
GNN-based Network Intrusion Detection Systems (NIDSs) rely heavily on labelled
network traffic which can not only restrict the amount and structure of input
traffic, but also the NIDSs potential to adapt to unseen attacks. To overcome
these restrictions, we present Anomal-E, a GNN approach to intrusion and
anomaly detection that leverages edge features and graph topological structure
in a self-supervised process. This approach is, to the best our knowledge, the
first successful and practical approach to network intrusion detection that
utilises network flows in a self-supervised, edge leveraging GNN. Experimental
results on two modern benchmark NIDS datasets not only clearly display the
improvement of using Anomal-E embeddings rather than raw features, but also the
potential Anomal-E has for detection on wild network traffic."
Generalized Beliefs for Cooperative AI,"Self-play is a common paradigm for constructing solutions in Markov games
that can yield optimal policies in collaborative settings. However, these
policies often adopt highly-specialized conventions that make playing with a
novel partner difficult. To address this, recent approaches rely on encoding
symmetry and convention-awareness into policy training, but these require
strong environmental assumptions and can complicate policy training. We
therefore propose moving the learning of conventions to the belief space.
Specifically, we propose a belief learning model that can maintain beliefs over
rollouts of policies not seen at training time, and can thus decode and adapt
to novel conventions at test time. We show how to leverage this model for both
search and training of a best response over various pools of policies to
greatly improve ad-hoc teamplay. We also show how our setup promotes
explainability and interpretability of nuanced agent conventions."
Forecasting Future World Events with Neural Networks,"Forecasting future world events is a challenging but valuable task. Forecasts
of climate, geopolitical conflict, pandemics and economic indicators help shape
policy and decision making. In these domains, the judgment of expert humans
contributes to the best forecasts. Given advances in language modeling, can
these forecasts be automated? To this end, we introduce Autocast, a dataset
containing thousands of forecasting questions and an accompanying news corpus.
Questions are taken from forecasting tournaments, ensuring high quality,
real-world importance, and diversity. The news corpus is organized by date,
allowing us to precisely simulate the conditions under which humans made past
forecasts (avoiding leakage from the future). Motivated by the difficulty of
forecasting numbers across orders of magnitude (e.g. global cases of COVID-19
in 2022), we also curate IntervalQA, a dataset of numerical questions and
metrics for calibration. We test language models on our forecasting task and
find that performance is far below a human expert baseline. However,
performance improves with increased model size and incorporation of relevant
information from the news corpus. In sum, Autocast poses a novel challenge for
large language models and improved performance could bring large practical
benefits."
Open Problems in Cooperative AI,"Problems of cooperation--in which agents seek ways to jointly improve their
welfare--are ubiquitous and important. They can be found at scales ranging from
our daily routines--such as driving on highways, scheduling meetings, and
working collaboratively--to our global challenges--such as peace, commerce, and
pandemic preparedness. Arguably, the success of the human species is rooted in
our ability to cooperate. Since machines powered by artificial intelligence are
playing an ever greater role in our lives, it will be important to equip them
with the capabilities necessary to cooperate and to foster cooperation."
On Single Point Forecasts for Fat-Tailed Variables,"We discuss common errors and fallacies when using naive ""evidence based""
empiricism and point forecasts for fat-tailed variables, as well as the
insufficiency of using naive first-order scientific methods for tail risk
management. We use the COVID-19 pandemic as the background for the discussion
and as an example of a phenomenon characterized by a multiplicative nature, and
what mitigating policies must result from the statistical properties and
associated risks. In doing so, we also respond to the points raised by
Ioannidis et al. (2020)."
Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions,"There is burgeoning interest in designing AI-based systems to assist humans
in designing computing systems, including tools that automatically generate
computer code. The most notable of these comes in the form of the first
self-described `AI pair programmer', GitHub Copilot, a language model trained
over open-source GitHub code. However, code often contains bugs - and so, given
the vast quantity of unvetted code that Copilot has processed, it is certain
that the language model will have learned from exploitable, buggy code. This
raises concerns on the security of Copilot's code contributions. In this work,
we systematically investigate the prevalence and conditions that can cause
GitHub Copilot to recommend insecure code. To perform this analysis we prompt
Copilot to generate code in scenarios relevant to high-risk CWEs (e.g. those
from MITRE's ""Top 25"" list). We explore Copilot's performance on three distinct
code generation axes -- examining how it performs given diversity of
weaknesses, diversity of prompts, and diversity of domains. In total, we
produce 89 different scenarios for Copilot to complete, producing 1,689
programs. Of these, we found approximately 40% to be vulnerable."
AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly Detection,"Analyzing the distribution shift of data is a growing research direction in
nowadays Machine Learning, leading to emerging new benchmarks that focus on
providing a suitable scenario for studying the generalization properties of ML
models. The existing benchmarks are focused on supervised learning, and to the
best of our knowledge, there is none for unsupervised learning. Therefore, we
introduce an unsupervised anomaly detection benchmark with data that shifts
over time, built over Kyoto-2006+, a traffic dataset for network intrusion
detection. This kind of data meets the premise of shifting the input
distribution: it covers a large time span ($10$ years), with naturally
occurring changes over time (\eg users modifying their behavior patterns, and
software updates). We first highlight the non-stationary nature of the data,
using a basic per-feature analysis, t-SNE, and an Optimal Transport approach
for measuring the overall distribution distances between years. Next, we
propose AnoShift, a protocol splitting the data in IID, NEAR, and FAR testing
splits. We validate the performance degradation over time with diverse models
(MLM to classical Isolation Forest). Finally, we show that by acknowledging the
distribution shift problem and properly addressing it, the performance can be
improved compared to the classical IID training (by up to $3\%$, on average).
Dataset and code are available at"
Developing Optimal Causal Cyber-Defence Agents via Cyber Security Simulation,"In this paper we explore cyber security defence, through the unification of a
novel cyber security simulator with models for (causal) decision-making through
optimisation. Particular attention is paid to a recently published approach:
dynamic causal Bayesian optimisation (DCBO). We propose that DCBO can act as a
blue agent when provided with a view of a simulated network and a causal model
of how a red agent spreads within that network. To investigate how DCBO can
perform optimal interventions on host nodes, in order to reduce the cost of
intrusions caused by the red agent. Through this we demonstrate a complete
cyber-simulation system, which we use to generate observational data for DCBO
and provide numerical quantitative results which lay the foundations for future
work in this space."
HierarchicalForecast: A Reference Framework for Hierarchical Forecasting in Python,"Large collections of time series data are commonly organized into
cross-sectional structures with different levels of aggregation; examples
include product and geographical groupings. A necessary condition for coherent
decision-making and planning, with such datasets, is for the dis-aggregated
series' forecasts to add up exactly to the aggregated series forecasts, which
motivates the creation of novel hierarchical forecasting algorithms. The
growing interest of the Machine Learning community in cross-sectional
hierarchical forecasting systems states that we are in a propitious moment to
ensure that scientific endeavors are grounded on sound baselines. For this
reason, we put forward the HierarchicalForecast library, which contains
preprocessed publicly available datasets, evaluation metrics, and a compiled
set of statistical baseline models. Our Python-based framework aims to bridge
the gap between statistical, econometric modeling, and Machine Learning
forecasting research. Code and documentation are available in"
